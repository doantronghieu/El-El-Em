{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake LM\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"python_repl\"])\n",
    "responses = [\"Action: Python_REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "agent.run(\"whats 2 + 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japan\n"
     ]
    }
   ],
   "source": [
    "# The OpenAI LLM takes a text input and returns a completion\n",
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
    "    repo_id=\"google/flan-t5-xxl\"\n",
    ")\n",
    "prompt = \"In which country is Tokyo?\"\n",
    "completion = llm(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You can try having a pasta dish with vegetables such as a delicious spaghetti primavera or a pasta stir fry. Another option is to make a cheesy vegetable lasagna which includes lots of veggies like spinach, onions, and tomato sauce. Additionally, a comforting bowl of minestrone soup is a great way to increase your vegetable intake while still enjoying a delicious dish.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import JinaChat\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = JinaChat(temperature=0.)\n",
    "chat(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You help a user find a nutritious and tasty food to eat in one word.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=\"I like pasta with cheese, but I need to eat more vegetables, what should I eat?\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.jinachat.JinaChat.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: HTTP code 408 from API ().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'aime l'IA générative!\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import JinaChat\n",
    "from langchain.schema import HumanMessage\n",
    "chat = JinaChat(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French: I love generative AI!\"\n",
    "    )\n",
    "]\n",
    "print(chat(messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What NFL team won the Super Bowl in the year Justin Beiber was born?\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Justin Beiber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = VertexAI()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "answer = []\n",
      "for i in range(1, n + 1):\n",
      "    if i % 3 == 0 and i % 5 == 0:\n",
      "        answer.append(\"FizzBuzz\")\n",
      "    elif i % 3 == 0:\n",
      "        answer.append(\"Fizz\")\n",
      "    elif i % 5 == 0:\n",
      "        answer.append(\"Buzz\")\n",
      "    else:\n",
      "        answer.append(str(i))\n",
      "\n",
      "return answer\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# code generation\n",
    "\n",
    "question = \"\"\"\n",
    "Given an integer n, return a string array answer (1-indexed) where:\n",
    "\n",
    "answer[i] == \"FizzBuzz\" if i is divisible by 3 and 5.\n",
    "answer[i] == \"Fizz\" if i is divisible by 3.\n",
    "answer[i] == \"Buzz\" if i is divisible by 5.\n",
    "answer[i] == i (as a string) if none of the above conditions are true.\n",
    "\"\"\"\n",
    "\n",
    "llm = VertexAI(model_name=\"code-bison\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japan\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
    "    repo_id=\"google/flan-t5-xxl\"\n",
    ")\n",
    "prompt = \"In which country is Tokyo?\"\n",
    "completion = llm(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Replicate\n",
    "\n",
    "text2image = Replicate(\n",
    "    model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n",
    "    input={\"image_dimensions\": \"512x512\"},\n",
    ")\n",
    "image_url = text2image(\"a book cover for a book about creating generative ai applications in Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://replicate.delivery/pbxt/JsEOpBIRi8KLM5Hjpee3AKm7IQokIqmbKJ5Ki3mmT5hroxORA/out-0.png'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (langchain_cookbook)",
   "language": "python",
   "name": "pycharm-99d76ad9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
