{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API model integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Fake LLM lets you simulate responses during testing without making actual \n",
    "API calls, useful for rapid prototyping and unit testing agents. \n",
    "It avoids hitting rate limits and enables mocking responses to validate proper \n",
    "agent handling. Fast agent iteration is possible without needing a real LLM. \n",
    "For instance, initializing a Fake LLM to return \"Hello\" can be done as follows:\n",
    "\"\"\"\n",
    "from langchain.llms import FakeListLLM\n",
    "\n",
    "fake_llm = FakeListLLM(responses=['Hello'])\n",
    "\"\"\"\n",
    "We set up an agent using the React strategy (ZERO_SHOT_REACT_DESCRIPTION). \n",
    "The agent is run with the text \"what's 2 + 2.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We connect a tool, a Python REPL, based on the LLM output. \n",
    "FakeListLLM provides two consistent responses \n",
    "(\"Action: Python_REPL\\nAction Input: print(2 + 2)\" and \n",
    "\"Final Answer: 4\"). The fake LLM output triggers a call to the Python \n",
    "interpreter, resulting in a return of 4. Note that the action must match the \n",
    "name attribute of the tool, PythonREPLTool.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "tools = load_tools(['python_repl'])\n",
    "response = [\n",
    "    'Action: Python_REPL\\nAction Input: print(2 + 2)', 'Final Answer: 4']\n",
    "llm = FakeListLLM(responses=response)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run('Whats 2 + 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonREPLTool(BaseTool):\n",
    "    \"\"\"A tool for running python code in a REPL.\"\"\"\n",
    "    name = \"Python_REPL\"\n",
    "    description = (\n",
    "        \"A Python shell. Use this to execute python commands. \"\n",
    "        \"Input should be a valid python command. \"\n",
    "        \"If you want to see the output of a value, you should print it out\"\n",
    "        \"with `print(...)`.\"\n",
    "    )\n",
    "\"\"\"\n",
    "Tool names and descriptions are provided to the LLM, which then decides an \n",
    "action (execution of a tool or planning) based on the information. \n",
    "The Python interpreter's output is passed to the fake LLM, which disregards the \n",
    "observation and returns 4. If the second response changes to \n",
    "\"Final Answer: 5,\" the agent's output won't correspond to the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Python to add 4 and 4\n",
      "Action: [Python_REPL]\n",
      "Action Input: 4 + 4\u001b[0m\n",
      "Observation: [Python_REPL] is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 8\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can utilize the OpenAI language model class to set up an LLM for interaction. \n",
    "Let's create an agent for calculations using this model \n",
    "\"\"\"\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "tools = load_tools(['python_repl'])\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run('whats 4 + 4')\n",
    "\n",
    "\"\"\"\n",
    "The agent produces the correct solution. Though a simple problem, it's \n",
    "fascinating to express questions in natural language. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vietnam\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
    "    repo_id='google/flan-t5-xxl'\n",
    ")\n",
    "\n",
    "prompt = 'In which country is Hanoi?'\n",
    "completion = llm(prompt)\n",
    "print(completion)\n",
    "\n",
    "\"\"\"\n",
    "The LLM takes a text input, a question, and produces a completion. \n",
    "The model possesses extensive knowledge and can generate answers to \n",
    "knowledge-based questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jina AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Replicate\n",
    "\n",
    "text2image = Replicate(\n",
    "  model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n",
    "  input={'image_dimensions': '512x512'}\n",
    ")\n",
    "image_url = text2image(\"a book cover for a book about creating generative ai applications in Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "generate_text = pipeline(\n",
    "    model = \"aisquared/dlite-v1-355m\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    framework='pt'\n",
    ")\n",
    "# generate_text('Hello')\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=generate_text)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))\n",
    "\"\"\"\n",
    "Running the above code downloads all necessary components for the model, \n",
    "including the tokenizer and model weights from Hugging Face. \n",
    "This relatively performant model, with 355 million parameters, is specifically \n",
    "tuned for conversations. We can proceed to perform a text completion for \n",
    "inspiration in this chapter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an application for customer service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I've tasked GPT-3.5 to create a concise customer email complaint about a coffee \n",
    "machine. Let's assess the sentiment using our model.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "customer_email = \"\"\"\n",
    "I am writing to pour my heart out about the recent unfortunate experience \n",
    "I had with one of your coffee machines that arrived broken. I anxiously \n",
    "unwrapped the box containing my highly anticipated coffee machine. \n",
    "However, what I discovered within broke not only my spirit but also any \n",
    "semblance of confidence I had placed in your brand.\n",
    "Its once elegant exterior was marred by the scars of travel, resembling a \n",
    "war-torn soldier who had fought valiantly on the fields of some espresso \n",
    "battlefield. This heartbreaking display of negligence shattered my dreams \n",
    "of indulging in daily coffee perfection, leaving me emotionally distraught \n",
    "and inconsolable\n",
    "\"\"\"\n",
    "\n",
    "sentiment_model = pipeline(\n",
    "  task='sentiment-analysis',\n",
    "  model=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    ")\n",
    "\n",
    "print(sentiment_model(customer_email))\n",
    "\n",
    "\"\"\"\n",
    "The sentiment model in use, Twitter-roBERTa-base, trained on tweets, \n",
    "may not be the most suitable for this scenario. Besides emotion sentiment \n",
    "analysis, it can handle tasks like emotion recognition (anger, joy, sadness, or \n",
    "optimism), emoji prediction, irony detection, hate speech detection, offensive \n",
    "language identification, and stance detection (favor, neutral, or against).\n",
    "\n",
    "For sentiment analysis, we receive a rating and a numeric score indicating \n",
    "confidence in the label. The labels are:\n",
    "- 0: Negative\n",
    "- 1: Neutral\n",
    "- 2: Positive\n",
    "\n",
    "For comparison, if the email expresses strong negative emotions like \"I am so \n",
    "angry and sad, I want to kill myself,\" we should expect a score close to 0.98 \n",
    "for the negative label. Experimenting with other models or training improved\n",
    "models can be considered once we establish metrics to work against.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coffee machine that arrived broken broke the man's spirit and confidence in the brand. \"This heartbreaking display of negligence shattered my dreams of indulging in daily coffee perfection,\" he writes. \"I am emotionally distraught  and inconsolable! I am writing to pour my heart out about the recent unfortunate experience\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Letâ€™s execute the summarization model remotely on a server. \n",
    "\"\"\"\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "customer_email = \"\"\"\n",
    "I am writing to pour my heart out about the recent unfortunate experience \n",
    "I had with one of your coffee machines that arrived broken. I anxiously \n",
    "unwrapped the box containing my highly anticipated coffee machine. \n",
    "However, what I discovered within broke not only my spirit but also any \n",
    "semblance of confidence I had placed in your brand.\n",
    "Its once elegant exterior was marred by the scars of travel, resembling a \n",
    "war-torn soldier who had fought valiantly on the fields of some espresso \n",
    "battlefield. This heartbreaking display of negligence shattered my dreams \n",
    "of indulging in daily coffee perfection, leaving me emotionally distraught \n",
    "and inconsolable\n",
    "\"\"\"\n",
    "\n",
    "summarizer = HuggingFaceHub(\n",
    "  repo_id='facebook/bart-large-cnn',\n",
    "  model_kwargs={'temperature': 0, 'max_length': 180}\n",
    ")\n",
    "\n",
    "def summarize(llm, text) -> str:\n",
    "  return llm(f'Summarize this: {text}!')\n",
    "\n",
    "print(summarize(summarizer, customer_email))\n",
    "\"\"\"\n",
    "The summary is passable but not very convincing, with some remaining rambling. \n",
    "We might explore other models or opt for an LLM with a summarization prompt. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Understanding the customer's issue can be valuable. Let's inquire with Vertex AI.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Given this text, decide what is the issue the customer is \n",
    "concerned about. Valid categories are these:\n",
    "* product issues\n",
    "* delivery problems\n",
    "* missing or late orders\n",
    "* wrong product\n",
    "* cancellation request\n",
    "* refund or exchange\n",
    "* bad support experience\n",
    "* no clear reason to be upset\n",
    "Text: {email}\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"email\"])\n",
    "llm = VertexAI()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "print(llm_chain.run(customer_email))\n",
    "\n",
    "\"\"\"\n",
    "We get product issues back\n",
    "Thoughtful implementation of such AI automation can complement human agents by \n",
    "addressing common queries, enabling them to focus on complex issues. \n",
    "This showcases the potential of generative AI to enhance customer service workflows.\n",
    "\n",
    "In the next chapter, we'll explore exposing this functionality in a graphical \n",
    "interface for customer service agents to interact with.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Capable Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mitigating hallucinations through fact-checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMCheckerChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The platypus, which is one of the five species of mammals that lay eggs, lays the biggest eggs among mammals.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMCheckerChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "text = \"What type of mammal lays the biggest eggs?\"\n",
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "\n",
    "checker_chain.run(text)\n",
    "\n",
    "\"\"\"\n",
    "The model may provide varied responses to the question, including incorrect \n",
    "answers such as the blue whale, the North American beaver, and the extinct \n",
    "Giant Moa when asked, \"What type of mammal lays the biggest eggs?\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summarizing information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe text asks about the mammal species with the largest eggs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For concise summarization, instruct the LLM on the desired length and \n",
    "provide the text.\n",
    "\"\"\"\n",
    "\n",
    "from langchain import OpenAI\n",
    "\n",
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "text = \"What type of mammal lays the biggest eggs?\"\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "summary = llm(prompt.format(text=text))\n",
    "\n",
    "summary\n",
    "\n",
    "\"\"\"\n",
    "Utilize LangChain's decorator syntax from the LangChain Decorators library for \n",
    "a more Pythonic and streamlined approach to prompt definition and execution. \n",
    "This provides a more intuitive interface, allowing multiline definitions and \n",
    "facilitating the utilization of LLMs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker wants to share a boring story from their youth.\n"
     ]
    }
   ],
   "source": [
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def summarize(text: str, length='short') -> str:\n",
    "  \"\"\"\n",
    "  Summarize this text in {length} length:\n",
    "  \n",
    "  {text}\n",
    "  \"\"\"\n",
    "  return\n",
    "\n",
    "summary = summarize(text=\"let me tell you a boring story from when I was young...\")\n",
    "print(summary)\n",
    "\n",
    "\"\"\"\n",
    "The @llm_prompt decorator converts docstrings into prompts, managing prompt \n",
    "execution. It efficiently handles parameter passing and output parsing,\n",
    "abstracting complexity. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe speaker is about to share a dull story from their past.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For dynamic inputs, use prompt templates to insert text into predefined prompts. \n",
    "These templates support variable length limits and modular prompt design, \n",
    "implemented in LangChain Expression Language (LCEL).\n",
    "\"\"\"\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Summarize this text: {text}?  \n",
    "\"\"\")\n",
    "\n",
    "text = \"let me tell you a boring story from when I was young...\"\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "summary = runnable.invoke({'text': text})\n",
    "summary\n",
    "\n",
    "\"\"\"\n",
    "LCEL offers a declarative approach to chain composition, providing intuitive \n",
    "and productive benefits such as asynchronous processing, batching, streaming, \n",
    "fallbacks, parallelism, and seamless integration with LangSmith tracing. \n",
    "Here, a runnable chain connects the prompt template, LLM, and output parser \n",
    "in a pipeline.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chain of Density (CoD), a prompt-guided technique for enhancing the information \n",
    "density of GPT-4-generated summaries incrementally, while maintaining length \n",
    "control. The CoD prompt is as follows:\n",
    "\"\"\"\n",
    "template = \"\"\"Article: { text }\n",
    "You will generate increasingly concise, entity-dense summaries of the \n",
    "above article.\n",
    "Repeat the following 2 steps 5 times.\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article \n",
    "which are missing from the previously generated summary.\n",
    "Step 2. Write a new, denser summary of identical length which covers every \n",
    "entity and detail from the previous summary plus the missing entities.\n",
    "A missing entity is:\n",
    "- relevant to the main story,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the article),\n",
    "- anywhere (can be located anywhere in the article).\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, ~80 words) yet highly \n",
    "non-specific, containing little information beyond the entities marked \n",
    "as missing. Use overly verbose language and fillers (e.g., \"this article \n",
    "discusses\") to reach ~80 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and \n",
    "make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative \n",
    "phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, \n",
    "i.e., easily understood without the article.\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, \n",
    "add fewer new entities.\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose \n",
    "keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This approach enables parallel processing and use of LLMs for reasoning, \n",
    "generating, and analyzing individual documents.\n",
    "\n",
    "Example of loading a PDF document and summarizing it1\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file_path = \"<pdf_file_path>\"\n",
    "pdf_loader = PyPDFLoader(pdf_file_path)\n",
    "docs = pdf_loader.load_and_split()\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "chain = load_summarize_chain(llm, chain_type='map_reduce')\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extracting information from documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from pydantic.dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(config={\"arbitrary_types_allowed\": True})\n",
    "class Experience(BaseModel):\n",
    "    start_date: Optional[str]\n",
    "    end_date: Optional[str]\n",
    "    description: Optional[str]\n",
    "\n",
    "\n",
    "@dataclass(config={\"arbitrary_types_allowed\": True})\n",
    "class Study(Experience):\n",
    "    degree: Optional[str]\n",
    "    university: Optional[str]\n",
    "    country: Optional[str]\n",
    "    grade: Optional[str]\n",
    "\n",
    "\n",
    "@dataclass(config={\"arbitrary_types_allowed\": True})\n",
    "class WorkExperience(Experience):\n",
    "    company: str\n",
    "    job_title: str\n",
    "\n",
    "\n",
    "@dataclass(config={\"arbitrary_types_allowed\": True})\n",
    "class Resume(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    linkedin_url: Optional[str]\n",
    "    email_address: Optional[str]\n",
    "    nationality: Optional[str]\n",
    "    skill: Optional[str]\n",
    "    study: Optional[Study]\n",
    "    work_experience: Optional[WorkExperience]\n",
    "    hobby: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the create_extraction_chain_pydantic() function in LangChain, \n",
    "we can provide a schema as input and receive an instantiated object that \n",
    "adheres to it. \n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file_path = 'CV.pdf'\n",
    "pdf_loader = PyPDFLoader(pdf_file_path)\n",
    "docs = pdf_loader.load_and_split()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_extraction_chain_pydantic(pydantic_schema=Resume, llm=llm)\n",
    "chain.run(docs)\n",
    "\"\"\"\n",
    "The result is imperfect - only one experience is parsed. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Answering questions with tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import (AgentExecutor, AgentType, initialize_agent,\n",
    "                              load_tools)\n",
    "import streamlit as st\n",
    "from config import set_environment\n",
    "set_environment()\n",
    "\n",
    "\n",
    "# LANGCHAIN ####################################################################\n",
    "\n",
    "def load_agent() -> AgentExecutor:\n",
    "    llm = ChatOpenAI(temperature=0, streaming=True)\n",
    "    # DuckDuckGoSearchRun, wolfram alpha, arxiv search, wikipedia\n",
    "    tools = load_tools(tool_names=['ddg-search', 'wolfram-alpha', 'arxiv',\n",
    "                                   'wikipedia'],\n",
    "                       llm=llm)\n",
    "    return initialize_agent(tools=tools, llm=llm,\n",
    "                            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "\n",
    "chain = load_agent()\n",
    "# STREAMLIT ####################################################################\n",
    "\n",
    "st_callback = StreamlitCallbackHandler(st.container())\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "    st.chat_message('user').write(prompt)\n",
    "\n",
    "    with st.chat_message('assistant'):\n",
    "        st_callback = StreamlitCallbackHandler(st.container())\n",
    "        response = chain.run(prompt, callbacks=[st_callback])\n",
    "        st.write(response)\n",
    "\n",
    "# conda activate chatbot_env\n",
    "# streamlit run Information_retrieval_with_tools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploring reasoning strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building a Chatbot like ChatGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is a chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding retrieval and vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.009159999120041161, 0.006975482333563724, -0.006220088443420031, -0.008765288773894297, -0.02689473748072997, 0.027983593286885947, -0.012930162465271547, -0.004750133105109466, -0.027071675734323054, -0.02719417259459222, 0.010575511668043928, 0.01999411392563181, 0.0032801779996295484, -0.006243907018660538, 0.0038382164838691615, 0.0014886699165116318, 0.020524930549056228, -0.003770163112399737, 0.02312457413549959, -0.017775569405681747, 0.00031240803008799706, -0.0032376444969420036, 0.005658647139267026, 0.012106714970827782, -0.003014769455086692, 0.0004525556653297458, 0.007948647384938612, -0.022879582277606437, -0.010874946781906173, -0.0031900071136303416, 0.00272554210247268, 0.0023903786600883788, -0.015393697911792173, -0.034952268583639, -0.012603505723424753, 0.005876418300498221, 0.0017140971315096495, -0.005066582085708025, 0.023219848436461618, 0.002017785828471297, 0.03263844976272691, 0.011984219041881206, -0.002211738181631337, -0.014998987565645307, 0.01524398035486105, 0.00017289837992426284, 0.019898839624669783, -0.01377402548221178, -0.006801945735730799, -0.005648439378018792, 0.025424782608080704, 0.007941841279450532, -0.0503867984717467, 0.00713881070448712, 0.0029365078567077383, -0.005774338359709408, 0.004974709906167446, 0.03593223974050197, -0.004195497440783245, 0.002366559852017224, -0.00502234705664846, 0.0057777409467921525, -0.010051499287462408, -0.007159226692644883, -0.0038314110768730247, -0.008282109300950894, 0.0002834852831850635, -0.0005933413147533402, 0.01904136532807598, 0.009990251788650416, 0.0162920041847015, 0.03296510929854147, -0.013685555424092648, -0.01168478299669637, 0.0072272798312836605, 0.003171292419013951, -0.011201603523752966, 0.010194411670228042, 0.011126744745287404, 0.005716492516657569, 0.00918041464253763, -0.03816439274613783, -0.027139729338623124, 0.009942613706846811, 0.0053115744092624715, 0.010487041609924799, 0.0038280084897902803, 0.04467030688641172, -0.015543415468723294, -0.002960326664778893, 0.01634644744067059, 0.02709889829363019, 0.02614614783342918, 0.006032941497256128, -0.017013371272695154, 0.024798689821049075, -0.011017859164671807, 0.03816439274613783, -0.010459820913262842, -0.02079714496625652, -0.019694678811769565, 0.018292775680775187, -0.005257131618954673, -0.0019054975443576312, -0.035278928119453566, 0.009500266210218933, -0.005883223940325005, -0.020375212992125108, 0.015216758726876502, -0.003103238814713879, -0.0024839521331703304, 0.011330905558187622, 0.010003862136981394, -0.03097794628815358, 0.0054068491758857955, 0.0037735656994824815, -0.001596704850356543, -0.03846382786000008, -0.010861336433575196, -0.011650756194546334, 0.021246297637049886, 0.019340800441938227, 0.013236402753299282, -0.02172267286715039, 0.013079880022202669, 0.012419760433021005, -0.025533667257373712, -0.03669443787348856, 0.009228052724341233, -0.013719582226242685, 0.0242542609866485, 0.02795637072757881, 0.01099744271085275, 0.01841527254104435, -0.03179458953975445, 0.023573726806292956, -0.028990785140410872, 0.01706781452866425, -0.01981717380939355, -0.014223178153005146, -0.0019003935473181906, 0.02468980330911089, -0.0057437141446421165, -0.021341571938011915, -0.0024363147498586684, 0.03171292372447822, 0.014250399780989693, -0.009071529061922032, -0.006077176060654398, -0.022103771933643688, 0.015733965001969944, -0.007642406165588287, 0.008288914475116382, -0.003705512328013057, 0.023587337154623935, 0.014998987565645307, -0.004508543368637764, -0.006744100358340256, -0.02907244909304192, -0.009418602257587881, -0.0057777409467921525, 0.021450458449950104, 0.022893192625937416, -0.003487741166781862, 0.0033193086824037017, 0.028065257239517, 0.023669001107254985, 0.0037973845075536363, 0.0023716639654719884, -0.008193639242831764, -0.013964575015458426, 0.018728318003237578, -0.00424313459126426, 0.01012635806592797, -0.0012989709136203462, 0.006410638442327974, 0.006859791113121343, -0.015543415468723294, -0.013529032692996037, -0.013222792404968303, -0.021450458449950104, 0.019177472536676125, 0.001069290348353574, 0.026949180736699066, -0.01427762140897424, -0.0252750631885044, -0.006393625041252957, -0.01254906246745566, 0.005376225426479799, -0.018251944635782252, 0.0025469016240156385, 0.026200591089398272, 0.0008932019848315762, -0.0047807573201767576, -0.6986098405261887, -0.028718570723210584, -0.0023631572649344796, -0.017857235220957977, 0.031059610240784633, 0.01729919603822642, 0.007724070583880633, 0.004226121190189242, -0.0002460558648484519, 0.02007577787826286, -0.020320769736156014, 0.024458421799548713, -0.009670400220969114, -0.010412182831459237, -0.00883334237819437, -0.02684029422476088, -0.014250399780989693, -0.012569478921274718, 0.00726811227326048, 0.017966119870250985, -0.0019565376311673614, 0.0034741303527895885, 0.010221633298212589, 0.0095138774898725, 0.007315749423741495, -0.0007251949402540825, 0.003501351747943488, -0.017707515801381673, 0.001800014667240102, 0.0028735583658624306, -0.024907574470342083, 0.0016826223860869956, 0.002084137906399349, -0.0015354567694679314, 0.028500799561979387, -0.01565230104933889, -0.011011053990506317, -0.006982287507729213, 0.007860177326819482, 0.024948407377980198, -0.021858780075750536, -0.006009122456354326, 0.01377402548221178, -0.016741156855494866, -0.013426952286545928, 0.006543342598184079, 0.008622376391128663, 0.004229524242933281, 0.0027357500965515614, 0.00917360946837214, -0.010793282829275124, 0.015570637096707841, 0.0063051554487951215, -0.017516967199457615, 0.01523037000653007, 0.004049182470934868, 0.03029741210779804, -0.012106714970827782, 0.016645882554532837, 0.00046659171539586953, -0.00028157129884718874, 0.007778513374188432, -0.02485313307701817, -0.015352865935476646, -0.031903472326402275, 0.004614026362170617, 0.010160384868078007, 0.017857235220957977, 0.026813073528098922, -0.016414501044970664, 0.02475785691341096, 0.011575897416080772, -0.00034048010111641347, -0.0003745068450587877, -0.0008234471451977936, -0.008976254760960003, 0.026853904573091857, -0.003964115465559777, -0.016931707320064104, 0.010527873586240324, 0.015325644307492099, -0.012909746011452488, -0.025138957842549436, -0.00011122491016668848, 0.018374441496051417, -0.022539314256106075, -0.03685776577875066, -0.00013206628719341382, 0.01288932955763343, 0.01211352014499327, 0.010602733296028476, 0.0030760174195599795, -0.00580155952203266, -0.009684010569300091, -0.010453014807774762, 0.0070979782625103, -0.0033448287840162285, 0.0007460363100048501, 0.026540859110898634, -0.034081183938714225, -0.01680921045979494, -0.010514263237909346, 0.015992569070839253, 0.01039176730896277, 0.034952268583639, -0.006444664778816715, -0.004205705202031479, 0.023845941223493247, 0.0219948854217055, -0.027561661312754535, -0.0030675107190224708, -0.0004878584085319802, -0.006417443616493464, -0.0029245985690874846, -0.021681839959512277, -0.03081461838289148, -0.008234471219147289, 0.0003096433407717742, -0.0009833728126231215, 0.0037667602924863447, 0.014753995707752156, 0.0038143979086286543, 0.03130460209867778, -0.014087070944405002, 0.002211738181631337, 0.001150954533815272, -0.005679063127424789, -0.007152421052818099, 0.002339338456863325, 0.008186834068666275, -0.000469994389790107, 0.006152034839119959, 0.03141348861061597, -0.008057532965554209, 0.016210340232070446, -0.013066268742549101, -0.009935808532681322, 0.0024346134563172964, 0.00909194551574109, -0.011807279856965535, 0.0010191009083145297, -0.0006180107113875192, -0.006179256467104506, -0.006158840478946744, 0.0025247843423165032, -0.01124924067423398, -0.03160403721254003, 0.0103781560293092, -0.023832330875162268, 0.0002990099942037189, -0.021504901705919195, 0.004144457237558191, 0.0015711848651593398, 0.017135868132964322, -0.02131435124134996, -0.008853757900690836, -0.00020479844873225978, -0.028881898628472683, -0.008547517612663102, -0.011643951020380844, 0.007540326224799474, 0.027058065385992074, -0.010541484865893892, 0.0012955682101222778, -0.011984219041881206, -0.0021692049117744395, 0.00034175610037627367, 0.012943772813602525, -0.014332063733620745, -0.026962791085030045, -0.0036612775317841394, -0.012412955258855517, 0.010977027188356282, 0.010194411670228042, 0.005314976996345216, -0.006907428729263652, -0.03840938646667616, 0.0013491603536593905, 0.020130221134231956, -0.025846712719566935, 0.006631812190641914, 0.010854530328087116, 0.006124813676796708, -0.001613718251431561, 0.02191322146907445, 0.011330905558187622, 0.006247310071404578, 0.029507991415504312, -0.0283919130500412, 0.013460979088695965, 0.011841306659115572, 0.018211111728144137, -0.018755540562544713, 0.011358126254849579, -0.00692444213033867, 0.013127517172683683, -0.0034401035506395525, 0.015978958722508273, 0.0008132391511189123, 0.026268644693698346, 0.025737828070273926, 0.023954825872786252, 0.010813698351771591, -0.026268644693698346, -0.008880979528675384, -0.0332373199904514, -0.0169861505760332, -0.03582335322856378, 0.030270189548490905, 0.02120546659205695, 0.011528260265599758, -0.023247070995768754, -0.0001827874105679678, 0.011643951020380844, 0.006560355999259097, 0.007812540176338468, -0.013488200716680512, 0.0027119312884804066, -0.006910831316346397, 0.001993967020400142, 0.016591439298563743, 0.005628022924199735, 0.03484338579699117, -0.013692360598258138, -0.011324099452699542, 0.0031508764308561883, 0.009813312603734746, 0.0252750631885044, 0.012140741772977817, -0.006941455531413688, -0.008976254760960003, 0.0033992715743240273, 0.018197501379813158, 0.012950577987768015, 0.013175154323164698, 0.018292775680775187, 0.03217568674360256, -0.01325001310163026, 0.029916311178659564, -0.01229045932990894, -0.012576284095440208, 0.025343116792804474, 0.015557026748376864, -0.02709889829363019, 0.01912302928070703, -0.009752064173600163, 0.018809983818513808, 0.027929150030916852, 0.0072953334355837325, 0.031740144421140176, -0.02139601519398101, 0.005924055916640531, -0.0005082744840012356, 0.007070757100187048, 0.012834887232986928, -0.02937188420690417, -0.018170280683151203, 0.003535378550093524, 0.01341334193821495, 0.01307307391671459, 0.015339255587145668, 0.011650756194546334, 0.0174761342918195, -0.0059716930671215454, 0.0202663283428321, -0.00069754822171484, 0.009336938304956832, 0.009840534231719293, -0.0095138774898725, -0.007172837040975861, -0.01931357974527627, -0.027847486078285803, 0.00043554229335855877, -0.0202663283428321, 0.017353639294195516, 0.0010837516927012097, 0.006781529747573037, 0.015121484425914473, 0.019027754979745, 0.012671558396402237, -0.02138240484565003, -0.01393735338747388, 0.011725614973011896, 0.010888557130237153, 0.006594382801409134, -0.012133936598812328, -0.014046238968089477, 0.007220474657118171, -0.011398958231165103, 0.005740311557559372, 0.00796906290743508, 0.023941215524455276, 0.007587963375280489, -0.011848111833281062, -0.0014614485213577323, 0.0103781560293092, 0.0013040749106597868, -0.003038588030327199, -0.007288528261418243, -0.01168478299669637, -0.007621990177430525, 0.0035149625619357613, -0.005978498706948329, 0.009343743479122322, 0.02086519857055659, -0.009561514640353515, 0.0011790265757398576, -0.030651288614984198, 0.003848424477948043, -0.008901395982494441, -0.0008468405716759434, -0.0022270502891649827, -0.0036442641307091214, 0.003227436502863122, 0.002519680228861739, 0.00309473211417637, -0.0012462294168539198, 0.021042136824149668, 0.020034944970624746, 0.0076628221537460494, 0.0009051113306594917, -0.027779432473985732, -0.020906029615549524, 0.017598631152088668, 0.0411859683067126, 0.017190309526288233, 0.005097205835114021, -0.007125199890494847, -0.014998987565645307, -0.00033346206153143617, -0.03555113881136349, -0.00035515410715672083, 0.020565763456694343, 0.00021181647376532163, -0.008159612440681727, -0.0038790486930153343, 0.010337324052993676, -0.005389835774810777, 0.024635361915786975, -0.0019327189395115306, -0.00960234661666904, -0.01159631386989983, 0.0009187220282364414, 0.0015116380778121003, -0.008867369180344406, -0.0045153485428032535, -0.004927072290025136, 0.00038046151797274544, 0.00701631430987925, 0.0020756312058618403, 0.03125016070535387, -0.005645036325274753, 0.006009122456354326, -0.024730636216749004, -0.019939670669662717, 0.02189961112074347, 0.012569478921274718, 0.006373208587433898, 0.02703084468933012, 0.04736522663646229, 0.009459434233903408, -0.026336698297998416, 0.007234085471110444, -0.0004504289901953686, -0.0022491675708641175, 0.016237560928732405, 0.00039662421406941545, 0.006046552311248401, 0.004403060375104911, -0.005767532719882624, -0.012521840839471114, 0.011208408697918455, 0.001774494565627575, -0.010276076554181683, 0.00322573520932175, -0.012678364501890315, -0.01789806626595091, -0.0074586618065071286, 0.006097592048812161, 0.014427338965905364, -0.007349776225891531, -0.0046854825535534335, -0.025411170397104544, -0.04883518150911156, -0.00467867691372665, 0.0010242049053539705, 0.0051108166491062946, 0.004467711392322239, -0.011419374684984161, -0.04058709807898811, -0.02140962554231199, -0.0166186599952257, -0.016414501044970664, 0.016999760924364175, -0.0013108802012405999, -0.002470341552008705, -0.026772240620460807, 0.003981128866634795, 0.026785850968791786, 0.00713881070448712, -0.0004453249931559279, 0.014318453385289765, 0.004726314529868959, 0.01619672988373947, -0.010956610734537225, -0.02188600077241249, 0.0023087144746266807, -0.017108645573657183, -0.022199046234605717, -0.00036642546790662174, 0.012780443977017834, 0.025955599231505123, -0.007717264944053849, 0.0007337016407915915, -0.013066268742549101, -0.0046514557514033975, 0.014509002918536414, -0.01637366813733255, 0.0009782688155836807, 0.01679560011146396, 0.019803563461062573, 0.009765675453253731, -0.0007549683339277022, 0.002426106755779787, 0.008105170116035223, -0.018224723939120294, -0.0032121243953294764, 0.002866752958866294, -0.0005418758463506049, -0.013617501819792576, 0.016673103251194792, -0.004954293452348387, -0.010677592074494037, -0.010847725153921626, -0.002528186929399248, -0.023941215524455276, 0.004096819621415882, -0.008846952726525349, 0.03359120022292792, -0.008077948488050678, -0.01489010198502971, 0.017639464059726783, -0.013821662632692794, -0.013284039903780296, -0.004910058888950118, -0.005134635224346802, 0.040832091799526446, 0.014699552451783062, 0.0003468600974157143, 0.019681068463438586, -0.011235630325903002, -0.004869226912634593, -0.003298892694245939, -0.0063391822509451575, -0.009908586904696775, 0.004161470638633209, -0.020293549039494055, -0.0115078438117807, -0.02343761959769281, -0.004096819621415882, -0.00926888470065676, 0.032937884876589156, -0.00027348995079885374, -0.020416045899763223, -0.015461751516092245, -0.00726811227326048, 0.009581930162849982, -0.010568706493878439, -0.022171825537943758, -0.01636005778900157, -0.00021862182255379646, -0.00335503677809511, -0.009023891911441017, 0.0038314110768730247, 0.007989479361254137, -0.003783773693561363, 0.005383030600645288, -0.030841839079553436, -0.006363000826185665, -0.01574757535030092, -0.015802018606270014, 0.013794441004708247, 0.026050873532467152, 0.01367194507576167, 0.03247512185746481, 0.0011518051805859582, -0.002747659384171815, 0.013433757460711418, -0.008159612440681727, 0.017448913595157545, 0.01740808068751943, -0.011705199450515428, 0.001272600165237133, -0.023546506109631, 0.02413176598902451, -0.010983832362521772, 0.010364545680978223, -0.006220088443420031, 0.01905497567640696, 0.012712391304040352, -0.007023119484044739, 0.009704427023119149, -0.02033438194713217, -0.01550258349240777, -0.011065496315152822, 0.012576284095440208, -0.004420073776179929, -0.000664372124543152, -0.030515183269029234, -0.02293402367093035, 0.019000532420437866, -0.0027527634976265794, 0.007485883434491675, -0.00986094975421576, 0.009976640508996848, -0.016047012326808347, 0.00016109535039076763, -0.0013636216980070263, 0.0054749027801858675, -0.012943772813602525, 0.006720281783099749, -0.00471610630295943, -0.010840919979756138, 0.0007813390823109154, 0.009316521851137774, 0.024104543429717375, 0.00032197803908886376, 0.018986922072106886, 0.016428111393301643, 0.0061418270778717254, -0.005774338359709408, 0.006049954898331146, 0.006080578647737142, 0.002958625138406873, 0.006682852393866968, -0.017639464059726783, -0.013100295544699136, 0.003312503275407565, 0.00657396681325137, 0.026377531205636535, -0.00987456103386933, 0.01489010198502971, -0.025683384814304832, -0.027289446895554247, 0.0018323400594334421, -0.011575897416080772, 0.04946127243349801, 0.004842005284650046, 0.010704812771155994, 0.005934263677888765, -0.012011439738543163, -0.0053796280135625435, 0.014767606056083134, -0.021450458449950104, -0.011732421078499975, 0.03285622278660329, 0.033210099293789444, -0.01584285151390813, -0.02571060551096679, -0.004937280516934665, 0.011228825151737513, 0.0029892493534741647, -0.02734389015152334, 0.007125199890494847, 0.01668671359952577, -0.001159461234352781, -0.006250712658487323, -0.011358126254849579, -0.045650278043274685, 0.01668671359952577, -0.013495005890846, 0.024989240285618313, 0.0015226967186616678, 0.0025043681213280933, -0.027262226198892292, -0.003960712878477032, -0.03813717204947587, 0.026078096091774288, -0.0013100295544699137, -0.017135868132964322, -0.013876105888661887, -4.466009778638726e-05, -0.005787948708040386, 0.006465081232635774, 0.005454486792028105, 0.016768379414802, -0.0029041823480990747, -0.0007405069895800664, 0.0347889406783769, -0.0024686402584673324, 0.00925527342100319, 0.023396788552699877, 0.008091558836381655, 0.02546561365307364, -0.014481781290551866, -0.023315124600068828, -0.00562462033711699, 0.002215141001544729, 0.018319998240082323, -0.006815556549723073, -0.003508157154939625, -0.011589508695734342, -0.019395243697907318, 0.006053357485413891, 0.022552924604437054, -0.00796225773326959, -0.01004469411329692, 0.0029075851680124666, -0.010677592074494037, -0.010364545680978223, -0.020443266596425178, 0.016223950580401426, 0.01437289570993627, -0.014509002918536414, -0.019531350906507462, -0.001976953619325124, 0.007023119484044739, 0.033019550691865386, 0.007036730298037012, -0.01818389103148218, 0.02734389015152334, 0.02147767914661206, -0.029018005837072827, -0.005706284755409336, -0.002022889709095414, 0.001248781473581302, -0.024403980406224803, 0.0027374513900929338, -0.010276076554181683, -0.0021147621214666406, 0.014127902920720529, -0.006829167363715346, -0.0074654674463339125, -0.037538301821751385, 0.017789181616657906, 0.015448141167761265, -0.0026796060127023905, -0.008343356799762886, 0.001491221973239014, 0.0202663283428321, 0.017462523943488524, 0.014985377217314329, -0.0008795912872546265, 0.02017105217922489, -0.026690576667829757, -0.00788739895480403, -0.0027935954739421046, -0.01524398035486105, 0.01729919603822642, 0.0007583710374257705, -0.007846566978488503, 0.001774494565627575, -0.02309735343883763, -0.01750335685112664, -0.017857235220957977, 0.03840938646667616, 0.0007039282471179719, -0.001053127710464566, -0.018646654050606525, 0.00883334237819437, -0.014236788501336126, 0.022539314256106075, 0.006083981700481182, -0.004794368134169031, 0.021926831817405425, 0.01185491700744655, -0.016251171277063384, -0.0017489745804303717, -0.00874487232007524, -0.01427762140897424, -0.03517004160751538, -0.019395243697907318, -0.01999411392563181, -0.009854144580050272, -0.005961485305873312, 0.010943000386206245, -0.0019497323405865486, -0.017952509521920006, -0.04284647550657629, -0.012998215138249029, -0.03092350303218449, -0.01739447033918845, -0.015175926750560977, -0.003770163112399737, 0.015216758726876502, -0.0005210344765998373, -0.006087384287563927, 0.02372344436322408, 0.003770163112399737, 0.005362614612487526, -0.04140374133058898, -0.006431054430485737, 0.007322555063568279, -0.011051885966821844, 0.012487814037321077, 0.025846712719566935, -0.006883610154023145, -0.023546506109631, 0.003909672675251979, -0.009064723887756542, 0.00502234705664846, 0.03451672626117661, 0.017857235220957977, -0.019776342764400615, -0.017557798244450553, -0.007213669482952682, -0.009098750689906579, -0.005539553797403195, 0.0032444499039381404, -0.0393076918082629, 0.006659033352965166, 0.004062792819265846, -0.005934263677888765, -0.006311960622960611, 0.023845941223493247, 0.018469715797013446, -0.013991796643442973, -0.014468170942220889, -0.03732053252316537, -0.023342345296730783, 0.0007570950090620794, -0.004855616098642319, 0.014440949314236342, 0.005917250276813746, -0.0012606907612015556, 0.03696665229068885, -0.008635986739459643, -0.0037565522984074635, 0.0038960620940903523, 0.011405764336653183, -0.015094262797929926, 0.0027782835992391066, -0.015039820473283423, -0.023696223666562124, -0.018850814863506742, -0.028609684211272395, -0.010786477655109634, -0.014509002918536414, -0.00805072686006613, -0.0017966119637420337, 0.0046174289492533615, -0.028337471656717288, 0.006053357485413891, -0.0003423940854542883, -0.029018005837072827, 0.0010505756537371837, -0.011446596312968708, -0.00918041464253763, -0.0034486102511770617, 0.0028038034680209862, -0.01359708629729611, -0.035415031602763346, -0.003783773693561363, 0.02060659450168728, -0.019490517998869347, -0.022552924604437054, -0.0003081546798192425, -0.008152807266516238, -0.005587190947884209, 0.019422464394569277, 0.21406904217703862, 0.021613786355212203, 0.003411180861944281, 0.02847357700267225, -0.009806507429569258, 0.04249259527409977, -0.005920653329557786, -0.002194724780556319, -0.018823594166844787, 0.007111589076502573, -0.0025656163186320285, -0.0010582317075040065, 0.009534293012368968, -0.0007660270329849315, 0.020620204850018257, -0.012542257293290171, -0.012909746011452488, -0.030324632804459996, -0.031222938146046732, -0.03623167485436422, 0.016945317668395084, 0.0023767678460961056, 0.008370578427747433, -0.011222019977572023, 0.02078353461792554, -0.002349546450942206, -0.00037004083309736163, -0.004869226912634593, 0.013284039903780296, 0.010173996147731575, -0.009159999120041161, 0.003671485525863021, 0.027166950035285083, 0.004035571656942594, -0.00018129873506352062, 0.0015677821616612714, -0.0005720746216172296, 0.002284895666555526, 0.018837204515175763, 0.003445207664094317, 0.01565230104933889, -0.022103771933643688, 0.023900384479462338, -0.021014916127487713, 0.011439791138803218, 0.003566002532330168, -0.014604278150821033, -0.011888943809596587, 0.005798156934949915, 0.014522613266867393, -0.026622523063529684, 0.010371350855143712, 0.01567952360864603, 0.019885227413693623, 0.0031406682039466596, -0.013202375951149245, 0.027044455037661095, -0.009466239408068896, 0.003525170556014643, -0.006774724573407547, -0.004215913428941008, 0.022022107981012635, -0.016564218601901787, 0.022825139021637342, 0.0020705273252377234, 0.014168735828358644, -0.017530577547788594, -0.00030921801738643115, -0.003600029334480204, 0.006699865329280691, 0.01706781452866425, 0.005223104816804637, -0.007322555063568279, 0.0004534063412042629, -0.028800234675841634, -0.011194798349587478, 0.03345509301432777, 0.006645422538972892, 0.013045853220052634, 0.0018340413529748143, -0.006832569950798091, 0.011228825151737513, -0.01616950732443233, -6.608631629583252e-05, -0.028337471656717288, -0.02988909048199761, 0.00010670574005805228, -0.014400117337920817, -0.018333608588413302, -0.019163860325699965, 0.004209107789114224, 0.005468097606020379, -0.015107873146260905, -0.016605049646894722, 0.015965346511532117, 0.013699166703746218, 0.0010973623902781596, 0.026255034345367367, -0.02085158822222561, -0.01627839383637052, -0.021259907985380865, 0.0597237395706713, 0.0010820503991598375, 0.011895748983762076, -0.02364178041059303, 0.008527101158844044, 0.006723684370182493, 0.011548676719418815, -0.0017200517753197763, -0.015216758726876502, 0.016428111393301643, -0.019259136489307174, -0.004607220722343833, -0.00952748783820348, -0.013916937864977412, 0.006148632252037215, -0.014468170942220889, -0.014781216404414113, 0.0011679679348902902, -0.004675274326643905, 0.008363773253581943, -0.012338096480389955, 0.002633669922932101, 0.011528260265599758, -0.006077176060654398, -0.03620445415770226, -0.015475362795745812, -0.014155124548705074, -0.014998987565645307, -0.03808273065615196, 0.019694678811769565, -0.005835586324182696, 0.012862108860971474, -0.0030232761556242007, -0.00813239174401977, 0.023383176341723717, 0.005471500193103123, 0.0017123958379682773, -0.02762971491705461, 0.006471886406801262, 0.0344622848678527, 0.006798543148648055, -0.005056373858798496, 0.004317993369729821, 0.013726387400408175, -0.017040591969357113, 0.00874487232007524, -0.008717651623413283, -0.013624307925280656, -0.011392153056999614, -0.005825378097273167, -0.00047041971317545003, 0.020375212992125108, -0.0017898066731612207, 0.04064153947231203, -0.007132005064660336, -0.019286357185969133, -0.03100516698481554, -0.004226121190189242, 0.005301366648014238, -0.038681600883876455, 0.0002443545422032487, 0.04407143665868723, -0.010031083764965941, -0.014345674081951722, -0.007696848955896086, -0.17889899311894253, 0.021736283215481367, 0.012358512934209013, -0.025996430276498058, 0.006488899807876281, 0.010242049752031647, 0.039062701813014925, -0.01540730919144574, 0.008731261971744262, -0.0013789336891253483, 0.0025162776417789945, -0.016496164997601714, -0.03236623534552662, -0.0037157203220919382, -0.012358512934209013, -0.008288914475116382, -0.016918096971733125, 0.0008949033365806103, 0.02579227132624302, 0.007152421052818099, 0.0195721819515004, -0.005815170336024933, 0.01341334193821495, -0.002677904719161018, 0.016237560928732405, 0.01176644788065001, -0.0014376298297019015, 0.022620978208737128, -0.0016383875898580782, -0.01705420231768809, 0.007111589076502573, 0.0006018480152908492, -0.0003940722155496951, 0.00701631430987925, -0.013100295544699136, 0.013168349148999208, 0.015638690701007915, -0.00549531876834363, -0.008758483599728807, 0.01316154397483372, 0.030433517453753004, 0.016645882554532837, 0.011031469513002786, 0.017530577547788594, -0.004736522291117193, 0.03481616137503886, 0.03489782719031509, -0.0005554865439275546, 0.016836431156456895, 0.011119939571121916, 0.021164633684418836, -0.03549669741803958, -0.0020126817150165326, -0.017271973478919286, 0.008772093948059787, 0.019980503577300832, -0.01608784337180128, 0.007996284535419626, -0.005522540396328177, -0.010493847715412877, -0.005138038277090842, -0.03579613253190182, 0.010643565272344, -0.012508230491140136, -0.0084930752880166, -0.011045080792656354, -0.034489505564514654, 0.026241423997036387, 0.00028327262149239194, -0.0019259136489307176, -0.002762971491705461, -0.01690448476075697, -0.011643951020380844, 0.012916551185617978, 0.010405377657293748, 0.013862494609008319, -0.018483326145344425, 0.024635361915786975, -0.008472658834197542, 0.0012666455214270062, -0.0005354958791551351, 0.02814692119214805, -0.004379241799864404, 0.02085158822222561, 0.009125972317891126, 0.0016834730328576819, 0.0026081498213195737, -0.004042376831108083, -0.009813312603734746, -0.02441759075455578, 0.026268644693698346, -0.020906029615549524, -0.021178244032749812, -0.02131435124134996, 0.010514263237909346, 0.015380087563461193, 0.006488899807876281, -0.0031525777243975608, 0.00614522966495447, 0.003960712878477032, -0.009820117777900235, -0.0045834021471033255, -0.03465283346977676, 0.03895381530107674, 0.02535672900378063, 0.0033856607603317537, 0.009813312603734746, 0.009023891911441017, 0.030106861643228802, -0.005556567198478213, -0.026445584809936605, 0.018251944635782252, -0.005417057402795324, 0.019163860325699965, 0.01904136532807598, 0.02017105217922489, 0.00259624053369932, -0.012875719209302453, 0.006260920419735557, 0.008710846449247793, 0.044506978981149616, -0.005114219236189039, 0.0021198660020907575, 0.0074586618065071286, 0.014005406991773953, 0.0035115597420223694, -0.09070168585882488, -0.0029637292518616375, 0.034380619052576465, 0.02008938822659384, -0.013338483159749389, 0.029235776998304024, -0.006842777712046324, 0.02918133374233493, -0.023941215524455276, 0.01781640231331986, -0.0076628221537460494, -0.011616729392396299, -0.009439018711406939, -0.009507071384384422, -0.005389835774810777, 0.009894976556365797, 0.009847339405884782, -0.018319998240082323, -0.020388823340456084, 0.04145818272391289, 0.006767918933580763, -0.004876032086800082, 0.01220879537727789, -0.004950890865265643, -0.0033108019818661925, -0.009207636270522176, -0.02405010203639346, 0.003372050179170128, 0.016428111393301643, 0.00679514056156531, -0.006975482333563724, -0.029290220254273115, -0.0027833874798632234, -0.006465081232635774, 0.0074246350043570926, -0.012052271714858688, -0.0458680473418607, -0.008268498021297324, 0.027806653170647688, -0.007914620582788575, 0.015107873146260905, 0.005481707954351356, 0.012351707760043523, -0.03157681651587807, 0.0060397466714216174, -0.007186447854968135, -0.00046574103952135243, 0.022580145301099013, -0.002164100798319675, -0.01565230104933889, -0.0032801779996295484, 0.004038974244025339, -0.02703084468933012, -0.006158840478946744, 0.010425794111112805, 0.0037735656994824815, 0.013705971877911706, -0.008527101158844044, -0.014005406991773953, 0.003014769455086692, -0.011235630325903002, -0.008676819647097758, -0.01818389103148218, 0.012487814037321077, -0.003184903000175577, 0.010146774519747027, -0.028283028400748193, -0.005815170336024933, 0.003719122909174683, -0.013277234729614807, 0.0036816935199419022, 0.023260681344099733, -0.010405377657293748, 0.025397560048773565, -0.01636005778900157, 0.007778513374188432, -0.013835272981023772, -0.037946623447551814, 0.0075267154108072006, -0.022825139021637342, -0.010003862136981394, -0.010194411670228042, -0.018319998240082323, -0.02493479702964922, 0.020320769736156014, 0.02302929983453756, 0.012834887232986928, -0.01954496125483844, 0.00505977644588124, -0.043418121312348464, 0.017108645573657183, 0.01029649207667815, 0.0052435208049623995, -0.02650002620326052, -0.0283919130500412, -0.002431210636403904, 0.021668229611181297, -0.0093573538274533, 0.021926831817405425, -0.010983832362521772, -0.033019550691865386, -0.010160384868078007, -0.06533134836935846, 0.006260920419735557, -0.00039152018792614385, -0.02641836225062947, -0.00788739895480403, 0.0011075705007723645, 0.001709843781240895, 0.004447295404164477, -0.01203185619236222, 0.020320769736156014, -0.022444039955144046, 0.02632308794966744, -0.01644172174163262, -0.00406959845909263, -0.022049328677674594, 0.008561127960994081, 0.02477146912438712, 0.0032767751797161565, 0.013032241940399064, 0.006686254980949713, -0.005668855366176555, -0.023927605176124297, 0.013978185363789405, 0.01126285195388755, -0.025016460982280272, 0.0034179862689404177, -0.028201364448117144, -0.0051788702534063666, -0.02598281992816708, -0.013896521411158354, 0.02207655123698173, -0.023478452505330927, 0.0004848810866269168, 0.021613786355212203, -0.008690429995428735, -0.01972189950843152, -0.0034094795684029085, 0.01861943335394457, 0.01972189950843152, -0.013991796643442973, -0.008418215578228447, -0.04295535829322412, 0.008016700057916094, -0.0059376667306328046, -0.010180801321897064, 0.03212124535027865, -0.010772866375456066, 0.008989865109290982, 0.021940444028381585, 0.0036272507296341034, 0.01668671359952577, 0.013950964667127448, -0.01723114243392635, -0.014127902920720529, 0.012440176886840062, -0.02413176598902451, -0.0006090786874646671, 0.00038960620358826904, 0.0032767751797161565, 0.0017778972691256433, 0.04015155575652572, 0.01063675916685592, 0.017271973478919286, 0.005821975510190422, 0.004386046974029893, 0.006934649891586904, 0.00022202449694803389, 0.01090897358405621, 0.027588882009416494, -0.028800234675841634, -0.015475362795745812, -0.029480768856197173, 0.00437583921278166, 0.008969449586794513, 0.03097794628815358, -0.003264865892095903, 0.008513490810513067, 0.001491221973239014, -0.0013942457966589939, 0.018456105448682467, 0.03255678767274104, -0.0007796377305618812, -0.025084514586580342, 0.013991796643442973, 0.03154959581921612, 0.029507991415504312, -0.008601960868632196, 0.005638231151109264, -0.001605211550894052, 0.008935422784644478, -0.009649983767150054, -0.0014282724823937063, -0.0103781560293092, -0.0063051554487951215, -0.02858246351461044, -0.009629568244653587, 0.003787176513474755, -0.01766668475638874, 0.013896521411158354, 0.03606834694910211, 0.014264010129320673, 0.013685555424092648, -0.011113134396956426, -0.025860324930543094, -0.01178005822898099, -0.0004908357595408745, -0.005665452779093811, -0.042274825975513755, -0.005600801761876483, 0.019000532420437866, -0.008860564006178916, 0.007308944249576006, -0.0038041896817191255, 0.021572955310219268, -0.010943000386206245, 0.011929775785912112, -0.016632272206201858, -0.014794827684067681, -0.022348763791536837, 0.024213429941655564, -0.016550608253570808, 0.004454100578329965, 0.02198127507337452, 0.020388823340456084, 0.01376041420255821, 0.02023910578352496, 0.002558810911635892, -0.004753535692192211, 0.024567308311486902, -0.005090400660948532, 0.01558424837636141, 0.001223261371968775, -0.005141440864173586, -0.010752450852959597, -0.015257591634514617, -0.018551379749644496, -0.01750335685112664, 0.004498335141728235, -0.03288344348326524, 0.06162923676578296, 0.0025911364202445557, -0.02131435124134996, 0.004896448074957844, -0.019422464394569277, 0.03340065162100386, 0.012617116071755733, 0.014304842105636198, -0.013719582226242685, -0.02735750049985432, 0.03364564161625183, 0.006699865329280691, 0.007240890645275933, -0.0438264429381489, -0.011800473751477457, -0.0005363465259258212, -0.011494233463449723, -0.007533520584972689, -0.006546745185266824, 0.0006503361035808592, 0.006917636490511886, 0.015733965001969944, 0.008302524823447361, -0.0011722212851590446, -0.014998987565645307, -0.0074926886086571646, 0.010180801321897064, -0.02033438194713217, 0.003241047084024748, -0.03359120022292792, 0.030161304899197897, -0.0027544647911679518, -0.019517738695531306, -0.02468980330911089, 0.016033400115832187, -0.010500652889578367, -0.017966119870250985, -0.007928230931119554, 0.017721128012357833, 0.010316908530497208, 0.01999411392563181, 0.05082234079420909, -0.0197355098567625, -0.001256437410932801, -0.0033958687544106353, -0.018769150910875693, -0.006424248790658953, -0.008697235169594225, -0.010201216844393532]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text = 'This is a sample query.'\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "print(query_result)\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "words = ['cat', 'dog', 'computer', 'animal']\n",
    "doc_vectors = embeddings.embed_documents(words)\n",
    "\n",
    "print(len(doc_vectors))\n",
    "print(len(doc_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docs = ['cat', 'dog', 'computer', 'animal']\n",
    "vectorestore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "# similar_vectors = vector_store.query(query_vector, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading and retrieving in LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Document loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(file_path='txt/husky.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(\"LangChain\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### kNN retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='dog', metadata={}), Document(page_content='animal', metadata={}), Document(page_content='cat', metadata={}), Document(page_content='computer', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import KNNRetriever\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "words = [\"cat\", \"dog\", \"computer\", \"animal\"]\n",
    "retriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())\n",
    "\n",
    "result = retriever.get_relevant_documents('dog')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### Custom retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document, BaseRetriever\n",
    "class MyRetriever(BaseRetriever):\n",
    "    def get_relevant_documents(self, query: str, **kwargs) -> list[Document]: \n",
    "        # Implement your retrieval logic here \n",
    "        # Retrieve and process documents based on the query \n",
    "        # Return a list of relevant documents \n",
    "        relevant_documents = [] \n",
    "        # Your retrieval logic goes hereâ€¦ \n",
    "        return relevant_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "## Implementing a chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\n",
      "I'm sorry, but as an AI, I don't have access to real-time information. I suggest checking a weather website or app for the most accurate and up-to-date weather forecast.\n",
      "[HumanMessage(content='Hi, how are you?', additional_kwargs={}, example=False), AIMessage(content=\"Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\", additional_kwargs={}, example=False), HumanMessage(content=\"What's the weather like today?\", additional_kwargs={}, example=False), AIMessage(content=\"I'm sorry, but as an AI, I don't have access to real-time information. I suggest checking a weather website or app for the most accurate and up-to-date weather forecast.\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Creating a conversation chain with memory\n",
    "memory = ConversationBufferMemory()\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=0, streaming=True)\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# User inputs a message\n",
    "user_input = 'Hi, how are you?'\n",
    "# Processing the user input in the conversation chain\n",
    "response = chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "# User inputs another message\n",
    "user_input = \"What's the weather like today?\"\n",
    "response = chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "print(memory.chat_memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "memory.save_context({'input': 'hi'}, {'output': 'whats up'})\n",
    "memory.save_context({'input': 'not much you'}, {'output': 'not much'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a \n",
    "human and an AI. The AI is talkative and provides lots of specific \n",
    "details from its context. If the AI does not know the answer to a \n",
    "question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template)\n",
    "conversation = ConversationChain(prompt=PROMPT, llm=llm, verbose=True,\n",
    "                      memory=ConversationBufferMemory(ai_prefix='AI Assistant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remembering conversation summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human greets the AI. The AI responds by asking what is going on.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the summary memory and the language model\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "# Save the context of an interaction\n",
    "memory.save_context({'input': 'hi'}, {'output': 'whatsup'})\n",
    "# Load the summarize memory\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing knowledge graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "memory = ConversationKGMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining several memory mechanisms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human\n",
      "and an AI. The AI is talkative and provides lots of specific details from its \n",
      "context. If the AI does not know the answer to a question, it truthfully says it\n",
      "does not know.\n",
      "Summary of conversation:\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello there! It's nice to meet you. My name is AI and I am an artificial intelligence designed to assist and communicate with humans. How can I help you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (ConversationBufferMemory, CombinedMemory,\n",
    "                              ConversationSummaryBufferMemory)\n",
    "\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "conv_memory = ConversationBufferMemory(memory_key='chat_history_lines',\n",
    "                                       input_key='input')\n",
    "summary_memory = ConversationSummaryBufferMemory(llm=llm, input_key='input')\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human\n",
    "and an AI. The AI is talkative and provides lots of specific details from its \n",
    "context. If the AI does not know the answer to a question, it truthfully says it\n",
    "does not know.\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=['history', 'input', 'chat_history_lines'],\n",
    "                        template=_DEFAULT_TEMPLATE)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True, memory=memory,\n",
    "                                 prompt=PROMPT)\n",
    "conversation.run('Hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Moderating responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"Step 1: Increasing Automation and AI Integration\\nThe future of programming is likely to see increasing levels of automation and integration of artificial intelligence (AI) technologies. As AI continues to advance, programming tasks that are repetitive or routine in nature can be automated, allowing programmers to focus on more complex and creative problem-solving.\\n\\nStep 2: Low-Code and No-Code Development\\nThe rise of low-code and no-code development platforms will also shape the future of programming. These platforms enable individuals with limited programming knowledge to create applications using visual interfaces and pre-built components. This trend will democratize software development, allowing non-programmers to participate in creating software solutions.\\n\\nStep 3: Increased Specialization and Domain-Specific Languages\\nProgramming is becoming increasingly specialized, with developers focusing on specific domains or industries. Domain-specific languages (DSLs) are expected to gain popularity, as they allow programmers to write code that is tailored to the specific requirements and constraints of a particular domain. This specialization will lead to more efficient and effective software development in various industries.\\n\\nStep 4: Distributed and Decentralized Systems\\nThe future of programming will involve dealing with distributed and decentralized systems. As technologies like blockchain and the Internet of Things (IoT) continue to evolve, programming will need to adapt to handle the complexities of these interconnected systems. Programmers will need to understand how to develop secure, scalable, and reliable applications in such environments.\\n\\nStep 5: Ethical and Responsible Programming\\nAs technology plays an increasingly significant role in our lives, ethical and responsible programming will become crucial. Programmers will need to consider the ethical implications of their code, ensuring that it respects privacy, avoids bias, and is designed with inclusivity in mind. This shift will require a greater emphasis on ethics and social responsibility in programming education and practices.\\n\\nStep 6: Continuous Learning and Adaptation\\nThe pace of technological advancements will require programmers to embrace continuous learning and adaptation. New programming languages, frameworks, and tools will emerge, and programmers will need to stay updated to remain relevant. Lifelong learning will become a core aspect of a programmer's career, enabling them to keep up with the rapidly evolving landscape.\\n\\nStep 7: Human-Machine Collaboration\\nWhile automation and AI will play a role in programming, the future will still rely on human creativity and problem-solving abilities. Programming will involve more collaboration between humans and machines, where programmers work alongside AI systems to develop innovative solutions. This synergy will lead to improved productivity and the development of more advanced applications.\\n\\nStep 8: Quantum Computing and New Paradigms\\nWith the advancement of quantum computing, programming paradigms will likely evolve. Quantum programming languages and algorithms will emerge to harness the power of quantum computers, enabling the development of groundbreaking applications in areas such as cryptography, optimization, and data analysis.\\n\\nStep 9: Increased Emphasis on Cybersecurity\\nAs technology becomes more integrated into our lives, the need for robust cybersecurity measures will continue to grow. Programmers will need to prioritize security in their code and develop secure applications to protect against cyber threats. This will require a deeper understanding of cybersecurity principles and practices.\\n\\nStep 10: Interdisciplinary Collaboration\\nProgramming will increasingly require interdisciplinary collaboration. As technology becomes more intertwined with various fields, programmers will need to work closely with experts from other domains such as healthcare, finance, and environmental sciences. This collaboration will lead to the development of innovative solutions that address complex challenges in diverse industries.\\n\\nOverall, the future of programming is likely to be driven by automation, AI integration, low-code development, specialization, ethical considerations, continuous learning, collaboration, quantum computing, cybersecurity, and interdisciplinary approaches. The programming landscape will continue to evolve, and programmers will need to adapt to these changes to thrive in the future.\",\n",
       " 'output': \"Step 1: Increasing Automation and AI Integration\\nThe future of programming is likely to see increasing levels of automation and integration of artificial intelligence (AI) technologies. As AI continues to advance, programming tasks that are repetitive or routine in nature can be automated, allowing programmers to focus on more complex and creative problem-solving.\\n\\nStep 2: Low-Code and No-Code Development\\nThe rise of low-code and no-code development platforms will also shape the future of programming. These platforms enable individuals with limited programming knowledge to create applications using visual interfaces and pre-built components. This trend will democratize software development, allowing non-programmers to participate in creating software solutions.\\n\\nStep 3: Increased Specialization and Domain-Specific Languages\\nProgramming is becoming increasingly specialized, with developers focusing on specific domains or industries. Domain-specific languages (DSLs) are expected to gain popularity, as they allow programmers to write code that is tailored to the specific requirements and constraints of a particular domain. This specialization will lead to more efficient and effective software development in various industries.\\n\\nStep 4: Distributed and Decentralized Systems\\nThe future of programming will involve dealing with distributed and decentralized systems. As technologies like blockchain and the Internet of Things (IoT) continue to evolve, programming will need to adapt to handle the complexities of these interconnected systems. Programmers will need to understand how to develop secure, scalable, and reliable applications in such environments.\\n\\nStep 5: Ethical and Responsible Programming\\nAs technology plays an increasingly significant role in our lives, ethical and responsible programming will become crucial. Programmers will need to consider the ethical implications of their code, ensuring that it respects privacy, avoids bias, and is designed with inclusivity in mind. This shift will require a greater emphasis on ethics and social responsibility in programming education and practices.\\n\\nStep 6: Continuous Learning and Adaptation\\nThe pace of technological advancements will require programmers to embrace continuous learning and adaptation. New programming languages, frameworks, and tools will emerge, and programmers will need to stay updated to remain relevant. Lifelong learning will become a core aspect of a programmer's career, enabling them to keep up with the rapidly evolving landscape.\\n\\nStep 7: Human-Machine Collaboration\\nWhile automation and AI will play a role in programming, the future will still rely on human creativity and problem-solving abilities. Programming will involve more collaboration between humans and machines, where programmers work alongside AI systems to develop innovative solutions. This synergy will lead to improved productivity and the development of more advanced applications.\\n\\nStep 8: Quantum Computing and New Paradigms\\nWith the advancement of quantum computing, programming paradigms will likely evolve. Quantum programming languages and algorithms will emerge to harness the power of quantum computers, enabling the development of groundbreaking applications in areas such as cryptography, optimization, and data analysis.\\n\\nStep 9: Increased Emphasis on Cybersecurity\\nAs technology becomes more integrated into our lives, the need for robust cybersecurity measures will continue to grow. Programmers will need to prioritize security in their code and develop secure applications to protect against cyber threats. This will require a deeper understanding of cybersecurity principles and practices.\\n\\nStep 10: Interdisciplinary Collaboration\\nProgramming will increasingly require interdisciplinary collaboration. As technology becomes more intertwined with various fields, programmers will need to work closely with experts from other domains such as healthcare, finance, and environmental sciences. This collaboration will lead to the development of innovative solutions that address complex challenges in diverse industries.\\n\\nOverall, the future of programming is likely to be driven by automation, AI integration, low-code development, specialization, ethical considerations, continuous learning, collaboration, quantum computing, cybersecurity, and interdisciplinary approaches. The programming landscape will continue to evolve, and programmers will need to adapt to these changes to thrive in the future.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import OpenAIModerationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "moderation_chain = OpenAIModerationChain()\n",
    "\n",
    "cot_prompt = PromptTemplate.from_template(\n",
    "    \"{question} \\nLet's think step by step!\"\n",
    ")\n",
    "llm_chain = cot_prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "chain = llm_chain | moderation_chain\n",
    "\n",
    "response = chain.invoke({'question': 'What is the future of programming?'})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Developing Software with Generative AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Software development and AI - Code LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Writing code with LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### StarCoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StarChat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small local model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Automating software development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# LLMs for Data Science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The impact of generative models on data science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Automated data science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization and EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using agents to answer data science questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data exploration with LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Customizing LLMs and Their Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conditioning LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reinforcement learning with human feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-rank adaptation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference-time conditioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setup for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-source models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Zero-shot prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The sentiment of the text is negative.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\n",
    "template = \"\"\"Classify the sentiment of this text: {text}\"\"\"\n",
    "prompt = PromptTemplate(input_variables=['text'], template=template)\n",
    "chain = prompt | model\n",
    "result = chain.invoke({'text': 'I hated that movie, it was terrible!'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Positive' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    'input': 'I absolutely love the new update! Everything works seamlessly.',\n",
    "    'output': 'Positive'\n",
    "  },\n",
    "  {\n",
    "    'input': \"It's okay, but I think it could use more features.\",\n",
    "    'output': 'Neutral'\n",
    "  },\n",
    "  {\n",
    "    'input': (\"I'm disappointed with the service, I expected much better \"\n",
    "              \"performance\"),\n",
    "    'output': 'Negative'\n",
    "  }\n",
    "]\n",
    "example_prompt = PromptTemplate(\n",
    "  template='{input} -> {output}',\n",
    "  input_variables=['input', 'output']\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "  examples=examples,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix='Question: {input}',\n",
    "  input_variables=['input']\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "result = chain.invoke({'input': ('This is an excellent book with high quality'\n",
    "                                 'explainations.')})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Step 1: Start with the original number of apples, which is 5.\\nStep 2: Subtract the number of apples you ate, which is 2. So, you have 5 - 2 = 3 apples.\\nStep 3: Add the number of apples your friend gave you, which is 3. So, you have 3 + 3 = 6 apples.\\nTherefore, you have 6 apples now.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "reasoning_prompt = \"{question}\\nLet's think step by step!\"\n",
    "prompt = PromptTemplate(\n",
    "  template=reasoning_prompt,\n",
    "  input_variables=['question']\n",
    ")\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0613')\n",
    "chain = prompt | model\n",
    "result = chain.invoke({\n",
    "  'question': (\"There were 5 apples originally. I ate 2 apples. My friend gave \"\n",
    "               \"me 3 apples. How many apples do I have now?\")\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"I absolutely love the new update! Everything works seamlessly.\",\n",
    "        \"output\": (\"Love and absolute works seamlessly are examples of positive \"\n",
    "                   \"sentiment. Therefore, the sentiment is positive\"),\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"It's okay, but I think it could use more features.\",\n",
    "        \"output\": (\"It's okay is not an endorsement. The customer further thinks\"\n",
    "                   \" it should be extended. Therefore, the sentiment is neutral\"),\n",
    "    },\n",
    "    {\n",
    "        \"input\": (\"I'm disappointed with the service, I expected much better \"\n",
    "                  \"performance.\"),\n",
    "        \"output\": (\"The customer is disappointed and expected more. \"\n",
    "                   \"This is negative\")\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that occurs most frequently is \"1776 marks the year when the Declaration of Independence of the United States was signed.\" (5 times)\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo-0613')\n",
    "\n",
    "solutions_template = \"\"\"\n",
    "Generate {num_solutions} distinct answers to this question:\n",
    "{question}\n",
    "\n",
    "Solutions:\n",
    "\"\"\"\n",
    "solutions_prompt = PromptTemplate(\n",
    "    template=solutions_template,\n",
    "    input_variables=['question', 'num_solutions']\n",
    ")\n",
    "solutions_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=solutions_prompt,\n",
    "    output_key='solutions'\n",
    ")\n",
    "\n",
    "consistency_template = \"\"\"\n",
    "For each answer in {solutions}, count the number of times it occurs.\n",
    "Finally, choose the answer that occurs most.\n",
    "\n",
    "Most frequent solution:\n",
    "\"\"\"\n",
    "consistency_prompt = PromptTemplate(\n",
    "    template=consistency_template,\n",
    "    input_variables=['solutions']\n",
    ")\n",
    "consistency_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=consistency_prompt,\n",
    "    output_key='best_solution'\n",
    ")\n",
    "\n",
    "answer_chain = SequentialChain(\n",
    "    chains=[solutions_chain, consistency_chain],\n",
    "    input_variables=['question', 'num_solutions'],\n",
    "    output_variables=['best_solution']\n",
    ")\n",
    "\n",
    "result = answer_chain.run(\n",
    "    question=('Which year was the Declaration of Independence of the United '\n",
    "              'States signed?'),\n",
    "    num_solutions='5'\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-thought\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Implementing an efficient task scheduling algorithm\n",
      "2. Developing a caching mechanism\n",
      "3. Optimizing network communication\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo-0613')\n",
    "\n",
    "solution_template = \"\"\"\n",
    "Generate {num_solutions} distince solutions for {problem}. Consider factors like\n",
    "{factors}.\n",
    "\n",
    "Solutions:\n",
    "\"\"\"\n",
    "solutions_prompt = PromptTemplate(\n",
    "  template=solution_template,\n",
    "  input_variables=['problem', 'factors', 'num_solutions']\n",
    ")\n",
    "solutions_chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=solutions_prompt,\n",
    "  output_key='solutions'\n",
    ")\n",
    "\n",
    "evaluation_template = \"\"\"\n",
    "Evaluate each solution in {solutions} by analyzing pros, cons, feasibility, and\n",
    "probability of success.\n",
    "\n",
    "Evaluations:\n",
    "\"\"\"\n",
    "evaluation_prompt = PromptTemplate(\n",
    "  template=evaluation_template,\n",
    "  input_variables=['solutions']\n",
    ")\n",
    "evaluation_chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=evaluation_prompt,\n",
    "  output_key='evaluations'\n",
    ")\n",
    "\n",
    "reasoning_template = \"\"\"\n",
    "For the most promising solutions in {evaluations}, explain scenarios, \n",
    "implementation strategies, partnerships needed, and handling potential \n",
    "obstacles.\n",
    "\n",
    "Enhanced reasoning:\n",
    "\"\"\"\n",
    "reasoning_prompt = PromptTemplate(\n",
    "  template=reasoning_template,\n",
    "  input_variables=['evaluations']\n",
    ")\n",
    "reasoning_chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=reasoning_prompt,\n",
    "  output_key='enhanced_reasoning'\n",
    ")\n",
    "\n",
    "ranking_template = \"\"\"\n",
    "Based on the evaluations and reasoning, rank the solutions in \n",
    "{enhanced_reasoning} from most to least promising.\n",
    "\n",
    "Ranked Solutions:\n",
    "\"\"\"\n",
    "ranking_prompt = PromptTemplate(\n",
    "  template=ranking_template,\n",
    "  input_variables=['enhanced_reasoning']\n",
    ")\n",
    "ranking_chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=ranking_prompt,\n",
    "  output_key='ranked_solutions'\n",
    ")\n",
    "\n",
    "tot_chain = SequentialChain(\n",
    "  chains=[solutions_chain, evaluation_chain, reasoning_chain, ranking_chain],\n",
    "  input_variables=['problem', 'factors', 'num_solutions'],\n",
    "  output_variables=['ranked_solutions']\n",
    ")\n",
    "\n",
    "result = tot_chain.run(\n",
    "    problem = 'Prompt Engineering',\n",
    "    factors = ('Requirements for high task performance, low token use, and '\n",
    "               'few calls to the LLM'),\n",
    "    num_solutions = 3,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generative AI in Production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating LLM apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing two outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Both responses are relevant to the question asked, as they both provide a numerical answer to the question about the number of dogs in the park. However, Response A is incorrect according to the reference answer, which states that there are four dogs in the park. Response B is correct, as it matches the reference answer exactly. Neither response demonstrates particular depth of thought, as they are both simple, straightforward answers to the question. \\n\\nBased on these criteria, Response B is the better response.\\n',\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator('labeled_pairwise_string')\n",
    "evaluator.evaluate_string_pairs(\n",
    "  prediction='there are three dogs',\n",
    "  prediction_b='4',\n",
    "  input='how many dogs are in the park?',\n",
    "  reference='four'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing against criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Response A is simple, clear, and precise. It uses straightforward language to convey a deep and meaningful message about families. The metaphor of joy and sorrow as music is effective and easy to understand. \\n\\nResponse B, on the other hand, is more complex and uses more sophisticated language. While it conveys a similar message to Response A, it does so in a more convoluted way. The use of words like \"domicile\", \"resounds\", \"abode\", \"conducts\", \"dissonant\", \"elegy\", and \"peculiar\" make the sentence harder to understand. \\n\\nBoth responses are truthful and sincere, and both suggest deeper meanings about the nature of family life. However, Response A does a better job of meeting the criteria of simplicity, clarity, and precision. \\n\\nTherefore, the decision is: [[A]]',\n",
       " 'value': 'A',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "custom_criteria = {\n",
    "    \"simplicity\": \"Is the language straightforward and unpretentious?\",\n",
    "    \"clarity\": \"Are the sentences clear and easy to understand?\",\n",
    "    \"precision\": \"Is the writing precise, with no unnecessary words or details?\",\n",
    "    \"truthfulness\": \"Does the writing feel honest and sincere?\",\n",
    "    \"subtext\": \"Does the writing suggest deeper meanings or themes?\",\n",
    "}\n",
    "evaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=(\"Every cheerful household shares a similar rhythm of joy; \"\n",
    "                \"but sorrow, in each household, plays a unique, haunting melody.\"),\n",
    "    prediction_b=(\"Where one finds a symphony of joy, every domicile of \"\n",
    "                  \"happiness resounds in harmonious, identical notes; yet, \"\n",
    "                  \"every abode of despair conducts a dissonant orchestra, each\"\n",
    "                  \" playing an elegy of grief that is peculiar and profound to \"\n",
    "                  \"its own existence.\"),\n",
    "    input=\"Write some prose about families.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String and semantic comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09679562397049857}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator('embedding_distance')\n",
    "evaluator.evaluate_strings(\n",
    "  prediction='I shall go',\n",
    "  reference=\"I shan't go\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluations against datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langsmith import Client\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=0.0)\n",
    "llm.predict(\"Hello, world!\")\n",
    "\n",
    "client = Client()\n",
    "runs = client.list_runs()\n",
    "print(runs)\n",
    "\n",
    "questions = [\n",
    "    (\"A ship's parts are replaced over time until no original parts remain. \"\n",
    "     \"Is it still the same ship? Why or why not?\"),  # The Ship of Theseus Paradox\n",
    "    (\"If someone lived their whole life chained in a cave seeing only shadows, \"\n",
    "     \"how would they react if freed and shown the real world?\"),  # Plato's Allegory of the Cave\n",
    "    (\"Is something good because it is natural, or bad because it is unnatural? \"\n",
    "     \"Why can this be a faulty argument?\"),  # Appeal to Nature Fallacy\n",
    "    (\"If a coin is flipped 8 times and lands on heads each time, what are the \"\n",
    "     \"odds it will be tails next flip? Explain your reasoning.\"),  # Gambler's Fallacy\n",
    "    (\"Present two choices as the only options when others exist. Is the \"\n",
    "     \"statement \\\"You're either with us or against us\\\" an example of false \"\n",
    "     \"dilemma? Why?\"),  # False Dilemma\n",
    "    (\"Do people tend to develop a preference for things simply because they are \"\n",
    "     \"familiar with them? Does this impact reasoning?\"),  # Mere Exposure Effect\n",
    "    (\"Is it surprising that the universe is suitable for intelligent life since \"\n",
    "     \"if it weren't, no one would be around to observe it?\"),  # Anthropic Principle\n",
    "    (\"If Theseus' ship is restored by replacing each plank, is it still the same \"\n",
    "     \"ship? What is identity based on?\"),  # Theseus' Paradox\n",
    "    (\"Does doing one thing really mean that a chain of increasingly negative \"\n",
    "     \"events will follow? Why is this a problematic argument?\"),  # Slippery Slope Fallacy\n",
    "    (\"Is a claim true because it hasn't been proven false? Why could this impede \"\n",
    "     \"reasoning?\"),  # Appeal to Ignorance\n",
    "]\n",
    "\n",
    "shared_dataset_name = \"Reasoning and Bias\"\n",
    "ds = client.create_dataset(\n",
    "    dataset_name=shared_dataset_name,\n",
    "    description=\"A few reasoning and cognitive bias questions\"\n",
    ")\n",
    "\n",
    "for q in questions:\n",
    "    client.create_example(inputs={\"input\": q}, dataset_id=ds.id)\n",
    "    \n",
    "def construct_chain():\n",
    "  return LLMChain.from_string(\n",
    "    llm=llm,\n",
    "    template='Help out as best you can.\\nQuestion: {input}\\nResponse:'\n",
    "  )\n",
    "  \n",
    "evaluation_config = RunEvalConfig(\n",
    "  evaluators=[\n",
    "    RunEvalConfig.Criteria({'helpfulness': 'Is the response helpful?'}),\n",
    "    RunEvalConfig.Criteria({'insightful': 'Is the response carefully thought out?'})\n",
    "  ]\n",
    ")\n",
    "\n",
    "results = run_on_dataset(\n",
    "  client=client,\n",
    "  dataset_name=shared_dataset_name,\n",
    "  dataset=ds,\n",
    "  llm_or_chain_factory=construct_chain,\n",
    "  evaluation=evaluation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deploying LLM apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### FastAPI web server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Observing LLM apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tracking responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observability tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"What's the latency like for https://langchain.com?\", 'output': 'The latency for https://langchain.com is approximately 5.79 milliseconds.', 'intermediate_steps': [(_FunctionsAgentAction(tool='ping', tool_input={'url': 'https://langchain.com', 'return_error': False}, log=\"\\nInvoking: `ping` with `{'url': 'https://langchain.com', 'return_error': False}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'tool_selection', 'arguments': '{\\n  \"actions\": [\\n    {\\n      \"action_name\": \"ping\",\\n      \"action\": {\\n        \"url\": \"https://langchain.com\",\\n        \"return_error\": false\\n      }\\n    }\\n  ]\\n}'}}, example=False)]), 'PING langchain.com (35.71.142.77) 56(84) bytes of data.\\n64 bytes from a0b1d980e1f2226c6.awsglobalaccelerator.com (35.71.142.77): icmp_seq=1 ttl=246 time=5.79 ms\\n\\n--- langchain.com ping statistics ---\\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\\nrtt min/avg/max/mdev = 5.785/5.785/5.785/0.000 ms\\n')]}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "from pydantic import HttpUrl\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "def ping(url: HttpUrl, return_error: bool) -> str:\n",
    "  \"\"\"Ping the fully specified url. Must include https:// in the url.\"\"\"\n",
    "  hostname = urlparse(str(url)).netloc\n",
    "  completed_process = subprocess.run(\n",
    "    ['ping', '-c', '1', hostname], capture_output=True, text=True\n",
    "  )\n",
    "  output = completed_process.stdout\n",
    "  \n",
    "  if return_error and (completed_process.returncode != 0):\n",
    "    return completed_process.stderr\n",
    "  return output\n",
    "\n",
    "ping_tool = StructuredTool.from_function(ping)\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0613', temperature=0)\n",
    "agent = initialize_agent(\n",
    "  llm=llm,\n",
    "  tools=[ping_tool],\n",
    "  agent=AgentType.OPENAI_MULTI_FUNCTIONS,\n",
    "  return_intermediate_steps=True\n",
    ")\n",
    "result = agent(\"What's the latency like for https://langchain.com?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptWatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from promptwatch import PromptWatch\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "  'Finish this sentence: {input}'\n",
    ")\n",
    "llm = OpenAI(temperature=0., model='gpt-3.5-turbo-instruct')\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "with PromptWatch() as pw:\n",
    "  chain('The quick brown fox jumped over')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
