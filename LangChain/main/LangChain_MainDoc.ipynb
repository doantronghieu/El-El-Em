{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "  print(\n",
    "    f\"\\n{'-' * 100}\\n\".join(\n",
    "      [f\"Document {i+1}:\\n\\n\" +\n",
    "        d.page_content for i, d in enumerate(docs)]\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export LANGCHAIN_TRACING_V2=\"true\"\n",
    "# export LANGCHAIN_API_KEY=\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  ('system', 'You are world class technical documentation writer.'),\n",
    "  ('user', '{input}')\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "result = chain.invoke({'input': 'how can langsmith help with testing?'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# To index data, load it using WebBaseLoader.\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "\n",
    "# To create a vectorstore, need an embedding model and a vectorstore. \n",
    "# Use embedding model to ingest documents into a vectorstore.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Build the index.\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Simple local vectorstore, FAISS\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "# Data is indexed in the vectorstore\n",
    "\n",
    "# Create a retrieval chain that takes an incoming question, looks up relevant \n",
    "# documents, and passes them along with the original question into an LLM to \n",
    "# answer the question.\n",
    "# The retrieval method should consider the entire input history\n",
    "# The final LLM chain should take into account the entire history\n",
    "template = \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# Chain that takes in recent input and conversation history, \n",
    "# using an LLM to generate a search query.\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the chain, which returns a dictionary with the response from the LLM \n",
    "# in the answer key.\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Index data using WebBaseLoader.\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Ingest documents into the vectorstore using the embedding model.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Build the index.\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Simple local vectorstore, FAISS\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()\n",
    "# Data is indexed in the vectorstore\n",
    "\n",
    "# Create a retrieval chain for an incoming question, lookup relevant documents,\n",
    "# and pass them, along with the original question, to an LLM for answering.\n",
    "# The retriever and LLM considers the entire input history,\n",
    "# The prompt is passed to an LLM to generate a search query.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Chain: recent input + conversation history -> LLM -> generate search query\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "# Test by passing a follow-up question from the user.\n",
    "chat_history = [\n",
    "  HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "  AIMessage(content=\"Yes!\")\n",
    "]\n",
    "retrieval_chain.invoke({\n",
    "  \"chat_history\": chat_history,\n",
    "  \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import (create_openai_functions_agent,\n",
    "                              AgentExecutor)\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Index data using WebBaseLoader.\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()  # Ingest documents into the vectorstore\n",
    "\n",
    "# Build index\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)  # Local vectorstore\n",
    "retriever = vector.as_retriever()\n",
    "# Data is indexed in the vectorstore\n",
    "\n",
    "\"\"\"\n",
    "Create an agent using OpenAI models to determine its steps.\n",
    "\n",
    "Determine necessary tools for agent. For example, it will have access to:\n",
    "1. The retriever created for answering questions about LangSmith\n",
    "2. A search tool (Tavily) for providing up-to-date information.\n",
    "\"\"\"\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "  retriever=retriever,\n",
    "  name=\"langsmith_search\",\n",
    "  description=(\"Search for information about LangSmith. For any questions \"\n",
    "               \"about LangSmith, you must use this tool!\"),\n",
    ")\n",
    "# https://python.langchain.com/docs/integrations/retrievers/tavily\n",
    "search_tool = TavilySearchResults()\n",
    "tools = [retriever_tool, search_tool]\n",
    "\n",
    "# Use tools to get a predefined prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with LangServe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangServe allows developers to deploy LangChain chains as a REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "serve.py contains server logic for serving the application. \n",
    "It includes the definition of the chain, FastAPI app, and a route for serving \n",
    "the chain (langserve.add_routes). \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt + model + output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a chain that generates a joke from a topic.\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\"\"\"\n",
    "Utilize LCEL to combine multiple components into one chain.\n",
    "\n",
    "The | symbol works like a unix pipe operator, connecting components to feed \n",
    "output as input for the next.\n",
    "\n",
    "The user input flows through the prompt template, model, and output parser in \n",
    "this chain.\n",
    "\"\"\"\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The prompt is a BasePromptTemplate that takes in a dictionary of template \n",
    "variables to produce a PromptValue, a wrapper around a completed prompt that can\n",
    "be used by LLM/ChatModel. It can work with both language model \n",
    "types, as it has the ability to produce both BaseMessages and strings.\n",
    "\"\"\"\n",
    "\n",
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "print(prompt_value.__str__)\n",
    "print(prompt_value.to_messages())\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PromptValue is passed to the ChatModel, which outputs a BaseMessage.\n",
    "message = model.invoke(prompt_value)\n",
    "print(message.__str__)\n",
    "\n",
    "# Model outputs a string if it were an LLM.\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass the model output to the output_parser, a BaseOutputParser that takes a \n",
    "# string/BaseMessage as input. The StrOutputParser converts any input to a string.\n",
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval-augmented generation chain for added context in question responses.\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# query -> inmemory store -> retrieved documents\n",
    "# runnable component can be used alone or in conjunction with other components.\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "  [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "  embedding=OpenAIEmbeddings()\n",
    ")\n",
    "# Retrieve documents and include them in the context\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# The prompt template takes in context and question as values to be \n",
    "# substituted in the prompt. \n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Prepare expected inputs by using retrieved document entries and original user\n",
    "# question, using the retriever for document search, and RunnablePassthrough to\n",
    "# pass the user's question.\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "  {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "# anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "\n",
    "model = (\n",
    "  chat_openai\n",
    "  .configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"chat_openai\",\n",
    "    openai=openai,\n",
    "    # anthropic=anthropic,\n",
    "  )\n",
    "  .with_fallbacks([openai])\n",
    ")\n",
    "\n",
    "chain = (\n",
    "  {\"topic\": RunnablePassthrough()}\n",
    "  | prompt\n",
    "  | chat_openai\n",
    "  | output_parser\n",
    ")\n",
    "\n",
    "configurable_chain = (\n",
    "  {\"topic\": RunnablePassthrough()}\n",
    "  | prompt\n",
    "  | model\n",
    "  | output_parser\n",
    ")\n",
    "\n",
    "llm_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | openai\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "# anthropic_chain = (\n",
    "#     {\"topic\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | anthropic\n",
    "#     | output_parser\n",
    "# )\n",
    "\n",
    "fallback_chain = chain.with_fallbacks([llm_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVOKE\n",
    "# Input a topic and receive a joke\n",
    "print(chain.invoke(\"ice cream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAM\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH\n",
    "print(chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASYNC\n",
    "print(chain.ainvoke(\"ice cream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_chain.invoke(\n",
    "  input=\"ice cream\",\n",
    "  config={\"model\": \"openai\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = configurable_chain.stream(\n",
    "    input=\"ice cream\",\n",
    "    config={\"model\": \"openai\"}\n",
    ")\n",
    "for chunk in stream:\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM instead of chat model\n",
    "llm_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different model provider\n",
    "# anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input/Output Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description of inputs/outputs accepted by a Runnable Pydantic model dynamically \n",
    "generated from any Runnable's structure. \n",
    "Call .schema() to obtain a JSONSchema representation.\n",
    "\"\"\"\n",
    "\n",
    "# The input schema of the chain is the input schema of its first part, the prompt.\n",
    "print(chain.input_schema.schema())\n",
    "print(prompt.input_schema.schema())\n",
    "print(model.input_schema.schema())\n",
    "print()\n",
    "\n",
    "# The output schema of the chain is the output schema of its last part, \n",
    "# in this case a ChatModel, which outputs a ChatMessage\n",
    "print(chain.output_schema.schema())\n",
    "print(prompt.output_schema.schema())\n",
    "print(model.output_schema.schema())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stream, Invoke, Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "  print(s.content, end=\"\", flush=True)\n",
    "\n",
    "print(chain.invoke({\"topic\": \"bears\"}))\n",
    "\n",
    "print(chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Stream, Invoke, Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for s in chain.astream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\", flush=True)\n",
    "\n",
    "await chain.ainvoke({\"topic\": \"bears\"})\n",
    "\n",
    "await chain.abatch([{\"topic\": \"bears\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Stream Intermediate Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streaming JSONPatch chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "\n",
    "class LogEntry(TypedDict):\n",
    "    id: str\n",
    "    \"\"\"ID of the sub-run.\"\"\"\n",
    "    name: str\n",
    "    \"\"\"Name of the object being run.\"\"\"\n",
    "    type: str\n",
    "    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n",
    "    tags: List[str]\n",
    "    \"\"\"List of tags for the run.\"\"\"\n",
    "    metadata: Dict[str, Any]\n",
    "    \"\"\"Key-value pairs of metadata for the run.\"\"\"\n",
    "    start_time: str\n",
    "    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"\n",
    "\n",
    "    streamed_output_str: List[str]\n",
    "    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of this run.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "    end_time: Optional[str]\n",
    "    \"\"\"ISO-8601 timestamp of when the run ended.\n",
    "    Only available after the run has finished.\"\"\"\n",
    "\n",
    "\n",
    "class RunState(TypedDict):\n",
    "    id: str\n",
    "    \"\"\"ID of the run.\"\"\"\n",
    "    streamed_output: List[Any]\n",
    "    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "\n",
    "    logs: Dict[str, LogEntry]\n",
    "    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n",
    "    contain only the runs that matched the filters.\"\"\"\n",
    "\n",
    "\n",
    "# This is useful for streaming JSONPatch through an HTTP server and applying\n",
    "# the ops on the client to rebuild the run state.\n",
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"where did harrison work?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streaming the incremental RunState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing diff=False will give incremental values of RunState. More \n",
    "# verbose output is produced when there are repetitive parts.\n",
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"where did harrison work?\", include_names=[\"Docs\"], diff=False\n",
    "):\n",
    "    print(\"-\" * 70)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LECL supports parallel requests through the use of RunnableParallel. \n",
    "# This executes every element in parallel.\n",
    "import time\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "template1 = \"tell me a joke about {topic}\"\n",
    "chain1 = ChatPromptTemplate.from_template(template1) | model\n",
    "template2 = \"write a short (2 line) poem about {topic}\"\n",
    "chain2 = ChatPromptTemplate.from_template(template2) | model\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chain1.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chain2.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "combined.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallelism on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "combined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model I/O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "# Configurations for specific models, initialized with parameters like temperature.\n",
    "# LLM objects take strings as input and output strings,\n",
    "# ChatModel objects take a list of messages as input and output a message.\n",
    "  \n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "print(f\"LLM: {llm.invoke(messages)}\")\n",
    "print(f\"Chat Model: {chat_model.invoke(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM applications don't input user data directly. \n",
    "They incorporate it into a prompt template, which gives more context for the task. \n",
    "\n",
    "Example, our application would simply require the user provide a description of\n",
    "the company/product, without having to give specific instructions to the model.\n",
    "\n",
    "PromptTemplates simplify the process of converting user input into a properly\n",
    "formatted prompt. \n",
    "\n",
    "They allow to selectively format variables, combine templates, and create a \n",
    "single prompt.\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"What would be a good company name for a company that makes {product}?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a list of messages, including information about their content, role, \n",
    "and position in the list.\n",
    "\n",
    "ChatPromptTemplate is a list of ChatMessageTemplates, each specifying the \n",
    "formatting instructions for a ChatMessage. \n",
    "\n",
    "The ChatMessageTemplate includes the message's role and content. \n",
    "\"\"\"\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "system_template = (\"You are helpful assistant that translate {input_language} \"\n",
    "                   \"to {output_language}\")\n",
    "human_template = \"{text}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system_template),\n",
    "  (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\",\n",
    "                            output_language=\"French\",\n",
    "                            text=\"I love programming\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output parsers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform language model raw output into usable formats. \n",
    "# Types include: converting LLM text into JSON, turning a ChatMessage into a \n",
    "# string, and converting additional information from a call into a string.\n",
    "\n",
    "# Parser for comma separated values.\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composing with LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine steps into one chain. \n",
    "# The chain will take input variables, pass them to a prompt template, create \n",
    "# a prompt, run it through a language model, the output will be passed through\n",
    "# an output parser. \n",
    "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "# | syntax to join components.\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "chain.invoke({\"text\": \"colors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PromptTemplate to create a string prompt template. \n",
    "# PromptTemplate uses Python's str.format syntax by default.\n",
    "# The template allows for unlimited variables, even without any.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "  \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat prompt consists of a list of chat messages\n",
    "# each: [content, role] parameter.\n",
    "# In the OpenAI Chat Completions API, a message can be linked to \n",
    "# an AI assistant, a human, or a system role.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ChatPromptTemplate.from_messages accepts various message representations. \n",
    "# These include the 2-tuple format of (type, content), \n",
    "chat_template1 = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are helpful AI bot. Your name is {name}\"),\n",
    "  (\"human\", \"Hello, how are you doing?\"),\n",
    "  (\"ai\", \"I'm doing well, thanks!\"),\n",
    "  (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages1 = chat_template1.format_messages(name=\"Bob\", \n",
    "                                         user_input=\"What is your name?\")\n",
    "print(messages1)\n",
    "\n",
    "# or instances of MessagePromptTemplate or BaseMessage.\n",
    "chat_template2 = ChatPromptTemplate.from_messages([\n",
    "  SystemMessage(content=(\n",
    "    \"You are a helpful assistant that re-writes the user's text to sound more \"\n",
    "    \"upbeat.\"\n",
    "  )),\n",
    "  HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "])\n",
    "messages2 = chat_template2.format_messages(text=\"I don't like eating tasting things.\")\n",
    "print(messages2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate, ChatPromptTemplate are Runnable interfaces, LCEL's core\n",
    "# Support (a)invoke, (a)stream, (a)batch, and astream_log calls. \n",
    "# dictionary of prompt variables -> PromptTemplate -> StringPromptValue\n",
    "# dictionary -> ChatPromptTemplate -> ChatPromptValue.\n",
    "prompt_val = prompt_template.invoke({\"adjective\":\"funny\", \"content\": \"chickens\"})\n",
    "print(prompt_val.__str__)\n",
    "print(prompt_val.to_string().__str__())\n",
    "print(prompt_val.to_messages(), \"\\n\")\n",
    "\n",
    "chat_val = chat_template2.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "print(chat_val.to_string())\n",
    "print(chat_val.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### String prompt composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each template is joined together\n",
    "# Can work with prompts or strings (with the first element being a prompt)\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = (\n",
    "  PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "  + \", make it funny\"\n",
    "  + \"\\n\\n and in {language}\"\n",
    ")\n",
    "print(prompt.__str__, \"\\n\")\n",
    "print(prompt.format(topic=\"sports\", language=\"spanish\"), \"\\n\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = LLMChain(llm=model,  prompt=prompt)\n",
    "chain.run(topic=\"sports\", language=\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chat prompt composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A chat prompt is a list of messages. \n",
    "# Each element represents a new message within the completed prompt.\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = SystemMessage(content=\"You are a nice pirate\")\n",
    "\n",
    "# Create a pipeline by combining the message with other messages (templates). \n",
    "# Use a Message when there are no variables to format\n",
    "# Use a MessageTemplate when variables are present. \n",
    "# A string can be used (inferred as a HumanMessagePromptTemplate).\n",
    "new_prompt = (\n",
    "  prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\"\n",
    ")\n",
    "new_prompt.format_messages(input=\"i said hi\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = LLMChain(llm=model, prompt=new_prompt)\n",
    "chain.run(\"i said hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Selector Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Select by length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Manages examples based on length, ensuring prompt construction within the \n",
    "context window limit. It adjusts the number of selected examples: fewer for \n",
    "longer inputs and more for shorter ones.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "  {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "  {\"input\": \"tall\", \"output\": \"short\"},\n",
    "  {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "  {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "  {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "example_prompt = PromptTemplate(\n",
    "  input_variables=[\"input\", \"output\"],\n",
    "  template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "  # The examples it has available to choose from.\n",
    "  examples=examples,\n",
    "  # The PromptTemplate being used to format the examples.\n",
    "  example_prompt=example_prompt,\n",
    "  # The maximum length that the formatted examples should be.\n",
    "  # Length is measured by the get_text_length function below.\n",
    "  max_length=25,\n",
    "  # The function used to get the length of a string, which is used\n",
    "  # to determine which examples to include. It is commented out because\n",
    "  # it is provided as a default value if none is specified.\n",
    "  # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  prefix=\"Give me the antonym of every input\",\n",
    "  suffix=\"Input: {adjective}\\nOutput:\",\n",
    "  input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "# An example with small input, so it selects all examples.\n",
    "print(dynamic_prompt.format(adjective=\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example with long input, so it selects only one example.\n",
    "long_string = (\"big and huge and massive and large and gigantic and tall and \"\n",
    "               \"much much much much much bigger than everything else\")\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add an example to an example selector as well.\n",
    "new_example = {\"input\": \"big\", \"output\": \"small\"}\n",
    "dynamic_prompt.example_selector.add_example(new_example)\n",
    "print(dynamic_prompt.format(adjective=\"enthusiastic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Select by maximal marginal relevance (MMR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combines similarity to inputs and diversity. It selects examples with embeddings\n",
    "having the highest cosine similarity to inputs, adding them iteratively while \n",
    "penalizing closeness to already selected examples.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import (\n",
    "  MaxMarginalRelevanceExampleSelector, SemanticSimilarityExampleSelector\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "  {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "  {\"input\": \"tall\", \"output\": \"short\"},\n",
    "  {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "  {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "  {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "  # The list of examples available to select from.\n",
    "  examples=examples,\n",
    "  # Embedding class for semantic similarity measurement.\n",
    "  embeddings=OpenAIEmbeddings(),\n",
    "  # VectorStore class for embedding storage and similarity search.\n",
    "  vectorstore_cls=FAISS,\n",
    "  # The number of examples to produce.\n",
    "  k=2\n",
    ")\n",
    "example_prompt = PromptTemplate(\n",
    "  input_variables=[\"input\", \"output\"],\n",
    "  template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "mnr_prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  prefix=\"Give the antonum of every input\",\n",
    "  suffix=\"Input: {adjective}\\nOutput:\",\n",
    "  input_variables=[\"adjective\"]\n",
    ")\n",
    "print(mnr_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Select by n-gram overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The NGramOverlapExampleSelector orders examples by their similarity to the input,\n",
    "measured by an ngram overlap score (float between 0.0 and 1.0). \n",
    "A threshold score can be set, excluding examples with a score below or equal to \n",
    "the threshold. The default threshold is -1.0, reordering without exclusion. \n",
    "A threshold of 0.0 excludes examples with no ngram overlaps.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "  input_variables=[\"input\", \"output\"],\n",
    "  template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "# Examples of a fictional translation task.\n",
    "examples = [\n",
    "    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},\n",
    "    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},\n",
    "    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},\n",
    "]\n",
    "example_selector = NGramOverlapExampleSelector(\n",
    "  # The examples it has available to choose from.\n",
    "  examples=examples,\n",
    "  # The PromptTemplate being used to format the examples.\n",
    "  example_prompt=example_prompt,\n",
    "  # The threshold, at which selector stops. Default -1.0\n",
    "  threshold=-1.0,\n",
    "  # Negative Threshold: sorts examples by ngram overlap score, including none.\n",
    "  # Threshold > 1.0: excludes all examples, returning an empty list.\n",
    "  # Threshold = 0.0: sorts examples by ngram overlap score,\n",
    "  # excluding those with no ngram overlap with the input.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  prefix=\"Give the Spanish translation of every input\",\n",
    "  suffix=\"Input: {sentence}\\nOutput:\",\n",
    "  input_variables=[\"sentence\"],\n",
    ")\n",
    "\n",
    "# Example input with significant ngram overlap: \"Spot can run.\" \n",
    "# No overlap with: \"My dog barks.\"\n",
    "print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples can be added to NGramOverlapExampleSelector.\n",
    "new_example = {\"input\": \"Spot plays fetch.\", \"output\": \"Spot juega a buscar.\"}\n",
    "example_selector.add_example(new_example)\n",
    "print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold to exclude examples. \n",
    "# 0.0 excludes examples with no ngram overlaps with the input. \n",
    "# The example \"My dog barks.\" is excluded because it has no ngram overlaps with\n",
    "# \"Spot can run fast.\"\n",
    "example_selector.threshold = 0.0\n",
    "print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Select by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects examples based on cosine similarity of embeddings to the inputs.\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "  input_variables=[\"input\", \"output\"],\n",
    "  template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "  # The list of examples available to select from.\n",
    "  examples=examples,\n",
    "  # Embedding class for generating semantic similarity measurements.\n",
    "  embeddings=OpenAIEmbeddings(),\n",
    "  # The VectorStore class stores embeddings and performs similarity searches.\n",
    "  vectorstore_cls=Chroma,\n",
    "  # Number of examples to generate.\n",
    "  k=1,\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  prefix=\"Give the antonym of every input\",\n",
    "  suffix=\"Input: {adjective}\\nOutput:\",\n",
    "  input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input denotes emotion; choose the happy/sad example.\n",
    "print(similar_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example selectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# The Example Selector is responsible for choosing which examples to include in\n",
    "# the prompt, especially when there are a lot of examples. \n",
    "# Its base interface is defined below:\n",
    "class BaseExampleSelector(ABC):\n",
    "  \"\"\"Interface for selecting examples to include in prompts.\"\"\"\n",
    "  \n",
    "  @abstractmethod\n",
    "  def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "    \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
    "    \n",
    "  @abstractmethod\n",
    "  def add_example(self, example: Dict[str, str]) -> Any:\n",
    "    \"\"\"Add new examle to store.\"\"\"\n",
    "\n",
    "# LangChain requires a select_examples method, which returns a list of examples \n",
    "# based on input variables. There are different types of example selectors \n",
    "# available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors.base import BaseExampleSelector\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "# To use an example selector, create a list of examples (input-output pairs). \n",
    "# Selecting examples for translation task.\n",
    "examples = [\n",
    "  {\"input\": \"hi\", \"output\": \"ciao\"},\n",
    "  {\"input\": \"bye\", \"output\": \"arrivaderci\"},\n",
    "  {\"input\": \"soccer\", \"output\": \"calcio\"},\n",
    "]\n",
    "\n",
    "# Example selector based on word length.\n",
    "class CustomExampleSelector(BaseExampleSelector):\n",
    "  def __init__(self, examples):\n",
    "    self.examples : List = examples\n",
    "  \n",
    "  def add_example(self, example):\n",
    "    self.examples.append(example)\n",
    "  \n",
    "  def select_examples(self, input_variables):\n",
    "    # Assumes input includes a 'input' key.\n",
    "    new_word = input_variables[\"input\"]\n",
    "    new_word_length = len(new_word)\n",
    "    \n",
    "    # Initialize variables for the best match and its length difference.\n",
    "    best_match = None\n",
    "    smallest_diff = float(\"inf\")\n",
    "    \n",
    "    # Iterate through examples.\n",
    "    for example in self.examples:\n",
    "      # Calculate the length difference using the first word of the example.\n",
    "      current_diff = abs(len(example[\"input\"]) - new_word_length)\n",
    "      \n",
    "      # Update the best match if the current one is closer in length.\n",
    "      if current_diff < smallest_diff:\n",
    "        smallest_diff = current_diff\n",
    "        best_match = example\n",
    "    \n",
    "    return [best_match]\n",
    "\n",
    "example_selector = CustomExampleSelector(examples)\n",
    "\n",
    "print(example_selector.select_examples({\"input\": \"okay\"}))\n",
    "\n",
    "example_selector.add_example({\"input\": \"hand\", \"output\": \"mano\"})\n",
    "print(example_selector.select_examples({\"input\": \"okay\"}))\n",
    "\n",
    "# Example selector can now be used in a prompt.\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "  \"Input: {input} -> Output: {output}\"\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix=\"Input: {input} -> Output:\",\n",
    "  prefix=\"Translate the following words from English to Italain:\",\n",
    "  input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(prompt.format(input=\"word\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-shot prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING AN EXAMPLE SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# Create a few-shot prompt template from either a set of examples or an \n",
    "# Example Selector object.\n",
    "# Setting up self-ask with search examples in few-shot learning.\n",
    "\n",
    "## CREATE THE EXAMPLE SET\n",
    "# Create a list of few-shot examples, each example is a dictionary with input\n",
    "# variables as keys and their corresponding values.\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\",\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "    \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\",\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "    \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\",\n",
    "  },\n",
    "]\n",
    "\n",
    "## CREATE A FORMATTER FOR THE FEW-SHOT EXAMPLES\n",
    "# PromptTemplate object to format few-shot examples into a string.\n",
    "example_prompt = PromptTemplate(\n",
    "  input_variables=[\"question\", \"answer\"],\n",
    "  template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "# print(example_prompt.format(**examples[0]))\n",
    "\n",
    "## FEED EXAMPLES AND FORMATTER TO FEWSHOTPROMPTEMPLATE\n",
    "# FewShotPromptTemplate object takes in few-shot examples and formatter.\n",
    "prompt = FewShotPromptTemplate(\n",
    "  examples=examples,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix=\"Question: {input}\",\n",
    "  input_variables=[\"input\"],\n",
    ")\n",
    "# print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING AN EXAMPLE SELECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FEED EXAMPLES INTO EXAMPLESELECTOR\n",
    "# `SemanticSimilarityExampleSelector` returns a subset of the most similar \n",
    "# few-shot examples for `FewShotPromptTemplate` by leveraging an embedding model \n",
    "# and a vector store to determine nearest neighbors.\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "  # List of available examples to choose from.\n",
    "  examples=examples,\n",
    "  # Embedding class for semantic similarity measurement.\n",
    "  embeddings=OpenAIEmbeddings(),\n",
    "  # VectorStore class for storing embeddings and performing similarity searches.\n",
    "  vectorstore_cls=Chroma,\n",
    "  # Number of examples to generate.\n",
    "  k=1,\n",
    ")\n",
    "\n",
    "# Choose the most similar input example.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "# for example in selected_examples:\n",
    "#   print(\"\\n\")\n",
    "#   for k, v in example.items():\n",
    "#     print(f\"{k}: {v}\")\n",
    "\n",
    "## FEED EXAMPLE SELECTOR INTO FEWSHOTPROMPTEMPLATE\n",
    "# FewShotPromptTemplate object with an example selector and formatter for \n",
    "# few-shot examples. \n",
    "prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix=\"Question: {input}\",\n",
    "  input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-shot examples for chat models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Fixed Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (ChatPromptTemplate, \n",
    "                               FewShotChatMessagePromptTemplate)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\"\"\"\n",
    "The common few-shot prompting technique is to use a fixed prompt example. \n",
    "Select, evaluate a chain without worrying about additional moving parts in production.\n",
    "\n",
    "Template's components include a list of dictionary examples and an example\n",
    "prompt, which converts each example into 1 or more messages using its \n",
    "format_messages method. Example is to create a human message and an\n",
    "AI message response, or a human message followed by a function call message.\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "  {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "  {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "]\n",
    "\n",
    "# assemble examples into the few-shot prompt template\n",
    "# Template for formatting individual examples.\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"human\", \"{input}\"),\n",
    "  (\"ai\", \"{output}\"),\n",
    "])\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  examples=examples,\n",
    ")\n",
    "# print(few_shot_prompt.format())\n",
    "\n",
    "# Assemble final prompt and use it with a model\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "  few_shot_prompt,\n",
    "  (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = final_prompt | ChatOpenAI(temperature=0.0)\n",
    "chain.invoke({\"input\": \"What's the square of a triangle?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dynamic few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (SemanticSimilarityExampleSelector,\n",
    "        ChatPromptTemplate, FewShotChatMessagePromptTemplate)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "\"\"\"\n",
    "Conditionally choose examples based on input by utilizing an example_selector\n",
    "- example_selector: selects few-shot examples and their return order. \n",
    "Implements BaseExampleSelector interface. \n",
    "Common example: SemanticSimilarityExampleSelector with vectorstore.\n",
    "- example_prompt: converts each example into 1 or more messages via \n",
    "format_messages method. Common example: one human message and one AI message \n",
    "response, or a human message followed by a function call message. \n",
    "\n",
    "These can be combined with messages and chat templates to create the final prompt.\n",
    "\"\"\"\n",
    "\n",
    "# Using a vectorstore to select examples based on semantic similarity requires \n",
    "# initial population.\n",
    "examples = [\n",
    "  {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "  {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "  {\"input\": \"2+4\", \"output\": \"6\"},\n",
    "  {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},\n",
    "  {\n",
    "    \"input\": \"Write me a poem about the moon\",\n",
    "    \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",\n",
    "  },\n",
    "]\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "# Create the example_selector with a vectorstore.\n",
    "# Instruct it to fetch only the top 2 examples.\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "  vectorstore=vectorstore,\n",
    "  k=2\n",
    ")\n",
    "# Prompt template loads examples using the `select_examples` method.\n",
    "# example_selector.select_examples({\"input\": \"hourse\"})\n",
    "\n",
    "# Create the prompt template using the example_selector.\n",
    "# Specify formatting for each example: 1 human message and 1 AI message.\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"human\", \"{input}\"),\n",
    "  (\"ai\", \"{output}\"),\n",
    "])\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "  # Input variables determine values for the example_selector.\n",
    "  input_variables=[\"input\"],\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    ")\n",
    "# print(few_shot_prompt.format(input=\"What's 3+3?\"))\n",
    "\n",
    "# Final prompt template assembly\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are wondrous wizard of math.\"),\n",
    "  few_shot_prompt,\n",
    "  (\"human\", \"{input}\"),\n",
    "])\n",
    "# print(final_prompt.format(input=\"What's 3+3?\"))\n",
    "\n",
    "# Connect model to the few-shot prompt.\n",
    "chain = final_prompt | ChatOpenAI(temperature=0.0)\n",
    "print(chain.invoke({\"input\": \"What's 3+3?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of `MessagePromptTemplate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "\"\"\"\n",
    "LangChain offers various MessagePromptTemplate options, such as \n",
    "AIMessagePromptTemplate, SystemMessagePromptTemplate, and HumanMessagePromptTemplate,\n",
    "for generating AI, system, and human messages.\n",
    "\n",
    "In cases where the chat model supports arbitrary role chat messages, use \n",
    "ChatMessagePromptTemplate to specify the role name.\n",
    "\"\"\"\n",
    "\n",
    "template = \"May the {subject} be with you\"\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "  role=\"Jedi\", template=template\n",
    ")\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n",
    "                               MessagesPlaceholder)\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\"\"\"\n",
    "LangChain's MessagesPlaceholder allows full control over message rendering. \n",
    "This is helpful when unsure of the correct role for message prompts or when \n",
    "inserting a message list during formatting.\n",
    "\"\"\"\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "  MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "  human_message_template\n",
    "])\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "  content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "chat_prompt.format_prompt(\n",
    "  conversation=[human_message, ai_message], word_count=\"10\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partial with strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\"\"\"\n",
    "Partial a prompt template when variables are received at different times. \n",
    "For instance, if a prompt template requires variables foo and baz, and the foo\n",
    "value is obtained early on but the baz value is obtained later, partialing the\n",
    "template with the foo value and using the partial template is a more efficient\n",
    "solution.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{foo}{bar}\")\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(\"1: \", partial_prompt.format(bar=\"baz\"))\n",
    "\n",
    "# Initialize the prompt with partial variables.\n",
    "prompt = PromptTemplate(\n",
    "  template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"}\n",
    ")\n",
    "print(\"2: \", prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partial with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\"\"\"\n",
    "When a variable needs to be consistently retrieved in a specific manner, \n",
    "such as date or time prompts. Instead of hard-coding the current date in the \n",
    "prompt or passing it with other input variables, it is convenient to utilize a \n",
    "function that always returns the current date.\n",
    "\"\"\"\n",
    "\n",
    "def _get_datetime():\n",
    "  now = datetime.now()\n",
    "  return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "  input_variables=[\"adjective\"],\n",
    "  partial_variables={\"date\": _get_datetime}\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Composing prompts using a PipelinePrompt. \n",
    "It allows reusing prompt parts and comprises:\n",
    "- Final prompt: The returned prompt.\n",
    "- Pipeline prompts: A list of tuples with a string name and a prompt template. \n",
    "Each formatted template is passed as a variable with the corresponding name to \n",
    "future templates.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "\n",
    "input_prompts = [\n",
    "  (\"introduction\", introduction_prompt),\n",
    "  (\"example\", example_prompt),\n",
    "  (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "  final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")\n",
    "print(pipeline_prompt.input_variables, \"\\n\")\n",
    "print(pipeline_prompt.format(\n",
    "    person=\"Elon Musk\",\n",
    "    example_q=\"What's your favorite car?\",\n",
    "    example_a=\"Tesla\",\n",
    "    input=\"What's your favorite social media site?\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quick Start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\"\"\"\n",
    "Chat models implement the Runnable interface. \n",
    "They support (a)invoke/stream/batch, and astream_log calls. \n",
    "They accept List[BaseMessage] as inputs, including str (converted to \n",
    "HumanMessage) and PromptValue.\n",
    "\"\"\"\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "messages = [\n",
    "  SystemMessage(content=\"You're a helpful assistant\"),\n",
    "  HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "\n",
    "# chat.invoke(messages)\n",
    "# chat.batch([messages])\n",
    "# for chunk in chat.stream(messages): \n",
    "#   print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# await chat.ainvoke(messages)\n",
    "# async for chunk in chat.astream(messages):\n",
    "#   print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "\"\"\"\n",
    "LangChain offers an optional caching layer for chat models, serving two purposes: \n",
    "1. Cost-saving by minimizing API calls for recurrent completions, \n",
    "2. Improved application speed through a reduction in API calls to the LLM provider.\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "set_llm_cache(InMemoryCache())\n",
    "# First occurrence, not in cache; hence, it takes longer.\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Function calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Defining functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Python function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "print(json.dumps(convert_to_openai_tool(multiply), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pydantic class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "print(json.dumps(convert_to_openai_tool(multiply), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LangChain Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "import json\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "class MultiplySchema(BaseModel):\n",
    "  \"\"\"Multiply tool schema.\"\"\"\n",
    "  \n",
    "  a: int = Field(..., description=\"First integer\")\n",
    "  b: int = Field(..., description=\"Second integer\")\n",
    "  \n",
    "class Multiply(BaseTool):\n",
    "  args_schema: Type[BaseModel] = MultiplySchema\n",
    "  name: str = \"multiply\"\n",
    "  description: str = \"Multiply two integers together.\"\n",
    "  \n",
    "  def _run(self, a: int, b: int, **kwargs: Any) -> Any:\n",
    "    return a * b\n",
    "\n",
    "print(json.dumps(convert_to_openai_tool(Multiply()), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# llm.invoke(\"What's 5 times three\", tools=[convert_to_openai_tool(multiply)])\n",
    "\n",
    "llm_with_tool = llm.bind_tools([multiply], tool_choice=\"multiply\")\n",
    "# llm_with_tool.invoke(\"what's 5 times three\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "for chunk in chat.stream(\"Write me a song about goldfish on the moon\"):\n",
    "  print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Tracking token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# All operations within the context manager are monitored. \n",
    "# Example: tracking multiple sequential calls.\n",
    "with get_openai_callback() as cb:\n",
    "  result = llm.invoke(\"Tell me a joke\")\n",
    "  print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# If a multi-step chain or agent is used, it will track all steps.\n",
    "\n",
    "llm = OpenAI()\n",
    "tools = load_tools([\"serpapi\", \"llm_math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "  response = agent.run(\n",
    "    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n",
    "  )\n",
    "  print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "  print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "  print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "  print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quick Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "LLMs, implementing the Runnable interface (LCEL's core). They support (a)invoke,\n",
    "(a)stream, (a)batch, and astream_log calls.\n",
    "\n",
    "They accept string inputs or objects convertible to string prompts, \n",
    "such as List[BaseMessage] and PromptValue.\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "prompt = \"What are some theories about the relationship between unemployment and inflation?\"\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.batch([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.ainvoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.abatch([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in llm.astream(prompt):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Custom LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\"\"\"\n",
    "Creating a custom LLM wrapper for LangChain requires implementing:\n",
    "1. A _call method: takes a string and optional stop words, returns a string.\n",
    "2. A _llm_type property: returns a string for logging.\n",
    "Optionally, implement:\n",
    "3. An _identifying_params property: returns a dictionary for class printing.\n",
    "\"\"\"\n",
    "\n",
    "# Implement a basic custom LLM returning the first n characters of the input.\n",
    "class CustomLLM(LLM):\n",
    "  n: int\n",
    "  \n",
    "  @property\n",
    "  def _llm_type(self) -> str:\n",
    "    return \"custom\"\n",
    "  \n",
    "  def _call(\n",
    "    self,\n",
    "    prompt: str,\n",
    "    stop: Optional[List[str]] = None,\n",
    "    run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    **kwargs: Any,\n",
    "  ) -> str:\n",
    "    if stop is not None:\n",
    "      raise ValueError(\"stop kwargs are not permitted.\")\n",
    "    return prompt[:self.n]\n",
    "  \n",
    "  @property\n",
    "  def _identifying_params(self) -> Mapping[str, Any]:\n",
    "    \"\"\"Get the identifying parameters.\"\"\"\n",
    "    return {\"n\": self.n}\n",
    "\n",
    "llm = CustomLLM(n=10)\n",
    "\n",
    "print(llm.invoke(\"This is a foobar thing\"))\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "\"\"\"\n",
    "LangChain offers an optional caching layer for LLMs, providing cost savings by \n",
    "minimizing repetitive API calls and improving application speed.\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# Initially, not cached, hence longer duration.\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The second time is faster.\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "LLMs implement the Runnable interface with default methods like (a)invoke, \n",
    "(a)batch, and (a)stream.\n",
    "\n",
    "The default behavior returns an Iterator (or AsyncIterator for async streaming) \n",
    "of the final result from the underlying LLM provider. \n",
    "While this doesn't offer token-by-token streaming without native LLM provider \n",
    "support, it ensures compatibility for code expecting iterators of tokens across \n",
    "all our LLM integrations.\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)\n",
    "for chunk in llm.stream(\"Write me a song about sparkling water.\"):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Document loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\"\"\"\n",
    "CSV (Comma-Separated Values) files are delimited text files where a comma \n",
    "separates values, creating data records with one or more fields per line. \n",
    "\"\"\"\n",
    "\n",
    "# Use a single-row-per-document approach when loading CSV data.\n",
    "loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Customizing the CSV parsing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv',\n",
    "                   csv_args={\n",
    "                     'delimiter': ',',\n",
    "                     'quotechar': '\"',\n",
    "                     'fieldnames': ['MLB Team', 'Payroll in millions', 'Wins'],\n",
    "                   })\n",
    "data = loader.load()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Specify a column to identify the document source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Utilize the source_column parameter to define the source for each document \n",
    "created from a row. If not specified, file_path will be used as the source for \n",
    "all documents generated from the CSV file. This is particularly beneficial for \n",
    "chains answering questions using sources.\n",
    "\"\"\"\n",
    "loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv', source_column=\"Team\")\n",
    "data = loader.load()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "\"\"\"\n",
    "HTML is the standard markup language for web documents. \n",
    "This guide explains loading HTML into a usable document format.\n",
    "\"\"\"\n",
    "\n",
    "loader = UnstructuredHTMLLoader('./example_data/fake-content.html')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading HTML with BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "loader = BSHTMLLoader('./example_data/fake-content.html')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = './example_data/facebook_chat.json'\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Using JSONLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Extract values under 'content' field within 'messages' key using JSONLoader.\n",
    "loader = JSONLoader(\n",
    "  file_path='./example_data/facebook_chat.json',\n",
    "  jq_schema='.messages[].content',\n",
    "  text_content=False,\n",
    ")\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JSON Lines file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "\"\"\"\n",
    "For loading documents from a JSON Lines file, set json_lines=True and provide \n",
    "jq_schema to extract page_content from a single JSON object.\n",
    "\"\"\"\n",
    "\n",
    "loader = JSONLoader(\n",
    "  file_path='./example_data/facebook_chat_messages.jsonl',\n",
    "  jq_schema='.content',\n",
    "  text_content=False,\n",
    "  json_lines=True,\n",
    ")\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "  file_path='./example_data/facebook_chat_messages.jsonl',\n",
    "  jq_schema='.',\n",
    "  content_key='sender_name',\n",
    "  json_lines=True,\n",
    ")\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extracting metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Define the metadata extraction function.\n",
    "\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "  metadata[\"sender_name\"] = record.get(\"sender_name\")\n",
    "  metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\n",
    "\n",
    "  return metadata\n",
    "\n",
    "\n",
    "loader = JSONLoader(\n",
    "  file_path='./example_data/facebook_chat.json',\n",
    "  jq_schema='.messages[]',\n",
    "  content_key=\"content\",\n",
    "  metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The metadata_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metadata extraction function.\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "  metadata[\"sender_name\"] = record.get(\"sender_name\")\n",
    "  metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\n",
    "\n",
    "  if \"source\" in metadata:\n",
    "      source = metadata[\"source\"].split(\"/\")\n",
    "      source = source[source.index(\"langchain\"):]\n",
    "      metadata[\"source\"] = \"/\".join(source)\n",
    "\n",
    "  return metadata\n",
    "\n",
    "\n",
    "loader = JSONLoader(\n",
    "  file_path='./example_data/facebook_chat.json',\n",
    "  jq_schema='.messages[]',\n",
    "  content_key=\"content\",\n",
    "  metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Common JSON structures with jq schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_path = './example_data/README.md'\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Retain Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unstructured creates distinct \"elements\" for various text chunks. \n",
    "By default, we merge them, but you can maintain separation by specifying\n",
    "mode=\"elements\".\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_path = './example_data/README.md'\n",
    "loader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Using PyPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load PDF using pypdf into an array of documents. Each document includes \n",
    "# page content and metadata, including page number.\n",
    "loader = PyPDFLoader(\"./example_data/layout-parser-paper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "# pprint(pages)\n",
    "\n",
    "faiss_index = FAISS.from_documents(documents=pages, embedding=OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"How chill the community be engaged?\", k=2)\n",
    "for doc in docs:\n",
    "  print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# The rapidocr-onnxruntime package extracts images as text.\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)\n",
    "pages = loader.load()\n",
    "print(pages[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using MathPix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import MathpixPDFLoader\n",
    "\n",
    "# Inspired by Daniel Gross's https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21\n",
    "loader = MathpixPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using Unstructured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retain Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unstructured segregates text into distinct \"elements\" internally. By default, \n",
    "it combines these elements, but you can preserve their separation by \n",
    "specifying mode=\"elements\".\n",
    "\"\"\"\n",
    "loader = UnstructuredPDFLoader(\n",
    "    \"example_data/layout-parser-paper.pdf\", mode=\"elements\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using PyPDFium2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "loader = PyPDFium2Loader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using PDFMiner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "loader = PDFMinerLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using PDFMiner to generate HTML text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "\n",
    "\"\"\"\n",
    "Helpful for semantically chunking texts into sections, parse the output HTML \n",
    "content with BeautifulSoup for structured information on font size, page numbers,\n",
    "PDF headers/footers, etc.\n",
    "\"\"\"\n",
    "\n",
    "loader = PDFMinerPDFasHTMLLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()[0]   # entire PDF is loaded as a single Document\n",
    "\n",
    "soup = BeautifulSoup(data.page_content, 'html.parser')\n",
    "content = soup.find_all('div')\n",
    "\n",
    "cur_fs = None\n",
    "cur_text = ''\n",
    "snippets = []   # first collect all snippets that have the same font size\n",
    "for c in content:\n",
    "  sp = c.find('span')\n",
    "  if not sp:\n",
    "    continue\n",
    "  st = sp.get('style')\n",
    "  if not st:\n",
    "    continue\n",
    "  fs = re.findall('font-size:(\\d+)px', st)\n",
    "  if not fs:\n",
    "    continue\n",
    "  fs = int(fs[0])\n",
    "  if not cur_fs:\n",
    "    cur_fs = fs\n",
    "  if fs == cur_fs:\n",
    "    cur_text += c.text\n",
    "  else:\n",
    "    snippets.append((cur_text, cur_fs))\n",
    "    cur_fs = fs\n",
    "    cur_text = c.text\n",
    "snippets.append((cur_text, cur_fs))\n",
    "# Strategies include removing duplicate snippets since headers/footers in a PDF \n",
    "# appear on multiple pages, indicating redundant information.\n",
    "\n",
    "cur_idx = -1\n",
    "semantic_snippets = []\n",
    "# Assumption: headings have higher font size than their respective content\n",
    "for s in snippets:\n",
    "  # if current snippet's font size > previous section's heading => it is a new heading\n",
    "  if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']:\n",
    "      metadata = {'heading': s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "      metadata.update(data.metadata)\n",
    "      semantic_snippets.append(Document(page_content='', metadata=metadata))\n",
    "      cur_idx += 1\n",
    "      continue\n",
    "\n",
    "  # if current snippet's font size <= previous section's content => content \n",
    "  # belongs to the same section (one can also create\n",
    "  # a tree like structure for sub sections if needed but that may require some \n",
    "  # more thinking and may be data specific)\n",
    "  if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] <= semantic_snippets[cur_idx].metadata['content_font']:\n",
    "      semantic_snippets[cur_idx].page_content += s[0]\n",
    "      semantic_snippets[cur_idx].metadata['content_font'] = max(\n",
    "          s[1], semantic_snippets[cur_idx].metadata['content_font'])\n",
    "      continue\n",
    "\n",
    "  # if current snippet's font size > previous section's content but less than \n",
    "  # previous section's heading than also make a new section (e.g. title of a PDF\n",
    "  # will have the highest font size but we don't want it to subsume all sections)\n",
    "  metadata = {'heading': s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "  metadata.update(data.metadata)\n",
    "  semantic_snippets.append(Document(page_content='', metadata=metadata))\n",
    "  cur_idx += 1\n",
    "\n",
    "print(semantic_snippets[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Fastest PDF parsing option with detailed metadata about PDF and pages, returns\n",
    "# one document per page.\n",
    "loader = PyMuPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "\n",
    "# You can pass options from PyMuPDF documentation as keyword arguments in the \n",
    "# load call, and they will be forwarded to the get_text() call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PyPDF Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"example_data/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using PDFPlumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# Outputs documents with detailed metadata for each PDF page. Each document \n",
    "# corresponds to a page.\n",
    "loader = PDFPlumberLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HTMLHeaderTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With an HTML string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "  <div>\n",
    "    <h1>Foo</h1>\n",
    "    <p>Some intro text about Foo.</p>\n",
    "    <div>\n",
    "      <h2>Bar main section</h2>\n",
    "      <p>Some intro text about Bar.</p>\n",
    "      <h3>Bar subsection 1</h3>\n",
    "      <p>Some text about the first subtopic of Bar.</p>\n",
    "      <h3>Bar subsection 2</h3>\n",
    "      <p>Some text about the second subtopic of Bar.</p>\n",
    "    </div>\n",
    "    <div>\n",
    "      <h2>Baz</h2>\n",
    "      <p>Some text about Baz</p>\n",
    "    </div>\n",
    "    <br>\n",
    "    <p>Some concluding text about Foo</p>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "  (\"h1\", \"Header 1\"),\n",
    "  (\"h2\", \"Header 2\"),\n",
    "  (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "pprint(html_header_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pipelined to another splitter, with html loaded from a web URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import (RecursiveCharacterTextSplitter,\n",
    "                                     HTMLHeaderTextSplitter)\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "headers_to_split_on = [\n",
    "  (\"h1\", \"Header 1\"),\n",
    "  (\"h2\", \"Header 2\"),\n",
    "  (\"h3\", \"Header 3\"),\n",
    "  (\"h4\", \"Header 4\"),\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "pprint(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split by character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# The simplest method splits based on characters (by default ) and measures \n",
    "# chunk length by the number of characters. How the text is split: by a single \n",
    "# character. How the chunk size is measured: by the number of characters.\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "  separator=\"\\n\\n\",\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200,\n",
    "  length_function=len,\n",
    "  is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([file])\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of passing metadata along with the documents, notice that it is split\n",
    "# along with the documents.\n",
    "metadatas = [{\"document\": 1}, {\"document\": 2}]\n",
    "documents = text_splitter.create_documents(\n",
    "    [state_of_the_union, state_of_the_union], metadatas=metadatas\n",
    ")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MarkdownHeaderTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import (MarkdownHeaderTextSplitter,\n",
    "                                     RecursiveCharacterTextSplitter)\n",
    "\n",
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "  (\"#\", \"Header 1\"),\n",
    "  (\"##\", \"Header 2\"),\n",
    "  (\"###\", \"Header 3\"),\n",
    "]\n",
    "# By default, MarkdownHeaderTextSplitter strips headers being split on from the \n",
    "# output chunks content. This can be disabled by setting strip_headers = False. \n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "  headers_to_split_on=headers_to_split_on, strip_headers=False,\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "pprint(md_header_splits); print()\n",
    "\n",
    "# Within each markdown group, any text splitter can then be applied.\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "pprint(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursively split JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json, requests\n",
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "\n",
    "# This large nested JSON object will be loaded into a Python dictionary.\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "\n",
    "# Recursively split JSON data to access or manipulate smaller chunks.\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "# The splitter can output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "# or a list of strings\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "# The json splitter does not split lists by default. The following preprocesses \n",
    "# the JSON, converting lists to a dictionary with index:item as key:val pairs.\n",
    "texts_lists = splitter.split_text(json_data=json_data, convert_lists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursively split by character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file_content = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=100,\n",
    "  chunk_overlap=20,\n",
    "  length_function=len,\n",
    "  is_separator_regex=False,\n",
    ")\n",
    "texts_splits = text_splitter.create_documents([file_content])\n",
    "pprint(texts_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Splits the text based on semantic similarity. This splits into sentences, \n",
    "# groups into sets of 3 sentences, and merges those similar in the embedding \n",
    "# space.\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file_content = f.read()\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "docs = text_splitter.create_documents([file_content])\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split by tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file_content = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "  chunk_size=100, chunk_overlap=0,\n",
    ")\n",
    "texts_splits = text_splitter.split_text(file_content)\n",
    "pprint(texts_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file_content = f.read()\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
    "text_splits = text_splitter.split_text(file_content)\n",
    "\n",
    "pprint(text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SentenceTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)\n",
    "text = \"Lorem \"\n",
    "\n",
    "count_start_and_stop_tokens = 2\n",
    "text_token_count = text_splitter.count_tokens(text=text) - count_start_and_stop_tokens\n",
    "print(f\"text_token_count: {text_token_count}\")\n",
    "\n",
    "token_multiplier = text_splitter.maximum_tokens_per_chunk // text_token_count + 1\n",
    "text_to_split = text*token_multiplier\n",
    "print(\n",
    "    f\"tokens in text to split: {text_splitter.count_tokens(text=text_to_split)}\")\n",
    "\n",
    "text_chunks = text_splitter.split_text(text=text_to_split)\n",
    "pprint(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file_content = f.read()\n",
    "  \n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "text_splits = text_splitter.split_text(file_content)\n",
    "\n",
    "pprint(text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hugging Face tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "with open(\"./example_data/state_of_the_union.txt\") as f:\n",
    "  file_content = f.read()\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "  tokenizer=tokenizer, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "text_splits = text_splitter.split_text(file_content)\n",
    "pprint(text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text embedding models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# embed_documents\n",
    "# Embed list of texts\n",
    "docs = [\n",
    "    \"Hi there!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"My friends call me World\",\n",
    "    \"Hello World!\"\n",
    "]\n",
    "embedded_docs = embeddings_model.embed_documents(docs)\n",
    "print(len(embedded_docs), len(embedded_docs[0]))\n",
    "print(embedded_docs[0]); print()\n",
    "\n",
    "# embed_query\n",
    "# Embed single query\n",
    "# Embed a text for comparing to others.\n",
    "query = \"What was the name mentioned in the conversation?\"\n",
    "embedded_query = embeddings_model.embed_query(query)\n",
    "print(len(embedded_query))\n",
    "print(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CacheBackedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore, InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Using local file system for storing embeddings and FAISS vector store for retrieval\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "# To use a different ByteStore, specify it when creating `CacheBackedEmbeddings`.\n",
    "# store = InMemoryByteStore()\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "  underlying_embeddings=embeddings_model,\n",
    "  document_embedding_cache=store,\n",
    "  namespace=embeddings_model.model\n",
    ")\n",
    "\n",
    "# The cache is initially empty.\n",
    "print(list(store.yield_keys()), \"\\n\")\n",
    "\n",
    "# Load the document, split into chunks, embed each, load into the vector store.\n",
    "raw_documents = TextLoader(\"./example_data/state_of_the_union.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create the vector store.\n",
    "db = FAISS.from_documents(documents, cached_embedder)\n",
    "# Re-creating the vector store is faster, as it avoids re-computing embeddings.\n",
    "\n",
    "# Some created embeddings\n",
    "print(list(store.yield_keys())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Get started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load the document\n",
    "raw_documents = TextLoader(\"./example_data/state_of_the_union.txt\").load()\n",
    "# Split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# Embed each chunk. Load it into the vector store.\n",
    "db_chroma = Chroma.from_documents(documents, embeddings)\n",
    "db_faiss = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Similarity search\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "result_chroma = db_chroma.similarity_search(query)\n",
    "result_faiss = db_faiss.similarity_search(query)\n",
    "print(result_chroma[0].page_content)\n",
    "print(result_faiss[0].page_content)\n",
    "print('-'*80)\n",
    "\n",
    "# Similarity document search based on an embedding vector\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "result_vector_chroma = db_chroma.similarity_search_by_vector(embedding_vector)\n",
    "result_vector_faiss = db_faiss.similarity_search_by_vector(embedding_vector)\n",
    "print(result_vector_chroma[0].page_content)\n",
    "print(result_vector_faiss[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asynchronous operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.qdrant import Qdrant\n",
    "\n",
    "# Create a vector store asynchronously\n",
    "db = await Qdrant.afrom_documents(documents, embeddings, \"http://localhost:6333\")\n",
    "\n",
    "# Similarity search\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = await db.asimilarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# Similarity search by vector\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "docs = await db.asimilarity_search_by_vector(embedding_vector)\n",
    "\n",
    "# Maximal Marginal Relevance optimizes for query similarity and diversity among \n",
    "# selected documents. This feature is also supported in the async API.\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "found_docs = await Qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10)\n",
    "for i, doc in enumerate(found_docs):\n",
    "    print(f\"{i + 1}.\", doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"What did the president say about technology?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector store-backed retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import faiss\n",
    "\n",
    "loader = TextLoader(\"./example_data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = faiss.FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "  # The default mode for the VectorStoreRetriever is similarity search. \n",
    "  # Specify the search type as maximum marginal relevance (if supported)\n",
    "  search_type=\"mmr\",\n",
    "  \n",
    "  # Set a retrieval method with a similarity score threshold, returning only \n",
    "  # documents surpassing that threshold.\n",
    "  search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5},\n",
    "  \n",
    "  # Specify search kwargs like `k` for retrieval customization.\n",
    "  search_kwargs={\"k\": 1}\n",
    ")\n",
    "\n",
    "query = \"what did he say about ketanji brown jackson\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MultiQueryRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = chroma.Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# Specify the LLM for query generation, and the retriever handles the rest.\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Supply a prompt with an output parser to split results into a list of queries.\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "\n",
    "\n",
    "class LineList(BaseModel):\n",
    "  # \"lines\" is the key (attribute name) of the parsed output\n",
    "  lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__(pydantic_object=LineList)\n",
    "\n",
    "  def parse(self, text: str) -> LineList:\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate \n",
    "five different versions of the given user question to retrieve relevant documents\n",
    "from a vector database. By generating multiple perspectives on the user question, \n",
    "your goal is to help the user overcome some of the limitations of the \n",
    "distance-based similarity search. Provide these alternative questions separated \n",
    "by newlines. Original question: {question}\"\"\"\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "multi_query_retriever = MultiQueryRetriever(\n",
    "    retriever=retriever,\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\",  # key (attribute name) of the parsed output\n",
    ")\n",
    "\n",
    "query = \"What does the course say about regression?\"\n",
    "unique_docs = multi_query_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contextual compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain_community.vectorstores import faiss\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import (LLMChainExtractor,\n",
    "      LLMChainFilter, EmbeddingsFilter, DocumentCompressorPipeline)\n",
    "\n",
    "# Initializes a vector store retriever, stores the data in chunks. The retriever\n",
    "# returns relevant and irrelevant docs, with relevant ones containing excess\n",
    "# irrelevant information.\n",
    "data = TextLoader(\"./example_data/state_of_the_union.txt\").load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = faiss.FAISS.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Extracts relevant content for the query from documents.\n",
    "llm_chain_extractor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Use an LLM chain to filter out or return initially retrieved \n",
    "# documents without manipulating their contents.\n",
    "llm_chain_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "# Embeddeds documents and query, and returning documents with similar embeddings\n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings, similarity_threshold=0.76\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "# Filters out redundant documents based on embedding similarity.\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "# Combines compressors in sequence.\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "  transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "compressors = {\n",
    "  \"llm_chain_extractor\": llm_chain_extractor,\n",
    "  \"llm_chain_filter\": llm_chain_filter,\n",
    "  \"embeddings_filter\": embeddings_filter,\n",
    "  \"pipeline_compressor\": pipeline_compressor,  \n",
    "}\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressors[\"pipeline_compressor\"], base_retriever=retriever\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import faiss\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "doc_list_1 = [\n",
    "  \"I like apples\",\n",
    "  \"I like oranges\",\n",
    "  \"Apples and oranges are fruits\",\n",
    "]\n",
    "doc_list_2 = [\n",
    "  \"You like apples\",\n",
    "  \"You like oranges\",\n",
    "]\n",
    "\n",
    "retriever_bm25 = BM25Retriever.from_texts(\n",
    "  doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "retriever_bm25.k = 2\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore_faiss = faiss.FAISS.from_texts(\n",
    "  doc_list_2, embedding, metadatas=[{\"source\": 2}] * len(doc_list_2)\n",
    ")\n",
    "retriever_faiss = vectorstore_faiss.as_retriever(\n",
    "  search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "retriever_ensemble = EnsembleRetriever(\n",
    "  retrievers=[retriever_bm25, retriever_faiss], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "pprint(retriever_ensemble.invoke(\"apples\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long-Context Reorder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "\n",
    "texts = [\n",
    "  \"Basquetball is a great sport.\",\n",
    "  \"Fly me to the moon is one of my favourite songs.\",\n",
    "  \"The Celtics are my favourite team.\",\n",
    "  \"This is a document about the Boston Celtics\",\n",
    "  \"I simply love going to the movies\",\n",
    "  \"The Boston Celtics won the game by 20 points\",\n",
    "  \"This is just a random text.\",\n",
    "  \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "  \"L. Kornet is one of the best Celtics players.\",\n",
    "  \"Larry Bird was an iconic NBA player.\",\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = chroma.Chroma.from_texts(texts, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "# Reorder documents: Less relevant document in the middle, more relevant at the\n",
    "# beginning/end.\n",
    "reodering = LongContextReorder()\n",
    "\n",
    "stuff_prompt_template = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "stuff_prompt = PromptTemplate(\n",
    "  template=stuff_prompt_template, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=stuff_prompt)\n",
    "\n",
    "# Prepare and run a custom Stuff chain with reordered docs as context.\n",
    "# Override prompts\n",
    "document_prompt = PromptTemplate(\n",
    "  input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "  llm_chain=llm_chain,\n",
    "  document_prompt=document_prompt,\n",
    "  document_variable_name=document_variable_name,\n",
    ")\n",
    "\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "reodered_docs = reodering.transform_documents(docs)\n",
    "pprint(chain.run(input_documents=reodered_docs, query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MultiVector Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain.retrievers.multi_vector import (MultiVectorRetriever,\n",
    "                                               SearchType)\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "loaders = [\n",
    "  TextLoader(\"./example_data/paul_graham_essay.txt\"),\n",
    "  TextLoader(\"./example_data//state_of_the_union.txt\"),\n",
    "]\n",
    "data = []\n",
    "for loader in loaders:\n",
    "  data.extend(loader.load())\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "id_key = \"doc_id\"\n",
    "byte_store = InMemoryByteStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Smaller chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to index the child chunks\n",
    "vectorstore = chroma.Chroma(\n",
    "  collection_name=\"full_documents\", embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "retriever = MultiVectorRetriever(\n",
    "  vectorstore=vectorstore,\n",
    "  byte_store=byte_store,\n",
    "  id_key=id_key,\n",
    ")\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "  _id = doc_ids[i]\n",
    "  _sub_docs = child_text_splitter.split_documents([doc])\n",
    "  for _doc in _sub_docs:\n",
    "    _doc.metadata[id_key] = _id\n",
    "  sub_docs.extend(_sub_docs)\n",
    "\n",
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "# The retriever performs a similarity search by default on the vector database. \n",
    "# LangChain Vector Stores also support Max Marginal Relevance search. \n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "# Vectorstore alone retrieves the small chunks\n",
    "print(retriever.vectorstore.similarity_search(\"justice breyer\")[0])\n",
    "# Retriever returns larger chunks\n",
    "print(len(retriever.get_relevant_documents(\"justice breyer\")[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Summarize the following document:\n",
    "{doc}\n",
    "\"\"\"\n",
    "chain = (\n",
    "  {\"doc\": lambda x: x.page_content}\n",
    "  | ChatPromptTemplate.from_template(template)\n",
    "  | ChatOpenAI(max_retries=0)\n",
    "  | StrOutputParser()\n",
    ")\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# Used to index the child chunks\n",
    "vectorstore = chroma.Chroma(\n",
    "  collection_name=\"summaries\", embedding_function=embeddings\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "retriever = MultiVectorRetriever(\n",
    "  vectorstore=vectorstore,\n",
    "  byte_store=byte_store,\n",
    "  id_key=id_key,\n",
    ")\n",
    "\n",
    "summary_docs = [\n",
    "  Document(page_content=summary, metadata={id_key: doc_ids[i]})\n",
    "           for i, summary in enumerate(summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "query = \"justice breyer\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hypothetical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "  {\n",
    "    \"name\": \"hypothetical_questions\",\n",
    "    \"description\": \"Generate hypothetical questions\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"questions\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"},\n",
    "        },\n",
    "    },\n",
    "      \"required\": [\"questions\"],\n",
    "    },\n",
    "  }\n",
    "]\n",
    "\n",
    "template = \"\"\"\n",
    "Generate a list of exactly 3 hypothetical questions that the below document \n",
    "could be used to answer:\n",
    "\n",
    "{doc}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chat = ChatOpenAI(max_retries=0).bind(\n",
    "  functions=functions, function_call={\"name\": \"hypothetical_questions\"},\n",
    ")\n",
    "\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    "\n",
    "chain = (\n",
    "  {\"doc\": lambda x: x.page_content}\n",
    "  | prompt\n",
    "  | chat\n",
    "  | output_parser\n",
    ")\n",
    "hypothetical_question = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# Used to index the child chunks\n",
    "vectorstore = chroma.Chroma(\n",
    "  collection_name=\"hypo_questions\", embedding_function=embeddings\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "retriever = MultiVectorRetriever(\n",
    "  vectorstore=vectorstore,\n",
    "  byte_store=byte_store,\n",
    "  id_key=id_key,\n",
    ")\n",
    "\n",
    "question_docs = []\n",
    "for i, question_list in enumerate(hypothetical_question):\n",
    "  question_docs.extend(\n",
    "      [Document(page_content=s, metadata={\n",
    "                id_key: doc_ids[i]}) for s in question_list]\n",
    "  )\n",
    "\n",
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parent Document Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "loaders = [\n",
    "  TextLoader(\"./example_data/paul_graham_essay.txt\"),\n",
    "  TextLoader(\"./example_data/state_of_the_union.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders: docs.extend(loader.load())\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Retrieving full documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only a child splitter is specified, yielding two keys for two added documents.\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# `vectorstore` used for indexing child chunks\n",
    "vectorstore = chroma.Chroma(collection_name=\"full_documents\", \n",
    "                            embedding_function=embeddings)\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "  vectorstore=vectorstore,\n",
    "  docstore=store,\n",
    "  child_splitter=child_text_splitter,\n",
    ")\n",
    "retriever.add_documents(docs, ids=None)\n",
    "\n",
    "print(list(store.yield_keys())) # 2 docs\n",
    "\n",
    "sub_docs = vectorstore.similarity_search(\"justice breyer\") # small chunks\n",
    "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\") # large docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Retrieving larger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full documents may be too large to retrieve. Split raw documents into larger \n",
    "# chunks, further split them into smaller chunks and index. During retrieval, \n",
    "# the larger chunks are retrieved.\n",
    "\n",
    "parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "vectorstore = chroma.Chroma(collection_name=\"split_parents\", \n",
    "                            embedding_function=embeddings)\n",
    "store = InMemoryStore()  # The storage layer for the parent documents\n",
    "retriever = ParentDocumentRetriever(\n",
    "  vectorstore=vectorstore,\n",
    "  docstore=store,\n",
    "  child_splitter=child_text_splitter,\n",
    "  parent_splitter=parent_text_splitter,\n",
    ")\n",
    "retriever.add_documents(docs)\n",
    "\n",
    "print(len(list(store.yield_keys())))  # larger chunks\n",
    "\n",
    "query = \"justice breyer\"\n",
    "sub_docs = vectorstore.similarity_search(query)  # small chunks\n",
    "retrieved_docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-querying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "with open(\"./example_data/movie_data.json\") as json_file:\n",
    "    movie_data = json.load(json_file)\n",
    "\n",
    "docs = [Document(**doc) for doc in movie_data[\"docs\"]]\n",
    "metadata_field_info = [AttributeInfo(**info)\n",
    "                       for info in movie_data[\"metadata_field_info\"]]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "vectorstore = chroma.Chroma.from_documents(docs, embeddings)\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,  # Filter k\n",
    ")\n",
    "\n",
    "pprint(retriever.invoke(\"I want to watch a movie rated higher than 8.5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Constructing from scratch with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain.chains.query_constructor.base import (AttributeInfo,\n",
    "          StructuredQueryOutputParser, get_query_constructor_prompt)\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "\n",
    "with open(\"./example_data/movie_data.json\") as json_file:\n",
    "    movie_data = json.load(json_file)\n",
    "\n",
    "docs = [Document(**doc) for doc in movie_data[\"docs\"]]\n",
    "metadata_field_info = [AttributeInfo(**info)\n",
    "                       for info in movie_data[\"metadata_field_info\"]]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "vectorstore = chroma.Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "prompt = get_query_constructor_prompt(\n",
    "  document_content_description,\n",
    "  metadata_field_info,\n",
    ")\n",
    "\n",
    "# The query constructor is crucial for a self-query retriever. \n",
    "# Make sure fine-tuning prompts, prompt examples, attribute descriptions, etc. \n",
    "output_parser = StructuredQueryOutputParser.from_components()\n",
    "query_constructor = prompt | llm | output_parser\n",
    "# pprint(prompt.format(query=\"dummy question\"))\n",
    "\n",
    "query = \"What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers\"\n",
    "result = query_constructor.invoke({\"query\": query})\n",
    "# pprint(result)\n",
    "\n",
    "# The structured query translator translates the `StructuredQuery` into a \n",
    "# metadata filter in the vector store's syntax.\n",
    "retriever = SelfQueryRetriever(\n",
    "    query_constructor=query_constructor,\n",
    "    vectorstore=vectorstore,\n",
    "    structured_query_translator=ChromaTranslator(),\n",
    ")\n",
    "\n",
    "query = \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n",
    "result = retriever.invoke(query)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time-weighted vector store retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Quickstart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import elasticsearch\n",
    "\n",
    "# Initialize a vector store and embed:\n",
    "collection_name = \"test_index\"\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = elasticsearch.ElasticsearchStore(\n",
    "    es_url=\"http://localhost:9200\", index_name=collection_name, embedding=embedding\n",
    ")\n",
    "\n",
    "# Initialize a record manager with a suitable namespace\n",
    "# Considering the vector store and collection name, like `redis/my_docs`, \n",
    "# `chromadb/my_docs`, or `postgres/my_docs`.\n",
    "vectorstore_name = \"elasticsearch\"\n",
    "namespace = f\"{vectorstore_name}/{collection_name}\"\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=\"sqlite:///record_manager_cache.sql\"\n",
    ")\n",
    "# Create a schema before utilizing the record manager.\n",
    "record_manager.create_schema()\n",
    "\n",
    "# Index test documents in an empty vector store.\n",
    "doc1 = Document(page_content=\"kitty\", metadata={\"source\": \"kitty.txt\"})\n",
    "doc2 = Document(page_content=\"doggy\", metadata={\"source\": \"doggy.txt\"})\n",
    "\n",
    "def _clear():\n",
    "  \"\"\"Hacky helper method to clear content.\"\"\"\n",
    "  index([], record_manager, vectorstore,\n",
    "      cleanup=\"full\", source_id_key=\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None deletion mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"incremental\" deletion mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"full\" deletion mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using with loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Quickstart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import uuid\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import faiss\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Build an agent with two tools: one to look up online information and another\n",
    "# to search specific data in an index. \n",
    "# `Agents` follow a self-determined, input-dependent sequence of steps, LangSmith\n",
    "# helps debugging (observability).\n",
    "\n",
    "#*** DEFINE TOOLS \n",
    "\n",
    "## TAVILY\n",
    "search = TavilySearchResults()\n",
    "# pprint(search.invoke(\"what is the weather in SF\"))\n",
    "\n",
    "## RETRIEVER\n",
    "# Create a retriever over our data. \n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = faiss.FAISS.from_documents(documents, embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "# pprint(retriever.get_relevant_documents(\"how to upload a dataset\"))\n",
    "\n",
    "# Convert index for retrieval into a tool (`agent` format).\n",
    "retriever_tool = create_retriever_tool(\n",
    "  retriever=retriever,\n",
    "  name=\"langsmith_search\",\n",
    "  description=\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "\n",
    "## TOOLS\n",
    "# list the tools for downstream use\n",
    "tools = [search, retriever_tool]\n",
    "\n",
    "# *** CREATE THE AGENT with the LLM, the prompt, and the tools\n",
    "# Choose the `LLM` to guide the `agent`.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Choose the prompt to guide the agent.\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# pprint(prompt.messages)\n",
    "\n",
    "# The `agent` takes input and decides actions, the `AgentExecutor` executes them\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "# Combine the brain `agent` with tools in `AgentExecutor` to repeatedly call\n",
    "# the agent and execute tools.\n",
    "# The agent is stateless and doesn't remember prior interactions.\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "#*** RUN THE AGENT\n",
    "query = \"how can langsmith help with testing?\"\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "# pprint(result)\n",
    "\n",
    "#*** ADDING IN MEMORY\n",
    "# To provide memory, pass in previous `chat_history`. The variable name must be\n",
    "# `chat_history` due to the specific prompt used; changing the prompt allows a\n",
    "# different variable name.\n",
    "# Initialize `chat_history` with an empty list for the first chat message.\n",
    "# To automatically track messages, wrap the process in a\n",
    "# `RunnableWithMessageHistory`\n",
    "message_history = ChatMessageHistory()\n",
    "session_id = uuid.uuid4()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "  agent_executor,\n",
    "  # `SessionId` is typically required in real-world scenarios\n",
    "  lambda session_id: message_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "agent_with_chat_history.invoke(\n",
    "  {\"input\": \"hi! I'm bob\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"what's my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Agent Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain import hub\n",
    "from langchain_community.tools import tavily_search\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "#* Initialize Tools\n",
    "tools = [tavily_search.TavilySearchResults(max_results=1)]\n",
    "\n",
    "#* Create Agent\n",
    "# Modifiable prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "pprint(prompt.messages)\n",
    "\n",
    "# Choose the `LLM` to drive the agent.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "# Construct the OpenAI Functions agent\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "#* Run Agent\n",
    "# Create an agent executor with the agent and tools.\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "# agent_executor.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "#* Using with chat history\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": [\n",
    "        HumanMessage(content=\"hi! my name is bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "    ],\n",
    "    \"input\": \"what's my name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.tools import tavily_search\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "#* Initialize Tools\n",
    "tools = [\n",
    "  tavily_search.TavilySearchResults(max_results=1),\n",
    "]\n",
    "\n",
    "#* Create Agent\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "#* Run Agent Using with chat history\n",
    "agent_executor.invoke({\n",
    "  \"chat_history\": [\n",
    "    HumanMessage(content=\"hi! my name is bob\"),\n",
    "    AIMessage(content=\"Hello Bob! How can I assist you today?\")\n",
    "  ],\n",
    "  \"input\": \"what's my name? Don't use tools to look this up unless you NEED to\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Chat Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import tavily_search\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "#* Initialize Tools\n",
    "tools = [\n",
    "  tavily_search.TavilySearchResults(max_results=1),\n",
    "]\n",
    "\n",
    "#* Create Agent\n",
    "prompt = hub.pull(\"hwchase17/react-chat-json\")\n",
    "llm = ChatOpenAI()\n",
    "agent = create_json_chat_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "  agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "#* Run Agent Using with chat history\n",
    "agent_executor.invoke({\n",
    "  \"chat_history\": [\n",
    "    HumanMessage(content=\"hi! my name is bob\"),\n",
    "    AIMessage(content=\"Hello Bob! How can I assist you today?\")\n",
    "  ],\n",
    "  \"input\": \"what's my name?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Your name is Bob.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"what's my name? Do not use tools unless you have to\",\n",
       " 'chat_history': [HumanMessage(content='hi! my name is bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?')],\n",
       " 'output': 'Your name is Bob.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Construct the JSON agent\n",
    "agent = create_structured_chat_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what's my name? Do not use tools unless you have to\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"hi! my name is bob\"),\n",
    "            AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-ask with search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.tools import tavily_search\n",
    "from langchain_community.llms import fireworks\n",
    "from langchain.agents import create_self_ask_with_search_agent, AgentExecutor\n",
    "\n",
    "#* Initialize Tools\n",
    "tools = [\n",
    "  tavily_search.TavilyAnswer(max_results=1, name=\"Intermediate Answer\"),\n",
    "]\n",
    "\n",
    "#* Create Agent\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "llm = fireworks.Fireworks()\n",
    "agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "#* Run Agent\n",
    "agent_executor.invoke({\n",
    "  \"input\": \"What is the hometown of the reigning men's U.S. Open champion?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How-to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\"\"\"\n",
    "Creates a custom agent using OpenAI Tool Calling. \n",
    "Add memory for conversation enablement.\n",
    "\"\"\"\n",
    "\n",
    "#* Load the LLM\n",
    "# Load the language model to control the agent.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "#* Define Tools\n",
    "# Python function to calculate word length. The function docstring is crucial.\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "  \"\"\"Returns the length of a word.\"\"\"\n",
    "  return len(word)\n",
    "\n",
    "tools = [get_word_length]\n",
    "# get_word_length.invoke(\"abc\")\n",
    "\n",
    "#* Create Prompt for OpenAI Function Calling for tool usage\n",
    "# Input variables: string `input` (user objective) and `agent_scratchpad` \n",
    "# (sequence of messages with agent tool invocations and outputs).\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\n",
    "    \"system\",\n",
    "    \"You are very powerful assistant, but don't know current events\",\n",
    "  ),\n",
    "  (\n",
    "    \"user\",\n",
    "    \"{input}\"\n",
    "  ),\n",
    "  MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "#* Bind tools to LLM\n",
    "# `Agent` identifies available tools by calling LLMs, trained to know when to \n",
    "# use them. We format and pass tools to the agent in OpenAI tool format. \n",
    "# Binding functions ensures tools are passed each time the model is invoked.\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "#* Create the Agent\n",
    "\n",
    "\n",
    "#* Adding memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Agent as an Iterator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returning Structured Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle parsing errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access intermediate steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cap the max number of iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeouts for agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangServe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
