{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()\n",
    "\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "  print(\n",
    "    f\"\\n{'-' * 100}\\n\".join(\n",
    "      [f\"Document {i+1}:\\n\\n\" +\n",
    "        d.page_content for i, d in enumerate(docs)]\n",
    "  )\n",
    ")# pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# James Briggs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Better Llama 2 with Retrieval Augmented Generation (RAG)](https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb)\n",
    "`RAG`, `LLAMA2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots with RAG: LangChain Full Walkthrough\n",
    "\n",
    "`RAG`, `PINECONE`, `OPENAI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os, time\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import pinecone\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "import langchain_pinecone\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "\n",
    "def augment_prompt(vectorstore: langchain_pinecone.Pinecone,\n",
    "                   query: str, k: int = 3) -> str:\n",
    "  # get top 3 results from knowledge base\n",
    "  results = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "  # get the text from the results\n",
    "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "\n",
    "  augmented_prompt = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Using the contexts below, answer the query.\n",
    "  \n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\"\n",
    "\n",
    "  return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset = load_dataset(\n",
    "  \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "  split=\"train\"\n",
    ")\n",
    "\n",
    "\n",
    "data = dataset.to_pandas()\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "  i_end = min(len(data), i + batch_size)\n",
    "  # get batch of data\n",
    "  batch = data.iloc[i:i_end]\n",
    "  # generate unique ids for each chunk\n",
    "  ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "  # get text to embed\n",
    "  texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "  # embed text\n",
    "  embeds = embed_model.embed_documents(texts)\n",
    "  # get metadata to store in Pinecone\n",
    "  metadata = [\n",
    "    {'text': x['chunk'],\n",
    "    'source': x['source'],\n",
    "    'title': x['title']} for i, x in batch.iterrows()\n",
    "  ]\n",
    "  # add to Pinecone\n",
    "  index.upsert(vectors=zip(ids, embeds, metadata))\n",
    "\n",
    "pprint(index.describe_index_stats())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.04838,\n",
      " 'namespaces': {'': {'vector_count': 4838}},\n",
      " 'total_vector_count': 4838}\n"
     ]
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "spec = pinecone.ServerlessSpec(cloud=\"gcp-starter\", region=\"Iowa (us-central1)\")\n",
    "\n",
    "index_name = \"chatbot\"\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "  pc.create_index(index_name, dimension=1536, metric=\"dotproduct\", spec=spec)\n",
    "  while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "pprint(index.describe_index_stats())\n",
    "\n",
    "text_field = \"text\"  # metadata field contains text chunk\n",
    "vectorstore = langchain_pinecone.Pinecone(\n",
    "  index, embed_model, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "prompt = augment_prompt(vectorstore=vectorstore, query=query, k=3)\n",
    "res = chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tech With Tim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANCED Python AI Agent Tutorial - Using RAG\n",
    "\n",
    "`RAG`, `AGENT`, `TOOLS`, `MULTILE DATA SOURCES`, `PROMPT INSTRUCTIONS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tool_csv.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "import pandas as pd\n",
    "import os\n",
    "from llama_index.core.query_engine import PandasQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "instruction_str = \"\"\"\\\n",
    "1. Convert the query to executable Python code using Pandas.\n",
    "2. The final line of code should be a Python expression that can be called with the `eval()` function.\n",
    "3. The code should represent a solution to the query.\n",
    "4. PRINT ONLY THE EXPRESSION.\n",
    "5. Do not quote the expression.\"\"\"\n",
    "\n",
    "template_pandas = \"\"\"\\\n",
    "You are working with a pandas dataframe in Python.\n",
    "The name of the dataframe is `df`.\n",
    "This is the result of `print(df.head())`:\n",
    "{df_str}\n",
    "\n",
    "Follow these instructions:\n",
    "{instruction_str}\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Expression:  \"\"\"\n",
    "prompt_template_pandas = PromptTemplate(template_pandas)\n",
    "\n",
    "data_csv = pd.read_csv(\"./data/population.csv\")\n",
    "engine_query_csv = PandasQueryEngine(\n",
    "  df=data_csv, verbose=True, instruction_str=instruction_str\n",
    ")\n",
    "engine_query_csv.update_prompts({\"pandas_prompt\": prompt_template_pandas})\n",
    "\n",
    "tool_csv = QueryEngineTool(\n",
    "    query_engine=engine_query_csv,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"population_data\",\n",
    "        description=\"this gives information about the world population and demographics\",\n",
    "    )\n",
    ")\n",
    "# pprint(engine_query_csv.query(\"what is the population of canada?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tool_note.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "note_path = \"./data/notes.txt\"\n",
    "\n",
    "def save_note(note: str, note_path: str):\n",
    "  if not os.path.exists(note_path):\n",
    "    open(note_path, \"w\")\n",
    "  \n",
    "  with open(note_path, \"a\") as f:\n",
    "    f.writelines([f\"{note}\\n\"])\n",
    "    \n",
    "  return \"Note Saved\"\n",
    "\n",
    "tool_note = FunctionTool.from_defaults(\n",
    "  fn=save_note,\n",
    "  name=\"note_saver\",\n",
    "  description=\"this tool can save a text based note to a file for the user\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tool_pdf.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index 'index_pdf'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (StorageContext, VectorStoreIndex, \n",
    "                              load_index_from_storage)\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "reader_pdf = PDFReader()\n",
    "\n",
    "def get_index(data, index_name):\n",
    "  index = None\n",
    "  \n",
    "  if not os.path.exists(index_name):\n",
    "    print(f\"Building index '{index_name}'\")\n",
    "    index = VectorStoreIndex.from_documents(data, show_progress=True)\n",
    "    index.storage_context.persist(persist_dir=index_name)\n",
    "  else:\n",
    "    print(f\"Loading index '{index_name}'\")\n",
    "    index = load_index_from_storage(\n",
    "      StorageContext.from_defaults(persist_dir=index_name)\n",
    "    )\n",
    "  \n",
    "  return index\n",
    "\n",
    "data_pdf = reader_pdf.load_data(file=\"./data/Canada.pdf\") # docs\n",
    "index_pdf = get_index(data_pdf, \"index_pdf\")\n",
    "engine_query_pdf = index_pdf.as_query_engine()\n",
    "tool_pdf = QueryEngineTool(\n",
    "  query_engine=engine_query_pdf,\n",
    "  metadata=ToolMetadata(\n",
    "    name=\"canada_data\",\n",
    "    description=\"this gives detailed information about the country canada\",\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "tools = [\n",
    "  tool_note,\n",
    "  tool_csv,\n",
    "  tool_pdf,\n",
    "]\n",
    "\n",
    "context = \"\"\"\\\n",
    "Purpose: The primary role of this agent is to assist users by providing accurate\\\n",
    "information about world population statistics and details about a country.\"\"\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = ReActAgent.from_tools(tools=tools, llm=llm, verbose=True, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I can use the \"canada_data\" tool to get information about the language token in Canada.\n",
      "Action: canada_data\n",
      "Action Input: {'input': 'language'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Canada is a linguistically diverse country with a significant portion of the population being able to speak both English and French. French is used as a language of instruction, in courts, and for government services in addition to English in some provinces. There are also Indigenous languages with official status in certain territories. Additionally, Canada is home to various sign languages, including American Sign Language (ASL) and Quebec Sign Language (LSQ).\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have enough information to answer the question.\n",
      "Answer: In Canada, the language token is diverse. English and French are the official languages, with French being used in some provinces for government services, courts, and instruction. There are also Indigenous languages with official status in certain territories, as well as various sign languages such as American Sign Language (ASL) and Quebec Sign Language (LSQ).\n",
      "\u001b[0mIn Canada, the language token is diverse. English and French are the official languages, with French being used in some provinces for government services, courts, and instruction. There are also Indigenous languages with official status in certain territories, as well as various sign languages such as American Sign Language (ASL) and Quebec Sign Language (LSQ).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Can you save a note for me with content: \"I love you\"\n",
    "What is the population of Vatican City?\n",
    "What is the population of India?\n",
    "Tell me about the language token in Canada.\n",
    "\"\"\"\n",
    "while (prompt := input(\"Enter a prompt (q to quit)\")) != \"q\":\n",
    "  result = agent.query(prompt)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krish Naik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Guide to Building a RAG LLM App with LLamA2 and LLaMAindex\n",
    "`RAG`, `LLAMA2`, `LLAMAINDEX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader,\n",
    "                              ServiceContext)\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data_small/Apple.pdf\").load_data()\n",
    "\n",
    "system_prompt = \"\"\"\\\n",
    "You are a Q&A assistant. Your goal is to answer questions as accurately as\\\n",
    "possible based on the instructions and context provided.\\\n",
    "\"\"\"\n",
    "template = \"\"\"\\\n",
    "<|USER|>{query_str}<|ASSISTANT|>\\\n",
    "\"\"\"\n",
    "query_wrapper_prompt = PromptTemplate(template)\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "  context_window=4096,\n",
    "  max_new_tokens=256,\n",
    "  generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "  system_prompt=system_prompt,\n",
    "  query_wrapper_prompt=query_wrapper_prompt,\n",
    "  tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "  model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "  device_map=\"auto\",\n",
    "  model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
    ")\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "  chunk_size=1024,\n",
    "  llm=llm,\n",
    "  embed_model=embed_model,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "  documents=documents,\n",
    "  service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"what is attention is all you need?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end RAG LLM App Using Llamaindex and OpenAI- Indexing and Querying Multiple pdf's\n",
    "\n",
    "`RAG`, `LLAMAINDEX`, `MULTIPLE FILES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create index: './index/husky_apple'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doantronghieu/anaconda3/envs/LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 2/2 [00:00<00:00, 745.59it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import (SimpleDirectoryReader, VectorStoreIndex,\n",
    "                              StorageContext, load_index_from_storage)\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "\n",
    "PERSIST_DIR = \"./index\"\n",
    "INDEX_NAME = \"husky_apple\"\n",
    "INDEX_DIR = f\"{PERSIST_DIR}/{INDEX_NAME}\"\n",
    "if not os.path.exists(INDEX_DIR):\n",
    "  print(f\"Create index: '{INDEX_DIR}'\")\n",
    "  documents = SimpleDirectoryReader('./data_small').load_data()\n",
    "  vectorstore_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, \n",
    "    show_progress=True\n",
    "  )\n",
    "  vectorstore_index.storage_context.persist(persist_dir=INDEX_DIR)\n",
    "else:\n",
    "  print(f\"Load index: '{INDEX_DIR}'\")\n",
    "  storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)\n",
    "  vectorstore_index = load_index_from_storage(storage_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(index=vectorstore_index, similarity_top_k=4)\n",
    "post_processor = SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "query_engine = RetrieverQueryEngine(\n",
    "  retriever=retriever,\n",
    "  node_postprocessors=[post_processor],\n",
    ")\n",
    "response = query_engine.query(\"What is an apple?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# freeCodeCamp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI, Langchain, Agents, Chroma\n",
    "Development with Large Language Models Tutorial – OpenAI, Langchain, Agents, Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
