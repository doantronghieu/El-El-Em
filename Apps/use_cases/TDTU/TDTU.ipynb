{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import yaml\n",
				"import add_packages\n",
				"from pprint import pprint\n",
				"import os\n",
				"import pandas as pd\n",
				"from tqdm.auto import tqdm\n",
				"\n",
				"from toolkit.langchain import (\n",
				"\tdocument_loaders, text_splitters, text_embedding_models, stores, \n",
				"\tprompts, utils, output_parsers, agents, documents, models,\n",
				"\trunnables, tools, chains\n",
				")\n",
				"\n",
				"from toolkit import sql, utils\n",
				"\n",
				"PATH_DATA = f\"{add_packages.APP_PATH}/data/tdtu/FEEE\"\n",
				"FILE_CFG = \"tdtu.yaml\"\n",
				"tqdm.pandas(desc=\"Processing\")\n",
				"\n",
				"with open(f\"{add_packages.APP_PATH}/my_configs/{FILE_CFG}\", 'r') as file:\n",
				"\tconfigs = yaml.safe_load(file)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# llm = models.chat_openai\n",
				"# embeddings = text_embedding_models.OpenAIEmbeddings()\n",
				"llm = models.create_llm(provider=\"openai\", version=\"gpt-4o-mini\")\n",
				"embeddings = text_embedding_models.OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
				"\n",
				"vectorstore = stores.faiss.FAISS"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"my_sql_db = sql.MySQLDatabase()\n",
				"# my_sql_db = sql.MySQLDatabase(\n",
				"# \tdbname=os.getenv(\"SQL_DB_NEON\"),\n",
				"# \thost=os.getenv(\"SQL_HOST_NEON\"),\n",
				"# \tport=os.getenv(\"SQL_PORT_NEON\"),\n",
				"# \tuser=os.getenv(\"SQL_USER_NEON\"),\n",
				"# \tpassword=os.getenv(\"SQL_PASSWORD_NEON\"),\n",
				"# )"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Data"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## txt"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### tdtu_feee_faq"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"path_txt = f\"{PATH_DATA}/tdtu_feee_faq.txt\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader_txt = document_loaders.TextLoader(path_txt)\n",
				"doc_txt = loader_txt.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\t# chunk_size=500, chunk_overlap=100,\n",
				"\tseparators=[\"##\"], chunk_size=1000, chunk_overlap=0,\n",
				")\n",
				"docs_txt = text_splitter.split_documents(doc_txt)\n",
				"docs_txt = docs_txt[1:]\n",
				"\n",
				"metadatas = {\n",
				"\t\"data\": \"general information\"\n",
				"}\n",
				"utils.remove_metadata(docs_txt, \"source\")\n",
				"utils.update_metadata(docs_txt, metadatas)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"docs_txt_tdtu_feee_faq = docs_txt"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### File 2"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"path_txt = f\"{PATH_DATA}/faq.txt\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader_txt = document_loaders.TextLoader(path_txt)\n",
				"doc_txt = loader_txt.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\t# chunk_size=500, chunk_overlap=100,\n",
				"\tseparators=[\"##\"], chunk_size=150, chunk_overlap=0,\n",
				")\n",
				"docs_txt = text_splitter.split_documents(doc_txt)\n",
				"docs_txt = docs_txt[1:]\n",
				"\n",
				"metadatas = {\n",
				"\t\"data\": \"frequently asked questions\"\n",
				"}\n",
				"utils.remove_metadata(docs_txt, \"source\")\n",
				"utils.update_metadata(docs_txt, metadatas)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## csv"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### NhanSu"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"file_xlsx = \"NhanSu.xlsx\"\n",
				"path_xlsx = f\"{PATH_DATA}/{file_xlsx}\"\n",
				"path_xlsx_processed = f\"{PATH_DATA}/{file_xlsx.split('.')[0]}1.xlsx\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pd.read_excel(\n",
				"\tpath_xlsx, \n",
				" \t# delimiter=\";\"\n",
				")\n",
				"\n",
				"df.head()\n",
				"\n",
				"# Prompting to get new col names"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Process"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = models.chat_openai\n",
				"\n",
				"template1 = \"\"\"\\\n",
				"...\n",
				"{text}\n",
				"\"\"\"\n",
				"\n",
				"template2 = \"\"\"\\\n",
				"...\n",
				"{text}\n",
				"\"\"\"\n",
				"\n",
				"prompt_template1 = prompts.PromptTemplate.from_template(template1)\n",
				"prompt_template2 = prompts.PromptTemplate.from_template(template2)\n",
				"\n",
				"chain1 = prompt_template1 | model | output_parsers.StrOutputParser()\n",
				"chain2 = prompt_template2 | model | output_parsers.StrOutputParser()\n",
				"\n",
				"# chain = runnables.RunnablePassthrough.assign(\n",
				"#   text=chain1\n",
				"# ).assign(\n",
				"#   text=chain2\n",
				"# )\n",
				"\n",
				"\n",
				"chain = runnables.RunnablePassthrough.assign(\n",
				"  text=chain1\n",
				")\n",
				"\n",
				"def process_xlsx_col(text: str) -> str:\n",
				"  result = chain.invoke({\"text\": text})['text']\n",
				"  return result\n",
				"\n",
				"def capitalize_first_letter(s):\n",
				"\treturn ' '.join([word.capitalize() for word in s.split()])\n",
				"\n",
				"def change_col_value(df: pd.DataFrame, column_name: str, value, new_value):\n",
				"\tdf[column_name] = df[column_name].replace(value, new_value)\n",
				"\treturn df\n",
				"\n",
				"def replace_col_value_if_contains(df, column_name, substring, new_substring):\n",
				"\tdf[column_name] = df[column_name].str.replace(substring, new_substring)\n",
				"\treturn df\n",
				"\n",
				"# query = '...'\n",
				"# result = process_xlsx_col(query)\n",
				"\n",
				"# pprint(result)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"col_to_process = \"Liên hệ\"\n",
				"\n",
				"df[col_to_process] = df[col_to_process].progress_apply(process_xlsx_col)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# df.to_excel(f\"{path_xlsx_processed}\", index=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"path_xlsx = path_xlsx_processed"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Load to sql"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"my_table_schema = [\n",
				"\t\"id SERIAL\",\n",
				"\t\"faculty TEXT\",\n",
				"\t\"name TEXT\",\n",
				"\t\"position TEXT\",\n",
				"\t\"major TEXT\",\n",
				"\t\"email TEXT\",\n",
				"\t\"office TEXT\",\n",
				"\t\"child_department TEXT\",\n",
				"\t\"PRIMARY KEY (id)\",\n",
				"]\n",
				"\n",
				"my_table = sql.MySQLTable(\n",
				"\tname=\"tdtu_feee_personnel\", \n",
				"\tschema=my_table_schema,\n",
				"\tdb=my_sql_db,\n",
				")\n",
				"my_table.create()\n",
				"\n",
				"db = stores.SQLDatabase.from_uri(my_sql_db.get_uri())\n",
				"\n",
				"table_cols = [col_description.split(\" \")[0] for col_description in my_table_schema][1:-1]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# my_table.insert_from_dataframe(df)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# df = pd.read_excel(path_xlsx)\n",
				"# df.columns = table_cols\n",
				"\n",
				"# my_table.insert_from_dataframe(df)\n",
				"\n",
				"# cols = ['name', 'position', 'major', 'office', 'child_department']\n",
				"# proper_nouns = [value for col in cols for value in my_table.get_discrete_values_col(col)]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# all_proper_nouns.extend(proper_nouns)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = ...\n",
				"examples_questions_to_sql = ..."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### ChuongTrinhDaoTao"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"file_xlsx = \"ChuongTrinhDaoTao.xlsx\"\n",
				"path_xlsx = f\"{PATH_DATA}/{file_xlsx}\"\n",
				"path_xlsx_processed = f\"{PATH_DATA}/{file_xlsx.split('.')[0]}1.xlsx\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pd.read_excel(\n",
				"\tpath_xlsx, \n",
				" \t# delimiter=\";\"\n",
				")\n",
				"\n",
				"df.head()\n",
				"\n",
				"# Prompting to get new col names"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Process"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = models.chat_openai\n",
				"\n",
				"template1 = \"\"\"\\\n",
				"...\n",
				"{text}\n",
				"\"\"\"\n",
				"\n",
				"template2 = \"\"\"\\\n",
				"...\n",
				"{text}\n",
				"\"\"\"\n",
				"\n",
				"prompt_template1 = prompts.PromptTemplate.from_template(template1)\n",
				"prompt_template2 = prompts.PromptTemplate.from_template(template2)\n",
				"\n",
				"chain1 = prompt_template1 | model | output_parsers.StrOutputParser()\n",
				"chain2 = prompt_template2 | model | output_parsers.StrOutputParser()\n",
				"\n",
				"# chain = runnables.RunnablePassthrough.assign(\n",
				"#   text=chain1\n",
				"# ).assign(\n",
				"#   text=chain2\n",
				"# )\n",
				"\n",
				"\n",
				"chain = runnables.RunnablePassthrough.assign(\n",
				"  text=chain1\n",
				")\n",
				"\n",
				"def process_xlsx_col(text: str) -> str:\n",
				"  result = chain.invoke({\"text\": text})['text']\n",
				"  return result\n",
				"\n",
				"def capitalize_first_letter(s):\n",
				"\treturn ' '.join([word.capitalize() for word in s.split()])\n",
				"\n",
				"def change_col_value(df: pd.DataFrame, column_name: str, value, new_value):\n",
				"\tdf[column_name] = df[column_name].replace(value, new_value)\n",
				"\treturn df\n",
				"\n",
				"def replace_col_value_if_contains(df, column_name, substring, new_substring):\n",
				"\tdf[column_name] = df[column_name].str.replace(substring, new_substring)\n",
				"\treturn df\n",
				"\n",
				"# query = '...'\n",
				"# result = process_xlsx_col(query)\n",
				"\n",
				"# pprint(result)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"col_to_process = \"Liên hệ\"\n",
				"\n",
				"df[col_to_process] = df[col_to_process].progress_apply(process_xlsx_col)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df.to_excel(f\"{path_xlsx_processed}\", index=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"path_xlsx = path_xlsx_processed"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Load to sql"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"my_table_schema = [\n",
				"\t\"id SERIAL\",\n",
				"\t\"faculty TEXT\",\n",
				"\t\"study_field TEXT\",\n",
				"\t\"link TEXT\",\n",
				"\t\"program_type TEXT\",\n",
				"\t\"education_level TEXT\",\n",
				"\t\"introduction TEXT\",\n",
				"\t\"career_prospects TEXT\",\n",
				"\t\"outcome TEXT\",\n",
				"\t\"syllabub TEXT\",\n",
				"\t\"admission_candidates TEXT\",\n",
				"\t\"registration TEXT\",\n",
				"\t\"tuition TEXT\",\n",
				" \t\"contact TEXT\",\n",
				"\t\"PRIMARY KEY (id)\",\n",
				"]\n",
				"my_table = sql.MySQLTable(\n",
				"\tname=\"tdtu_feee_admission\", \n",
				"\tschema=my_table_schema,\n",
				"\tdb=my_sql_db,\n",
				")\n",
				"my_table.create()\n",
				"\n",
				"table_cols = [col_description.split(\" \")[0] for col_description in my_table_schema][1:-1]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# my_table.insert_from_dataframe(df)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# df = pd.read_excel(path_xlsx)\n",
				"# df.columns = table_cols\n",
				"\n",
				"# my_table.insert_from_dataframe(df)\n",
				"\n",
				"# cols = ['faculty', 'study_field', 'program_type', 'education_level']\n",
				"# proper_nouns = [value for col in cols for value in my_table.get_discrete_values_col(col)]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# all_proper_nouns.extend(proper_nouns)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = ...\n",
				"examples_questions_to_sql = ..."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Vector store "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## txt"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### tdtu_feee_faq"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"qdrant_txt_tdtu_feee_faq = stores.QdrantStore(\n",
				"  embeddings_provider=\"openai\",\n",
				"\tembeddings_model=\"text-embedding-3-large\",\n",
				"\tllm=models.chat_openai,\n",
				"\tsearch_type=\"mmr\",\n",
				"  configs=configs,\n",
				"  distance=\"Cosine\",\n",
				"  retriever_types=\"base\",\n",
				"  **configs[\"vector_db\"][\"qdrant\"][\"tdtu_feee_faq\"],\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# qdrant_txt_tdtu_feee_faq.add_documents(docs_txt_tdtu_feee_faq)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"my_chain_rag_tdtu_feee_faq = chains.MyRagChain(\n",
				"\tllm=llm,\n",
				"\tretriever=qdrant_txt_tdtu_feee_faq.retriever,\n",
				"\tis_debug=False,\n",
				"\tjust_return_ctx=True,\n",
				"\t**configs[\"vector_db\"][\"qdrant\"][\"tdtu_feee_faq\"],\n",
				")\n",
				"\n",
				"tool_chain_rag_tdtu_feee_faq = my_chain_rag_tdtu_feee_faq.create_tool_chain_rag()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"examples_fewshot_tmp = dict(configs[\"sql\"][\"examples_questions_to_sql\"]).values()\n",
				"examples_questions_to_sql = [example for sublist in examples_fewshot_tmp for example in sublist]\n",
				"\n",
				"proper_nouns = configs[\"sql\"][\"proper_nouns\"]\n",
				"\n",
				"my_sql_db = sql.MySQLDatabase()\n",
				"\n",
				"cfg_sql = configs[\"sql\"]\n",
				"cfg_sql_tool = cfg_sql[\"tool\"]\n",
				"\n",
				"my_sql_chain = chains.MySqlChain(\n",
				"\tmy_sql_db=my_sql_db,\n",
				"\tllm=llm,\n",
				"\tembeddings=embeddings,\n",
				"\tvectorstore=vectorstore,\n",
				"\tproper_nouns=proper_nouns,\n",
				"\tk_retriever_proper_nouns=4,\n",
				"\texamples_questions_to_sql=examples_questions_to_sql,\n",
				"\tk_few_shot_examples=5,\n",
				"\tsql_max_out_length=2000,\n",
				"\tis_sql_get_all=True,\n",
				"\tis_debug=False,\n",
				"\ttool_name=cfg_sql_tool[\"name\"],\n",
				"\ttool_description=cfg_sql_tool[\"description\"],\n",
				"\ttool_metadata=cfg_sql_tool[\"metadata\"],\n",
				"\ttool_tags=cfg_sql_tool[\"tags\"],\n",
				")\n",
				"\n",
				"tool_chain_sql = my_sql_chain.create_tool_chain_sql()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"llm = models.create_llm(provider=\"openai\", version=\"gpt-4o-mini\")\n",
				"\n",
				"tools = [\n",
				"\ttool_chain_rag_tdtu_feee_faq,\n",
				"\ttool_chain_sql,\n",
				"]\n",
				"\n",
				"system_message_custom = configs[\"prompts\"][\"system_message_tdtu\"]\n",
				"prompt = prompts.create_prompt_tool_calling_agent(system_message_custom)\n",
				"\n",
				"agent = agents.MyStatelessAgent(\n",
				"\tllm=llm,\n",
				"\ttools=tools,\n",
				"\tprompt=prompt,\n",
				"\tagent_type=\"tool_calling\",\n",
				"\tagent_verbose=False,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"res = []\n",
				"async for chunk in agent.astream_events_basic(\n",
				"\t# \"Người phụ trách bộ môn Điều khiển tự động khoa Điện\",\n",
				"\t# \"Các tiến sĩ trong khoa Điện\", # adjust prompt to return all result\n",
				"\t# \"Các thạc sĩ trong khoa Điện\",\n",
				"\t# \"Ký túc xá\",\n",
				"\t\"Chi tiết Chương trình Đại học về Tự động hoá khoa Điện\",\n",
				"  show_tool_call=True,\n",
				"  history_type=\"mongodb\",\n",
				"  user_id=utils.generate_unique_id(thing=\"uuid_name\"),\n",
				"\tsession_id=utils.generate_unique_id(thing=\"uuid\"),\n",
				"):\n",
				"\tprint(chunk, end=\"\", flush=True)\n",
				"\n",
				"\tres.append(chunk)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = my_sql_chain.chain_sql.invoke({\n",
				"\t\"question\": \n",
				"   \t\"Các chương trình đào tạo khoa Điện có hình thức liên kết với các trường đại học nước ngoài?\",\n",
				"})\n",
				"\n",
				"result"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 115,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"import add_packages\n",
				"import boto3\n",
				"import os\n",
				"from operator import itemgetter\n",
				"from loguru import logger\n",
				"from typing import Union, Optional, List, Literal, AsyncGenerator, TypeAlias\n",
				"from pydantic import BaseModel\n",
				"\n",
				"from toolkit import utils\n",
				"\n",
				"from langchain_core.prompts import ChatPromptTemplate\n",
				"from langchain_core.output_parsers import StrOutputParser\n",
				"from langchain_core.runnables import RunnablePassthrough, Runnable, RunnableParallel\n",
				"\n",
				"from langchain.agents import (\n",
				"\tcreate_openai_tools_agent, create_openai_functions_agent, \n",
				"\tcreate_react_agent, create_self_ask_with_search_agent,\n",
				"\tcreate_xml_agent, create_tool_calling_agent,\n",
				"\tAgentExecutor\n",
				")\n",
				"from langchain_community.agent_toolkits import create_sql_agent\n",
				"\n",
				"from langchain_core.language_models.chat_models import BaseChatModel\n",
				"from langchain_core.prompts.chat import BaseChatPromptTemplate\n",
				"from langchain_core.runnables import Runnable\n",
				"from langchain_core.tools import BaseTool\n",
				"from langchain_core.agents import (\n",
				"\tAgentActionMessageLog, AgentFinish, AgentAction\n",
				")\n",
				"from langchain_core.messages import AIMessage, HumanMessage, ChatMessage\n",
				"\n",
				"from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
				"from langchain.agents.format_scratchpad.openai_tools import (\n",
				"\tformat_to_openai_tool_messages, \n",
				")\n",
				"from langchain.agents.format_scratchpad import (\n",
				"\tformat_to_openai_function_messages,\n",
				")\n",
				"from langchain.agents.output_parsers.openai_tools import (\n",
				"\tOpenAIToolsAgentOutputParser,\n",
				")\n",
				"\n",
				"from langchain_openai import ChatOpenAI\n",
				"from langchain_community.chat_message_histories.dynamodb import DynamoDBChatMessageHistory\n",
				"from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
				"#*==============================================================================\n",
				"\n",
				"dynamodb = boto3.resource(\"dynamodb\")\n",
				"\n",
				"#*==============================================================================\n",
				"\n",
				"TypeAgent: TypeAlias = Literal[\"tool_calling\", \"openai_tools\", \"react\", \"anthropic\"]\n",
				"\n",
				"TypeHistoryType: TypeAlias = Literal[\"in_memory\", \"dynamodb\", \"mongodb\"]\n",
				"TypeUserId: TypeAlias = Optional[str]\n",
				"\n",
				"TypeSessionId: TypeAlias = Union[str, None]\n",
				"\n",
				"#*==============================================================================\n",
				"\n",
				"prompt_tpl_check_ans = \"\"\"\\\n",
				"You are tasked with evaluating whether an AI's answer adequately addressed and satisfied the original question. Follow these steps carefully:\n",
				"\n",
				"1. Review the following:\n",
				"\n",
				"Original question:\n",
				"<original_question>\n",
				"{original_question}\n",
				"</original_question>\n",
				"\n",
				"AI's answer:\n",
				"<ai_answer>\n",
				"{ai_answer}\n",
				"</ai_answer>\n",
				"\n",
				"2. Analyze the AI's answer by considering the following:\n",
				"   - Does the answer directly address the main points of the original question?\n",
				"   - Is the information provided relevant and accurate?\n",
				"   - Does the answer provide sufficient detail to satisfy the query?\n",
				"   - Are there any parts of the original question left unanswered?\n",
				"   - Ensure that all keywords in the answer correspond to the keywords in the question.\n",
				"\n",
				"3. Based on your analysis, determine if the AI's answer adequately addressed and satisfied the original question.\n",
				"\n",
				"4. Provide your response as follows:\n",
				"   - If the answer adequately addressed and satisfied the query, output exactly: True\n",
				"   - If the answer did not adequately address or satisfy the query, or if you cannot determine this due to lack of information or context, output exactly: ERROR\n",
				"\n",
				"Do not provide any explanation or justification for your response. Your output must be either \"True\" or \"ERROR\" without any additional text.\n",
				"\n",
				"Examples of correct outputs:\n",
				"True\n",
				"ERROR\n",
				"\n",
				"Ensure your response is only one of these two options.\n",
				"\"\"\"\n",
				"prompt_check_ans = ChatPromptTemplate.from_template(prompt_tpl_check_ans)\n",
				"\n",
				"def check_ans(original_question: str, ai_answer: str, llm=ChatOpenAI()):\n",
				"\tchain_check_ans = (\n",
				"\t\t\t{\n",
				"\t\t\t\t\"original_question\": itemgetter(\"original_question\"),\n",
				"\t\t\t\t\"ai_answer\": itemgetter(\"ai_answer\")\n",
				"\t\t\t}\n",
				"\t\t\t| prompt_check_ans\n",
				"\t\t\t| llm\n",
				"\t\t\t| StrOutputParser()\n",
				"\t).with_retry()\n",
				"\n",
				"\tresult = chain_check_ans.invoke({\n",
				"\t\t\"original_question\": original_question,\n",
				"\t\t\"ai_answer\": ai_answer,\n",
				"\t})\n",
				"\treturn result\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"\n",
				"prompt_tpl_res_if_not_satis = \"\"\"\\\n",
				"You are tasked with generating a response to a user based on the number of retries for an AI-generated answer. You will be given three inputs: the AI's answer, the current retry count, and the maximum number of retries allowed.\n",
				"\n",
				"Here are the inputs you will work with:\n",
				"\n",
				"<ai_answer>\n",
				"{ai_answer}\n",
				"</ai_answer>\n",
				"\n",
				"<current_retry>{current_retry}</current_retry>\n",
				"\n",
				"<max_retry>{max_retry}</max_retry>\n",
				"\n",
				"Follow these steps to generate the appropriate response:\n",
				"\n",
				"1. Compare the value of current_retry to max_retry.\n",
				"\n",
				"2. If current_retry is less or equals to than max_retry:\n",
				"   - Inform the user that you will continue to retry to get the correct answer.\n",
				"   \n",
				"3. If current_retry is greater than max_retry:\n",
				"   - Tell the user to please try again.\n",
				"\n",
				"4. Ensure that your response is in the exact same language as the text in ai_answer. This means you should analyze the language used in ai_answer and formulate your response in that same language.\n",
				"\n",
				"Remember, do not include any explanations or additional information. Your output should only be the appropriate message to the user, written in the same language as ai_answer.\n",
				"\n",
				"Your response: \\\n",
				"\"\"\"\n",
				"prompt_res_if_not_satis = ChatPromptTemplate.from_template(prompt_tpl_res_if_not_satis)\n",
				"\n",
				"\n",
				"def response_if_not_satisfied(\n",
				"  ai_answer: str, current_retry: int, max_retry: int,\n",
				"  llm=ChatOpenAI()\n",
				"):\n",
				"\tchain_res_if_not_satis = (\n",
				"\t\t\t{\n",
				"\t\t\t\t\"ai_answer\": itemgetter(\"ai_answer\"),\n",
				"\t\t\t\t\"current_retry\": itemgetter(\"current_retry\"),\n",
				"\t\t\t\t\"max_retry\": itemgetter(\"max_retry\")\n",
				"\t\t\t}\n",
				"\t\t\t| prompt_res_if_not_satis\n",
				"\t\t\t| llm\n",
				"\t\t\t| StrOutputParser()\n",
				"\t).with_retry()\n",
				" \n",
				"\tresult = chain_res_if_not_satis.invoke({\n",
				"\t\t\"ai_answer\": ai_answer,\n",
				"\t\t\"current_retry\": current_retry,\n",
				"\t\t\"max_retry\": max_retry,\n",
				"\t})\n",
				"\treturn result\n",
				"\n",
				"#*==============================================================================\n",
				"\n",
				"class SchemaChatHistory(BaseModel):\n",
				"\thistory_type: TypeHistoryType = \"in_memory\"\n",
				"\tuser_id: TypeUserId = \"admin\"\n",
				"\tsession_id: TypeSessionId = None\n",
				"\thistory_size: Union[int, None] = 10\n",
				"\n",
				"class ChatHistory:\n",
				"\tdef __init__(self, schema: SchemaChatHistory):\n",
				"\t\tself.history_type = schema.history_type\n",
				"\n",
				"\t\tself.user_id = schema.user_id\n",
				"\t\tself.is_new_session = not bool(schema.session_id)\n",
				"\t\tself.session_id = schema.session_id if schema.session_id else utils.generate_unique_id(\"uuid_name\")\n",
				"\n",
				"\t\tself.history_size = schema.history_size\n",
				"\t\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tself.chat_history = []\n",
				"\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\tself.chat_history = DynamoDBChatMessageHistory(\n",
				"\t\t\t\ttable_name=\"LangChainSessionTable\", \n",
				"\t\t\t\tsession_id=self.session_id,\n",
				"\t\t\t\tkey={\n",
				"\t\t\t\t\t\"SessionId\": self.session_id,\n",
				"\t\t\t\t\t\"UserId\": self.user_id,\n",
				"\t\t\t\t},\n",
				"\t\t\t\thistory_size=self.history_size,\n",
				"\t\t\t)\n",
				"\t\telif self.history_type == \"mongodb\":\n",
				"\t\t\tself.chat_history = MongoDBChatMessageHistory(\n",
				"\t\t\t\tsession_id=self.session_id, # user name, email, chat id etc.\n",
				"\t\t\t\tconnection_string=os.getenv(\"MONGODB_ATLAS_CLUSTER_URI\"),\n",
				"\t\t\t\tdatabase_name=os.getenv(\"MONGODB_DB_NAME\"),\n",
				"\t\t\t\tcollection_name=os.getenv(\"MONGODB_COLLECTION_NAME_MSG\"),\n",
				"\t\t\t)\t\t\t\n",
				"\n",
				"\t\tif self.is_new_session:\n",
				"\t\t\twelcome_msg = \"Hello! How can I help you today?\"\n",
				"\t\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\t\tself.chat_history.append(AIMessage(welcome_msg))\n",
				"\t\t\telif self.history_type == \"dynamodb\" or self.history_type == \"mongodb\":\n",
				"\t\t\t\tself.chat_history.add_ai_message(welcome_msg)\n",
				"\n",
				"\t\tif self.user_id: logger.info(f\"User Id: {self.user_id}\")\n",
				"\t\tlogger.info(f\"Session Id: {self.session_id}\")\n",
				"\t\tlogger.info(f\"History Type: {self.history_type}\")\n",
				"\t\n",
				"\tasync def _add_messages_to_history(\n",
				"\t\tself,\n",
				"\t\tmsg_user: str,\n",
				"\t\tmsg_ai: str,\n",
				"\t):\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tif msg_user:\n",
				"\t\t\t\tself.chat_history.append(HumanMessage(msg_user))\n",
				"\t\t\tif msg_ai:\n",
				"\t\t\t\tself.chat_history.append(AIMessage(msg_ai))\n",
				"\t\telif self.history_type == \"dynamodb\" or self.history_type == \"mongodb\":\n",
				"\t\t\tif msg_user:\n",
				"\t\t\t\tawait self.chat_history.aadd_messages(messages=[HumanMessage(msg_user)])\n",
				"\t\t\tif msg_ai:\n",
				"\t\t\t\tawait self.chat_history.aadd_messages(messages=[AIMessage(msg_ai)])\n",
				"\n",
				"\tasync def _get_chat_history(self, is_truncate=True):\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tresult = self.chat_history\n",
				"\t\t\tif is_truncate: result = result[-self.history_size:]\n",
				"\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\tresult = self.chat_history.messages\n",
				"\t\telif self.history_type == \"mongodb\":\n",
				"\t\t\tresult = await self.chat_history.aget_messages()\n",
				"\t\t\tif is_truncate: result = result[-self.history_size:]\n",
				"\n",
				"\t\treturn result\n",
				"\n",
				"\tasync def clear_chat_history(self):\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tself.chat_history = []\n",
				"\t\telif self.history_type == \"dynamodb\" or self.history_type == \"dynamodb\":\n",
				"\t\t\tawait self.chat_history.aclear()\n",
				"\n",
				"class MyStatelessAgent:\n",
				"\tdef __init__(\n",
				"\t\tself,\n",
				"\t\tllm: Union[BaseChatModel, None],\n",
				"\t\ttools: list[BaseTool],\n",
				"\t\tprompt: Union[BaseChatPromptTemplate, None],\n",
				"\t\n",
				"\t\tagent_type: Literal[\n",
				"\t\t\t\"tool_calling\", \"openai_tools\", \"react\", \"anthropic\"\n",
				"\t\t] = \"tool_calling\",\n",
				"\t\tagent_verbose: bool = False,\n",
				"\t):\n",
				"\t\tself.llm = llm\n",
				"\t\tself.my_tools = tools\n",
				"\t\tself.prompt = prompt\n",
				"\n",
				"\t\tself.agent_type = agent_type\n",
				"\t\tself.agent_verbose = agent_verbose\n",
				"\t\n",
				"\t\tself.agent = self._create_agent()\n",
				"\t\tself.agent_executor = AgentExecutor(\n",
				"\t\t\tagent=self.agent, tools=self.my_tools, verbose=self.agent_verbose,\n",
				"\t\t\thandle_parsing_errors=True,\n",
				"\t\t\treturn_intermediate_steps=False,\n",
				"\t\t)\n",
				"\n",
				"\tdef _create_agent(self) -> Runnable:\n",
				"\t\tlogger.info(f\"Agent type: {self.agent_type}\")\n",
				"\t\n",
				"\t\tif self.agent_type == \"tool_calling\":\n",
				"\t\t\treturn create_tool_calling_agent(self.llm, self.my_tools, self.prompt)\n",
				"\t\telif self.agent_type == \"openai_tools\":\n",
				"\t\t\treturn create_openai_tools_agent(self.llm, self.my_tools, self.prompt)\n",
				"\t\telif self.agent_type == \"react\":\n",
				"\t\t\treturn create_react_agent(llm=self.llm, tools=self.my_tools, prompt=self.prompt)\n",
				"\t\telif self.agent_type == \"anthropic\": # todo\n",
				"\t\t\treturn create_xml_agent(llm=self.llm, tools=self.my_tools, prompt=self.prompt)\n",
				"\t\telse:\n",
				"\t\t\traise ValueError(\n",
				"\t\t\t\t\t\"Invalid agent type. Supported types are 'openai_tools' and 'react'.\")\n",
				"\n",
				"\tdef _create_chat_history(\n",
				"\t\tself,\n",
				"\t\thistory_type: TypeHistoryType = \"mongodb\",\n",
				"\t\tuser_id: TypeUserId = \"admin\",\n",
				"\t\tsession_id: TypeSessionId = None,\n",
				"\t\thistory_size: Union[int, None] = 20,\n",
				"\t) -> ChatHistory:\n",
				"\t\n",
				"\t\treturn ChatHistory(schema=SchemaChatHistory(\n",
				"\t\t\thistory_type=history_type, user_id=user_id, session_id=session_id,\n",
				"\t\t\thistory_size=history_size,\n",
				"\t\t)) \n",
				"\t\n",
				"\tasync def _add_messages_to_history(\n",
				"\t\tself,\n",
				"\t\thistory: ChatHistory,\n",
				"\t\thistory_type: TypeHistoryType,\n",
				"\t\tmsg_user: str,\n",
				"\t\tmsg_ai: str,\n",
				"\t):\n",
				"\t\tawait history._add_messages_to_history(msg_user, msg_ai)\n",
				"\t\n",
				"\tasync def invoke_agent(\n",
				"\t\tself,\n",
				"\t\tinput_message: str,\n",
				"\t\tcallbacks: Optional[List] = None,\n",
				"\t\tmode: Literal[\"sync\", \"async\"] = \"async\",\n",
				"\t\n",
				"\t\thistory_type: TypeHistoryType = \"mongodb\",\n",
				"\t\tuser_id: TypeUserId = \"admin\",\n",
				"\t\tsession_id: TypeSessionId = None,\n",
				"\t\n",
				"\t\thistory_size: Union[int, None] = 20,\n",
				"\t):\n",
				"\t\tresult = None\n",
				"\n",
				"\t\thistory = self._create_chat_history(\n",
				"\t\t\thistory_type, user_id, session_id, history_size,\n",
				"\t\t)\n",
				"\n",
				"\t\tinput_data = {\n",
				"\t\t\t\"input\": input_message, \"chat_history\": await history._get_chat_history()\n",
				"\t\t}\n",
				"\n",
				"\t\tconfigs = {}\n",
				"\t\tconfigs[\"callbacks\"] = callbacks if callbacks else []\n",
				"\n",
				"\t\tif mode == \"sync\":\n",
				"\t\t\tresult = self.agent_executor.invoke(input_data, configs)\n",
				"\t\telif mode == \"async\":\n",
				"\t\t\tresult = await self.agent_executor.ainvoke(input_data, configs)\n",
				"\n",
				"\t\tresult = result[\"output\"]\n",
				"\n",
				"\t\tawait self._add_messages_to_history(\n",
				"\t\t\thistory=history,\n",
				"\t\t\thistory_type=history_type,\n",
				"\t\t\tmsg_user=input_message,\n",
				"\t\t\tmsg_ai=result,\n",
				"\t\t)\n",
				"\t\n",
				"\t\treturn result\n",
				"\n",
				"\tasync def astream_events_basic_wrapper(\n",
				"\t\tself,\n",
				"\t\tinput_message: str,\n",
				"\t):\n",
				"\t\tresult = \"\"\n",
				"\t\tasync for chunk in self.astream_events_basic(input_message):\n",
				"\t\t\tresult += chunk\n",
				"\t\t\tprint(chunk, end=\"\", flush=True)\n",
				"\t\treturn result\n",
				"\n",
				"\tasync def astream_events_basic(\n",
				"\t\tself,\n",
				"\t\tinput_message: str,\n",
				"\n",
				"\t\thistory_type: TypeHistoryType = \"mongodb\",\n",
				"\t\tuser_id: TypeUserId = utils.generate_unique_id(thing=\"uuid_name\"),\n",
				"\t\tsession_id: TypeSessionId = utils.generate_unique_id(thing=\"uuid\"),\n",
				"\t\n",
				"\t\tshow_tool_call: bool = False,\n",
				"\t\thistory_size: Union[int, None] = 10,\n",
				"\t) -> AsyncGenerator[str, None]:\n",
				"\t\t\"\"\"\n",
				"\t\tasync for chunk in agent.astream_events_basic(\"Hello\"):\n",
				"\t\t\tprint(chunk, end=\"\", flush=True)\n",
				"\t\t\"\"\"\n",
				"\n",
				"\t\thistory = self._create_chat_history(\n",
				"\t\t\thistory_type, user_id, session_id, history_size,\n",
				"\t\t)\n",
				"\n",
				"\t\tresult = \"\"\n",
				"\t\tis_result_satisfied = False\n",
				"\t\tmax_retry = 2\n",
				"\t\tcurrent_retry = 0\n",
				"\n",
				"\t\t\"\"\" used for debugging\n",
				"\t\ta = agent.events\n",
				"\t\ta = [x for x in agent.events if x[\"event\"] == \"on_chat_model_stream\"]\n",
				"\t\ta_data = [x[\"data\"] for x in a]\n",
				"\t\ta_data_chunk = [x[\"chunk\"] for x in a_data]\n",
				"\t\ta_data_chunk_tool = [x for x in a if dict(a_data_chunk)[\"tool_call_chunks\"]]\n",
				"\t\ta_metadata_sql_chain = [x for x in a if \"...\" in x[\"metadata\"].keys()]\n",
				"\t\t\"\"\"\n",
				"  \n",
				"\t\t# self.events = [] # debug\n",
				"\n",
				"\t\twhile (not is_result_satisfied) and (current_retry <= max_retry):\n",
				"\t\t\tasync for event in self.agent_executor.astream_events(\n",
				"\t\t\t\tinput={\"input\": input_message, \"chat_history\": await history._get_chat_history()},\n",
				"\t\t\t\tversion=\"v2\",\n",
				"\t\t\t):\n",
				"\t\t\t\t# self.events.append(event) # debug\n",
				"\t\t\t\tevent_event = event[\"event\"]\n",
				"\t\t\t\tevent_name = event[\"name\"]\n",
				"\t\t\n",
				"\t\t\t\ttry: event_data_chunk = event[\"data\"][\"chunk\"]\n",
				"\t\t\t\texcept: pass\n",
				"\t\t\t\t\n",
				"\t\t\t\tif event_event == \"on_chat_model_stream\":\n",
				"\t\t\t\t\tchunk = dict(event_data_chunk)[\"content\"]\n",
				"\t\t\t\n",
				"\t\t\t\t\tif (event.get(\"metadata\", {}).get(\"ls_stop\") == ['\\nSQLResult:']) \\\n",
				"\t\t\t\t\t\t\tor (\"is_my_sql_chain_run\" in event.get(\"metadata\", {})) \\\n",
				"\t\t\t\t\t\t\tor (\"is_my_rag_chain_run\" in event.get(\"metadata\", {})):\n",
				"\t\t\t\t\t\tcontinue\n",
				"\n",
				"\t\t\t\t\tresult += chunk\n",
				"\t\t\t\t\tyield chunk\n",
				"\t\t\t\n",
				"\t\t\t\tif show_tool_call and event_event == \"on_chain_stream\":\n",
				"\t\t\t\t\tif event_name == \"RunnableSequence\":\n",
				"\t\t\t\t\t\ttry:\n",
				"\t\t\t\t\t\t\tchunk: str = dict(event_data_chunk[0])[\"log\"]\n",
				"\t\t\t\t\t\t\tchunk = f\"`[TOOL - CALLING]` {chunk}\"\n",
				"\t\t\t\t\n",
				"\t\t\t\t\t\t\tawait self._add_messages_to_history(\n",
				"\t\t\t\t\t\t\t\thistory=history,\n",
				"\t\t\t\t\t\t\t\thistory_type=history_type,\n",
				"\t\t\t\t\t\t\t\tmsg_user=None,\n",
				"\t\t\t\t\t\t\t\tmsg_ai=chunk,\n",
				"\t\t\t\t\t\t\t)\n",
				"\t\t\t\t\t\t\tresult += chunk\n",
				"\t\t\t\t\t\t\tyield chunk\n",
				"\t\t\t\t\t\texcept:\n",
				"\t\t\t\t\t\t\tpass\n",
				"\t\t\t\t\n",
				"\t\t\t\t\telif event_name == \"RunnableLambda\":\n",
				"\t\t\t\t\t\ttry:\n",
				"\t\t\t\t\t\t\tchunk = dict(event_data_chunk[1])[\"content\"]\n",
				"\t\t\t\t\t\t\tchunk = f\"`[TOOL - RESULT]` {chunk}\\n\\n\"\n",
				"\t\t\t\t\n",
				"\t\t\t\t\t\t\tawait self._add_messages_to_history(\n",
				"\t\t\t\t\t\t\t\thistory=history,\n",
				"\t\t\t\t\t\t\t\thistory_type=history_type,\n",
				"\t\t\t\t\t\t\t\tmsg_user=None,\n",
				"\t\t\t\t\t\t\t\tmsg_ai=chunk,\n",
				"\t\t\t\t\t\t\t)\n",
				"\t\t\t\t\n",
				"\t\t\t\t\t\t\tresult += chunk\n",
				"\t\t\t\t\t\t\tyield chunk\n",
				"\t\t\t\t\t\texcept:\n",
				"\t\t\t\t\t\t\tpass\n",
				"\t\t\t\n",
				"\t\t\tcurrent_retry += 1\n",
				"\n",
				"\t\t\tif check_ans(\n",
				"     \t\toriginal_question=input_message, ai_answer=result, llm=self.llm,\n",
				"      ) != \"ERROR\":\n",
				"\t\t\t\tis_result_satisfied = True\n",
				"\t\t\t\tyield(\"\\n\")\n",
				"\t\t\telse:\n",
				"\t\t\n",
				"\t\t\t\tif current_retry <= max_retry:\n",
				"\t\t\t\t\tfor _ in range(1):\n",
				"\t\t\t\t\t\tyield(\"\\n\")\n",
				"\t\t\t\n",
				"\t\t\t\t\tfor res in response_if_not_satisfied(\n",
				"\t\t\t\t\t\tai_answer=result,\n",
				"\t\t\t\t\t\tcurrent_retry=current_retry,\n",
				"\t\t\t\t\t\tmax_retry=max_retry,\n",
				"\t\t\t\t\t\tllm=self.llm,\n",
				"\t\t\t\t\t):\n",
				"\t\t\t\t\t\tyield res\n",
				"\t\t\t\t\t\n",
				"\t\t\t\t\thistory = self._create_chat_history(\n",
				"\t\t\t\t\t\thistory_type=history_type,\n",
				"\t\t\t\t\t\tuser_id=utils.generate_unique_id(thing=\"uuid_name\"), \n",
				"\t\t\t\t\t\tsession_id=utils.generate_unique_id(thing=\"uuid\"), \n",
				"\t\t\t\t\t\thistory_size=history_size,\n",
				"\t\t\t\t\t)\n",
				"\n",
				"\t\t\t\tresult = \"\"\n",
				"    \n",
				"\t\tawait self._add_messages_to_history(\n",
				"\t\t\thistory=history,\n",
				"\t\t\thistory_type=history_type,\n",
				"\t\t\tmsg_user=input_message,\n",
				"\t\t\tmsg_ai=result,\n",
				"\t\t)\n",
				"\t\n",
				"def hello():\n",
				"\t..."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 116,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-08-09 15:55:49.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_create_agent\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mAgent type: tool_calling\u001b[0m\n"
					]
				}
			],
			"source": [
				"llm = models.create_llm(provider=\"openai\", version=\"gpt-3.5-turbo-0125\")\n",
				"\n",
				"tools = [\n",
				"\ttool_chain_rag_tdtu_feee_faq,\n",
				"\ttool_chain_sql,\n",
				"]\n",
				"\n",
				"system_message_custom = configs[\"prompts\"][\"system_message_tdtu\"]\n",
				"prompt = prompts.create_prompt_tool_calling_agent(system_message_custom)\n",
				"\n",
				"agent = MyStatelessAgent(\n",
				"\tllm=llm,\n",
				"\ttools=tools,\n",
				"\tprompt=prompt,\n",
				"\tagent_type=\"tool_calling\",\n",
				"\tagent_verbose=False,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 118,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-08-09 15:56:03.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mUser Id: Michael Friedman-03a9d67d-3cfc-4020-a78b-89099f980ac5\u001b[0m\n",
						"\u001b[32m2024-08-09 15:56:03.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m218\u001b[0m - \u001b[1mSession Id: d09d5451-00f2-4aee-9773-75731975b6c7\u001b[0m\n",
						"\u001b[32m2024-08-09 15:56:03.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mHistory Type: mongodb\u001b[0m\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Xin lỗi, hiện tại tôi không có thông tin về giảng viên của Khoa Quản trị kinh doanh tại Trường Đại học Tôn Đức Thắng. Bạn có thể thử lại sau hoặc liên hệ với bộ phận hỗ trợ của trường để biết thêm thông tin chi tiết.Bạn cần thông tin gì khác không ạ?\n",
						"Xin lỗi, tôi sẽ tiếp tục thử để có được câu trả lời chính xác."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-08-09 15:56:17.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mUser Id: Grace Andrews-b86fe2ea-6180-4749-924b-7927135457bb\u001b[0m\n",
						"\u001b[32m2024-08-09 15:56:17.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m218\u001b[0m - \u001b[1mSession Id: cb72e054-c608-4355-ab0a-4e9ef306dfda\u001b[0m\n",
						"\u001b[32m2024-08-09 15:56:17.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mHistory Type: mongodb\u001b[0m\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Xin lỗi, hiện tại tôi không có thông tin về giảng viên của Khoa Quản trị kinh doanh tại Trường Đại học Tôn Đức Thắng. Bạn có thể thử lại sau hoặc liên hệ với bộ phận hỗ trợ của trường để biết thêm thông tin chi tiết.Bạn cần thông tin gì khác không ạ?\n",
						"Xin lỗi, tôi sẽ tiếp tục thử để có được câu trả lời chính xác."
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-08-09 15:56:30.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mUser Id: Sharon Torres-df0abb76-b218-4588-b21b-7b2e40162b90\u001b[0m\n",
						"\u001b[32m2024-08-09 15:56:30.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m218\u001b[0m - \u001b[1mSession Id: e5fe1154-d945-49b9-86e7-0197d0d0961c\u001b[0m\n",
						"\u001b[32m2024-08-09 15:56:30.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mHistory Type: mongodb\u001b[0m\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Xin lỗi, hiện tại tôi không có thông tin về giảng viên của Khoa Quản trị kinh doanh tại Trường Đại học Tôn Đức Thắng. Bạn có thể thử lại hoặc liên hệ với bộ phận hỗ trợ của trường để biết thêm thông tin chi tiết.Bạn cần thông tin gì khác không ạ?"
					]
				}
			],
			"source": [
				"res = []\n",
				"async for chunk in agent.astream_events_basic(\n",
				" \n",
				"\t\"Giảng viên khoa quản trị kinh doanh\",\n",
				"\n",
				"  show_tool_call=False,\n",
				"  history_type=\"mongodb\",\n",
				"  user_id=utils.generate_unique_id(thing=\"uuid_name\"),\n",
				"\tsession_id=utils.generate_unique_id(thing=\"uuid\"),\n",
				"):\n",
				"\tprint(chunk, end=\"\", flush=True)\n",
				"\n",
				"\tres.append(chunk)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"original_question = \"Ai là người phụ trách khoa Điện - Điện tử?\"\n",
				"ai_answer = \"Người phụ trách Khoa Điện - Điện tử tại Trường Đại học Tôn Đức Thắng là TS. Trần Thanh Phương, hiện đang giữ chức vụ Phó Trưởng Khoa - Phụ trách Khoa. Nếu bạn cần thêm thông tin về khoa hoặc các chương trình học, hãy cho tôi biết nhé!\"\n",
				"ai_answer = \"Hihi you are my baby\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for res in response_if_not_satisfied(\n",
				"\tai_answer=\"Xin lỗi, hiện tại tôi không có thông tin về giảng viên của Khoa Quản trị kinh doanh tại Trường Đại học Tôn Đức Thắng. Bạn có thể thử lại sau hoặc liên hệ với bộ phận hỗ trợ của trường để biết thêm thông tin chi tiết.Bạn cần thông tin gì khác không ạ?\",\n",
				"\tcurrent_retry=1,\n",
				"\tmax_retry=2,\n",
				"):\n",
				"  print(res, end=\"\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.9"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
