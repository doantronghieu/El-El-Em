{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
						"  from .autonotebook import tqdm as notebook_tqdm\n"
					]
				}
			],
			"source": [
				"import yaml\n",
				"import add_packages\n",
				"from pprint import pprint\n",
				"import os, re\n",
				"import pandas as pd\n",
				"# import tqdm\n",
				"from tqdm.auto import tqdm\n",
				"\n",
				"from toolkit.langchain import (\n",
				"\tdocument_loaders, text_splitters, text_embedding_models, vectorstores, \n",
				"\tchat_models, prompts, utils, output_parsers, agents, documents, llms,\n",
				"\trunnables, agent_tools\n",
				")\n",
				"\n",
				"tqdm.pandas(desc=\"Processing\")\n",
				"\n",
				"with open(f\"{add_packages.APP_PATH}/my_configs/tdtu.yaml\", 'r') as file:\n",
				"    configs_vtc = yaml.safe_load(file)\n",
				"\n",
				"PATH_TDTU = f\"{add_packages.APP_PATH}/data/tdtu\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Data"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Lectures Content"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Combine"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def extract_course_name(file_path):\n",
				"\twith open(file_path, 'r', encoding='utf-8') as file:\n",
				"\t\tfirst_line = file.readline().strip()\n",
				"\t\treturn first_line[len(\"# Course: \"):]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def remove_extra_newlines(text):\n",
				"\twhile '\\n\\n' in text:\n",
				"\t\ttext = text.replace('\\n\\n', '\\n')\n",
				"\treturn text\n",
				"\n",
				"\n",
				"def remove_double_spaces(text):\n",
				"\twhile '  ' in text:\n",
				"\t\ttext = text.replace('  ', ' ')\n",
				"\treturn text\n",
				"\n",
				"\n",
				"def process_add_space_after_hash(text):\n",
				"\tpattern = r'##(\\w)'\n",
				"\tprocessed_text = re.sub(pattern, r'## \\1', text)\n",
				"\treturn processed_text\n",
				"\n",
				"\n",
				"def process_remove_quotes(text):\n",
				"\tcleaned_text = text.replace('\"', '')\n",
				"\treturn cleaned_text\n",
				"\n",
				"\n",
				"def process_text_file(input_file, output_file, functions):\n",
				"\t# Read the first line of the input file\n",
				"\twith open(input_file, 'r') as file:\n",
				"\t\tfirst_line = file.readline().strip()\n",
				"\t\tlen_first_line = len(first_line)\n",
				"    \n",
				"\n",
				"\t# Extract course name from filename\n",
				"\tcourse_name = os.path.splitext(os.path.basename(input_file))[0]\n",
				"\t# If the first line doesn't match the filename, replace it with the filename's course name\n",
				"\tfirst_line = \"# Course: \" + course_name\n",
				"\n",
				"\t# Reopen the file to process the entire content\n",
				"\twith open(input_file, 'r') as file:\n",
				"\t\ttext = file.read()\n",
				"\n",
				"\t# Apply processing functions\n",
				"\tfor func in functions:\n",
				"\t\ttext = func(text)\n",
				"\n",
				"\t# Prepend the modified first line to the processed text\n",
				"\ttext = first_line + '\\n' + text[len_first_line + 1:]\n",
				"\n",
				"\t# Write the processed text to the output file\n",
				"\twith open(output_file, 'w') as file:\n",
				"\t\tfile.write(text)\n",
				"\n",
				"\n",
				"# Define the list of functions to apply\n",
				"functions_to_apply = [\n",
				"\tprocess_remove_quotes,\n",
				"\tremove_extra_newlines,\n",
				"\tremove_double_spaces,\n",
				"\tprocess_add_space_after_hash,\n",
				"]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# # Example usage:\n",
				"# input_file_path = '../data/vtc1/script/KỸ NĂNG TÌM VIỆC LÀM THÊM.txt'\n",
				"# output_file_path = '../data/vtc1/script/KỸ NĂNG TÌM VIỆC LÀM THÊM.txt'\n",
				"# process_text_file(input_file_path, output_file_path, functions_to_apply)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Process files"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# # Specify the folder containing the files\n",
				"# folder_path = f'{PATH_TDTU}/script/'\n",
				"\n",
				"# # Loop through each file in the folder\n",
				"# for filename in os.listdir(folder_path):\n",
				"#     if filename.endswith('.txt'):  # Process only text files\n",
				"#         input_file_path = os.path.join(folder_path, filename)\n",
				"#         output_file_path = os.path.join(\n",
				"#             folder_path, filename)  # Output file path\n",
				"#         process_text_file(input_file_path, output_file_path,\n",
				"#                           functions_to_apply)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Create documents "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### ONE course\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# # Example usage:\n",
				"# file_path = \"../data/vtc1/script/HỘI HỌA - MÀU SẮC VÀ PHỐI MÀU CƠ BẢN.txt\"\n",
				"# course_name = extract_course_name(file_path).lower()\n",
				"\n",
				"# text_loader_lectures_content = document_loaders.TextLoader(file_path)\n",
				"# document = text_loader_lectures_content.load()\n",
				"\n",
				"# text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"#   chunk_size=1000, chunk_overlap=200,\n",
				"# )\n",
				"# docs_lectures_content = text_splitter.split_documents(document)\n",
				"\n",
				"# metadatas = {\n",
				"#   \"data\": \"lectures content\",\n",
				"#   \"course_name\": course_name,\n",
				"# }\n",
				"# utils.remove_metadata(docs_lectures_content, \"source\")\n",
				"# utils.update_metadata(docs_lectures_content, metadatas)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### MULTIPLE course\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# def process_folder(folder_path):\n",
				"#     data_dict = {}\n",
				"#     for root, dirs, files in os.walk(folder_path):\n",
				"#         for file_name in files:\n",
				"#             if file_name.endswith('.txt'):\n",
				"#                 file_path = os.path.join(root, file_name)\n",
				"#                 course_name = extract_course_name(file_path).lower()\n",
				"\n",
				"#                 text_loader_lectures_content = document_loaders.TextLoader(file_path)\n",
				"#                 document = text_loader_lectures_content.load()\n",
				"\n",
				"#                 text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"#                     chunk_size=1000, chunk_overlap=200,\n",
				"#                 )\n",
				"#                 docs_lectures_content = text_splitter.split_documents(document)\n",
				"\n",
				"#                 metadatas = {\n",
				"#                     \"data\": \"lectures content\",\n",
				"#                     \"course_name\": course_name,\n",
				"#                 }\n",
				"#                 utils.remove_metadata(docs_lectures_content, \"source\")\n",
				"#                 utils.update_metadata(docs_lectures_content, metadatas)\n",
				"\n",
				"#                 data_dict[course_name] = docs_lectures_content\n",
				"\n",
				"#     return data_dict\n",
				"\n",
				"\n",
				"# folder_path = f\"{PATH_TDTU}/script/\"\n",
				"# docs_lectures_content_dict = process_folder(folder_path)\n",
				"# for file_name, content in docs_lectures_content_dict.items():\n",
				"#     print(f\"File: {file_name}, Course: {content[0].metadata['course_name']}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## FAQ"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# text_loader_faq = document_loaders.TextLoader(\n",
				"#   f\"{PATH_TDTU}/faq.txt\"\n",
				"# )\n",
				"# document = text_loader_faq.load()\n",
				"\n",
				"# text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"#   # chunk_size=500, chunk_overlap=100,\n",
				"#   separators=[\"##\"], chunk_size=150, chunk_overlap=0,\n",
				"# )\n",
				"# docs_faq = text_splitter.split_documents(document)\n",
				"# docs_faq = docs_faq[1:]\n",
				"# metadatas = {\n",
				"#   \"data\": \"frequently asked questions\"\n",
				"# }\n",
				"# utils.remove_metadata(docs_faq, \"source\")\n",
				"# utils.update_metadata(docs_faq, metadatas)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## csv - education_program"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [
				{
					"ename": "ParserError",
					"evalue": "Error tokenizing data. C error: Expected 6 fields in line 6, saw 12\n",
					"output_type": "error",
					"traceback": [
						"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
						"\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
						"Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m path_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATH_TDTU\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/FEEE/ChuongTrinhDaoTao.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_csv\u001b[49m\u001b[43m)\u001b[49m\n",
						"File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
						"File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
						"File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
						"File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
						"File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
						"File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
						"File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
						"File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
						"File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
						"\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 6, saw 12\n"
					]
				}
			],
			"source": [
				"path_csv = f\"{PATH_TDTU}/FEEE/ChuongTrinhDaoTao.csv\"\n",
				"\n",
				"df = pd.read_csv(path_csv)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"path_csv = f\"{PATH_TDTU}/ChuongTrinhDaoTao.csv\"\n",
				"\n",
				"df = pd.read_csv(path_csv)\n",
				"\n",
				"df['MÔ TẢ (course description)'] = df['MÔ TẢ (course description)'].progress_apply(\n",
				"  process_course_description)\n",
				"\n",
				"df.to_csv(f\"{PATH_TDTU}/courses_list1.csv\", index=False)\n",
				"\n",
				"list(df[\"MÔ TẢ (course description)\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# path_csv = f\"{PATH_TDTU}/courses_list1.csv\"\n",
				"# courses_list_cols = utils.get_csv_column_names(path_csv)\n",
				"\n",
				"# csv_loader_courses_list = document_loaders.CSVLoader(\n",
				"#     path_csv,\n",
				"#     # source_column=\"No\",\n",
				"#     csv_args={\n",
				"#         \"delimiter\": \",\",\n",
				"#         # \"quotechar\": \"''\",\n",
				"#         \"fieldnames\": courses_list_cols,\n",
				"#     },\n",
				"# )\n",
				"# document = csv_loader_courses_list.load()[1:]\n",
				"# docs_courses_information = document\n",
				"\n",
				"# metadatas = {\n",
				"#     \"data\": \"courses information\"\n",
				"# }\n",
				"\n",
				"# utils.remove_metadata(docs_courses_information, \"source\")\n",
				"# utils.remove_metadata(docs_courses_information, \"row\")\n",
				"# utils.update_metadata(docs_courses_information, metadatas)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Vector store "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Lectures Content"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "2"
				}
			},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-05-14 10:09:03.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mFound collection: `vtc-lectures-content-3072-v1`.\u001b[0m\n",
						"\u001b[32m2024-05-14 10:09:03.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1m`vtc-lectures-content-3072-v1` - Embeddings: openai - {'model': 'text-embedding-3-large'}, 3072\u001b[0m\n",
						"\u001b[32m2024-05-14 10:09:03.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m`vtc-lectures-content-3072-v1` - Retriever: Vectorstore\u001b[0m\n"
					]
				}
			],
			"source": [
				"qdrant_lectures_content = vectorstores.QdrantWrapper(\n",
				"  qdrant_host=os.getenv(\"QDRANT_HOST\"),\n",
				"  qdrant_api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
				"  configs=configs_vtc,\n",
				"  **configs_vtc[\"vector_db\"][\"qdrant\"][\"lectures_content\"],\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Add multiple courses\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# for docs in docs_lectures_content_dict.values():\n",
				"#   qdrant_lectures_content.add_documents(docs)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### If error"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# # Check position based on len(docs) of the course\n",
				"# for position, (key, value) in enumerate(docs_lectures_content_dict.items()):\n",
				"#     if len(value) == 71:\n",
				"#         print(\"Key:\", key)\n",
				"#         print(\"Position:\", position)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# # Continue if error\n",
				"# for docs in (list(docs_lectures_content_dict.values()))[36:]:\n",
				"#   qdrant_lectures_content.add_documents(docs)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Add one course\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# qdrant_lectures_content.add_documents(docs_lectures_content)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## FAQ"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "2"
				}
			},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-05-14 10:09:07.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mFound collection: `vtc-faq-3072-v1`.\u001b[0m\n",
						"\u001b[32m2024-05-14 10:09:07.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1m`vtc-faq-3072-v1` - Embeddings: openai - {'model': 'text-embedding-3-large'}, 3072\u001b[0m\n",
						"\u001b[32m2024-05-14 10:09:07.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m`vtc-faq-3072-v1` - Retriever: Vectorstore\u001b[0m\n"
					]
				}
			],
			"source": [
				"qdrant_faq = vectorstores.QdrantWrapper(\n",
				"  qdrant_host=os.getenv(\"QDRANT_HOST\"),\n",
				"  qdrant_api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
				"  configs=configs_vtc,\n",
				"  **configs_vtc[\"vector_db\"][\"qdrant\"][\"faq\"]\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# qdrant_faq.add_documents(docs_faq)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Courses List"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "2"
				}
			},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-05-14 10:09:10.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mFound collection: `vtc-courses-information-3072-v1`.\u001b[0m\n",
						"\u001b[32m2024-05-14 10:09:10.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1m`vtc-courses-information-3072-v1` - Embeddings: openai - {'model': 'text-embedding-3-large'}, 3072\u001b[0m\n",
						"\u001b[32m2024-05-14 10:09:10.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.vectorstores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m`vtc-courses-information-3072-v1` - Retriever: Vectorstore\u001b[0m\n"
					]
				}
			],
			"source": [
				"qdrant_courses_information = vectorstores.QdrantWrapper(\n",
				"  qdrant_host=os.getenv(\"QDRANT_HOST\"),\n",
				"  qdrant_api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
				"  configs=configs_vtc,\n",
				"  **configs_vtc[\"vector_db\"][\"qdrant\"][\"courses_information\"]\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# qdrant_courses_information.add_documents(docs_courses_information)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### ONE course\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tools"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [],
			"source": [
				"system_message_onlinica = configs_vtc[\"prompts\"][\"system_message_onlinica\"]\n",
				"prompt_onlinica = prompts.create_prompt_tool_calling_agent(system_message_onlinica)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-05-14 10:09:19.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.agents\u001b[0m:\u001b[36m_create_agent\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mAgent type: tool_calling\u001b[0m\n"
					]
				}
			],
			"source": [
				"tools = [\n",
				"  qdrant_lectures_content.retriever_tool,\n",
				"  qdrant_faq.retriever_tool,\n",
				"  qdrant_courses_information.retriever_tool,\n",
				"]\n",
				"\n",
				"llm = chat_models.chat_openai\n",
				"# llm = chat_models.chat_anthropic\n",
				"agent = agents.MyAgent(\n",
				"  prompt=prompt_onlinica, tools=tools,\n",
				"  agent_type=configs_vtc[\"agents\"][\"agent_type_onlinica\"], \n",
				"  llm=llm\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = [\n",
				"  \"xin chào. Tên tôi là Bob.\",\n",
				"  \"bạn có nhớ tên tôi là gì không\",\n",
				"  \n",
				"  \"digital marketing là gì\",\n",
				"  \n",
				"  \"làm cách nào để đăng ký tài khoản onlinica\",\n",
				"  \"có mấy loại tài khoản onlinica\",\n",
				"  \"các khoá học tại onlinica có thời hạn sử dụng bao lâu\",\n",
				"  \"onlinica có mấy hình thức thanh toán\",\n",
				"  \"có thể thanh toán bằng momo được không\",\n",
				"  \n",
				"  \"các khóa học về design\",\n",
				"  \"các khóa học về trí tuệ nhân tạo\",\n",
				"  \"các khóa học về  ai\",\n",
				"  \"các khóa học của nguyễn ngọc tú uyên\",\n",
				"  \"các khóa học của tú uyên\",\n",
				"  \"các khóa học thầy trần anh tuấn dạy\",\n",
				"  \n",
				"  \"cách quản lý thời gian\",\n",
				"  \"nguyên lý phối màu\",\n",
				"]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
						"\u001b[32;1m\u001b[1;3mXin lỗi vì sự nhầm lẫn trước đó. Tên của bạn là Bob. Có điều gì tôi có thể giúp bạn hôm nay không, Bob?\u001b[0m\n",
						"\n",
						"\u001b[1m> Finished chain.\u001b[0m\n",
						"('Xin lỗi vì sự nhầm lẫn trước đó. Tên của bạn là Bob. Có điều gì tôi có thể '\n",
						" 'giúp bạn hôm nay không, Bob?')\n"
					]
				}
			],
			"source": [
				"input_message = questions[1]\n",
				"# await agent.stream_conversable_agent(questions[2])\n",
				"result = agent.invoke_agent(input_message)\n",
				"# await agent.stream_agent(input_message)\n",
				"pprint(result)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test prompt"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.8"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
