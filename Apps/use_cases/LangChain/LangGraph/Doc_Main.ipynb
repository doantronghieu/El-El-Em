{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 24,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import add_packages\n",
				"import typing, operator, json, os\n",
				"from toolkit.langchain import (\n",
				"\tchat_models, prompts, graphs, agent_tools, agents\n",
				")\n",
				"\n",
				"from pprint import pprint\n",
				"\n",
				"ACTION_END = \"end\"\n",
				"ACTION_CONTINUE = \"continue\"\n",
				"ACTION_FUNCTION_CALL = \"function_call\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [Github](https://github.com/langchain-ai/langgraph/tree/main)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Class"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Overview\n",
				"\n",
				"LangGraph: Library for stateful, multi-actor apps with LLMs, built on LangChain. Extends LangChain Expression Language to coordinate multiple chains/actors across multiple steps of computation cyclically. \n",
				"\n",
				"The main use of LangGraph is to add cycles to LLM application.\n",
				"\n",
				"Cycles are crucial for agent-like behaviors, calling an LLM in a loop to determine the next action.\n",
				"\n",
				"When to Use\n",
				"\n",
				"- If cycles are needed.\n",
				"- Langchain Expression Language defines chains easily but lacks a mechanism for cycles. "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Quick start\n",
				"\n",
				"LangGraph revolves around the concept of state. During graph execution, a state is passed between nodes and updated by each node with its return value. The method of updating the state is determined by the graph type or a custom function.\n",
				"\n",
				"State for example is a list of chat messages (LangChain chat models's output) using the MessageGraph class.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# The graph has a single node named \"oracle\" that runs a chat model and gives\n",
				"# back the result.\n",
				"\n",
				"# Initialize MessageGraph. \n",
				"graph = graphs.MessageGraph()\n",
				"\n",
				"VAR_ORACLE = \"oracle\"\n",
				"\n",
				"# Add \"oracle\" node to graph, calling model with input. \n",
				"graph.add_node(VAR_ORACLE, chat_models.chat_openai)\n",
				"# Add edge from \"oracle\" to END. Execution will end after current node.\n",
				"graph.add_edge(VAR_ORACLE, graphs.END)\n",
				"\n",
				"# Set \"oracle\" as entrypoint to the graph. \n",
				"graph.set_entry_point(VAR_ORACLE)\n",
				"\n",
				"# Compile graph to prevent further modifications.\n",
				"runnable = graph.compile()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# When executing the graph:\n",
				"# LangGraph adds input message to internal state, passes state to entrypoint node \"oracle\".\n",
				"# \"Oracle\" node executes, invokes chat model.\n",
				"# Chat model returns AIMessage, added to state.\n",
				"# Execution progresses to special END value, outputs final state.\n",
				"# Result: list of two chat messages as output.\n",
				"runnable.invoke(prompts.HumanMessage(\"What is 1 + 1?\"))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Interaction with LCEL\n",
				"Add_node can take any function or runnable as input. The input to the runnable is the entire current state.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def call_oracle(messages: list):\n",
				"\treturn chat_models.chat_openai.invoke(messages)\n",
				"\n",
				"graph.add_node(VAR_ORACLE, call_oracle)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Documentation\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Prebuilt Example\n",
				"\n",
				"Several methods have been added to simplify the use of common, prebuilt graphs and components.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## ToolExecutor\n",
				"\n",
				"Class for calling tools, parameterized by a list of tools.\n",
				"\n",
				"It exposes a runnable interface to call tools by passing in an AgentAction to look up the relevant tool and call it with the appropriate input.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from langgraph.prebuilt import ToolExecutor\n",
				"\n",
				"tools = [...]\n",
				"tool_executor = ToolExecutor(tools)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## `chat_agent_executor.create_function_calling_executor`\n",
				"\n",
				"Helper function for creating a graph for chat model using function calling. Pass in model and list of tools. Model must support OpenAI function calling.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"----\n",
						"[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1714002642, 'localtime': '2024-04-24 16:50'}, 'current': {'last_updated_epoch': 1714002300, 'last_updated': '2024-04-24 16:45', 'temp_c': 16.1, 'temp_f': 61.0, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 290, 'wind_dir': 'WNW', 'pressure_mb': 1018.0, 'pressure_in': 30.05, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 67, 'cloud': 75, 'feelslike_c': 16.1, 'feelslike_f': 61.0, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 14.8, 'gust_kph': 23.8}}\"}]\n",
						"----\n",
						"The current weather in San Francisco is partly cloudy with a temperature of 61.0째F (16.1째C). The wind speed is 16.9 km/h coming from the WNW direction. The humidity is at 67% and the visibility is 16.0 km.\n",
						"----\n"
					]
				}
			],
			"source": [
				"from langgraph.prebuilt import chat_agent_executor\n",
				"from langchain_openai import ChatOpenAI\n",
				"from langchain_community.tools.tavily_search import TavilySearchResults\n",
				"from langgraph.prebuilt import chat_agent_executor\n",
				"from langchain_core.messages import HumanMessage\n",
				"\n",
				"tools = [TavilySearchResults(max_results=1)]\n",
				"model = ChatOpenAI()\n",
				"\n",
				"app = chat_agent_executor.create_function_calling_executor(model, tools)\n",
				"\n",
				"inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
				"for s in app.stream(inputs):\n",
				"\tprint(list(s.values())[0][\"messages\"][0].content)\n",
				"\tprint(\"----\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## `chat_agent_executor.create_tool_calling_executor`\n",
				"\n",
				"Helper function for creating a graph with a chat model using tool calling. Pass in model and list of tools. Model must support OpenAI tool calling.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"----\n",
						"[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1714002642, 'localtime': '2024-04-24 16:50'}, 'current': {'last_updated_epoch': 1714002300, 'last_updated': '2024-04-24 16:45', 'temp_c': 16.1, 'temp_f': 61.0, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 290, 'wind_dir': 'WNW', 'pressure_mb': 1018.0, 'pressure_in': 30.05, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 67, 'cloud': 75, 'feelslike_c': 16.1, 'feelslike_f': 61.0, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 14.8, 'gust_kph': 23.8}}\"}]\n",
						"----\n",
						"The current weather in San Francisco is as follows:\n",
						"- Temperature: 16.1째C (61.0째F)\n",
						"- Condition: Partly cloudy\n",
						"- Wind: 16.9 km/h from WNW\n",
						"- Pressure: 1018.0 mb\n",
						"- Humidity: 67%\n",
						"- Cloud Cover: 75%\n",
						"- Visibility: 16.0 km\n",
						"\n",
						"For more detailed information, you can visit [WeatherAPI](https://www.weatherapi.com/).\n",
						"----\n"
					]
				}
			],
			"source": [
				"from langgraph.prebuilt import chat_agent_executor\n",
				"from langchain_openai import ChatOpenAI\n",
				"from langchain_community.tools.tavily_search import TavilySearchResults\n",
				"from langgraph.prebuilt import chat_agent_executor\n",
				"from langchain_core.messages import HumanMessage\n",
				"\n",
				"tools = [TavilySearchResults(max_results=1)]\n",
				"model = ChatOpenAI()\n",
				"\n",
				"app = chat_agent_executor.create_tool_calling_executor(model, tools)\n",
				"\n",
				"inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
				"for s in app.stream(inputs):\n",
				"\tprint(list(s.values())[0][\"messages\"][0].content)\n",
				"\tprint(\"----\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Create Agent Executor\n",
				"\n",
				"Helper function for creating LangChain Agents graph by passing in agent and list of tools.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## StateGraph\n",
				"\n",
				"The main entrypoint, responsible for constructing the graph. The graph is parameterized by a state object passed to each node.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `__init__`\n",
				"\n",
				"When constructing the graph, pass in a schema for a state. Each node returns operations to update the state. Operations can SET specific attributes or ADD to existing attributes on the state, denoted by annotating the state object.\n",
				"\n",
				"Specify the schema with a typed dictionary: `from typing import TypedDict`.\n",
				"\n",
				"Annotate attributes using `from typing import Annotated`. Only supported annotation is `import operator; operator.add`. This annotation adds new result to existing value.\n",
				"\n",
				"Example:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class AgentState(typing.TypedDict):\n",
				"\tinput: str\n",
				"\t# Outcome of call to agent. Requires `None` as valid type, as initial state\n",
				"\tagent_outcome: typing.Union[agents.AgentAction, agents.AgentFinish, None]\n",
				"\t# List of actions and corresponding observations\n",
				"\t# Annotate with `operator.add` to indicate operations should be added to \n",
				"\t# existing values, not overwritten.\n",
				"\tintermediate_steps: typing.Annotated[list[tuple[agents.AgentAction, str]],\n",
				"                                      operator.add]\n",
				"\n",
				"# Initialize the StateGraph with this state\n",
				"graph = graphs.StateGraph(AgentState)\n",
				"\n",
				"# Create nodes and edges\n",
				"...\n",
				"\n",
				"# Compile the graph\n",
				"app = graph.compile()\n",
				"\n",
				"# Inputs: dictionary <- State: TypedDict\n",
				"inputs = {\n",
				"\t\"input\": \"hi\"\n",
				"\t# `agent_outcome` set by the graph at some point. Defaults to None\n",
				"\t# `intermediate_steps` built up over time by the graph. Defaults to empty list. \n",
				"\t# `intermediate_steps` annotated with `operator.add`\n",
				"}"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `.add_node`\n",
				"\n",
				"Adds a node to the graph: \n",
				"- key (a unique string representing the node's name)\n",
				"- action (a function or a runnable)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `.add_edge`\n",
				"\n",
				"Creates an edge between nodes, passing output from the first to the next.\n",
				"\n",
				"- start_key: Name of start node in graph.\n",
				"- end_key: Name of end node in graph."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `.add_conditional_edges`\n",
				"\n",
				"Adds conditional edges where only one downstream edge is taken based on the start node's results. \n",
				"\n",
				"- `start_key`: Name of start node in graph.\n",
				"- `condition`: Function to determine next edge based on start node output.\n",
				"- `conditional_edge_mapping`: Maps strings (`condition`'s output) to strings (downstream nodes).\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `.set_entry_point`\n",
				"\n",
				"The starting point of the graph, called first.\n",
				"\n",
				"- key: Name of the node to call first.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `.set_conditional_entry_point`\n",
				"\n",
				"Adds a conditional entry point where the graph calls the `condition` Callable to decide the first node to enter.\n",
				"\n",
				"- `condition`: Function to call to decide next step based on input to graph. Return string for conditional_edge_mapping.\n",
				"- `conditional_edge_mapping`: Mapping of string (`condition` outputs) to string (downstream nodes to call)."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### `.set_finish_point`\n",
				"\n",
				"Final result node in the graph.\n",
				"\n",
				"- key: Name of Node returns final output when called.\n",
				"\n",
				"Note: Only call this if you have not created an edge leading to END."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Graph\n",
				"\n",
				"Similar to StateGraph but does not update a state object over time. It relies on passing the full state from each step, where the output of one node becomes the input of the next.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## END\n",
				"\n",
				"Special node marks the end of the graph, ensuring that anything reaching it will be the final output.Possible uses:\n",
				"\n",
				"- As `end_key` in `add_edge`\n",
				"- value in `conditional_edge_mapping` for `add_conditional_edges`"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Conditional edges\n",
				"\n",
				"Use conditional edges to route execution to a node based on the current state using a function.\n",
				"\n",
				"Math can be difficult for LLMs, so allow them to call a \"multiply\" node using tool calling.\n",
				"\n",
				"Recreate graph with \"multiply\" to calculate result of recent message if tool call, bind calculator to OpenAI model as tool for model to use as needed to respond to the current state.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"NODE_ORACLE = \"oracle\"\n",
				"NODE_MULTIPLY = \"multiply\"\n",
				"\n",
				"@agent_tools.tool\n",
				"def multiply(n1: int, n2: int):\n",
				"\t\"\"\"\n",
				"\tMultiplies two numbers together.\n",
				"\t\"\"\"\n",
				"\treturn n1 * n2\n",
				"\n",
				"model = chat_models.chat_openai\n",
				"tools = [\n",
				"\tagent_tools.convert_to_openai_tool(multiply)\n",
				"]\n",
				"model_with_tools = model.bind(tools=tools)\n",
				"\n",
				"graph = graphs.MessageGraph()\n",
				"\n",
				"def invoke_model(\n",
				"\tstate: typing.List[prompts.BaseMessage]\n",
				"):\n",
				"\treturn model_with_tools.invoke(state)\n",
				"\n",
				"def invoke_tool(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				"):\n",
				"  tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
				"  \n",
				"  multiply_call = None\n",
				"  \n",
				"  for tool_call in tool_calls:\n",
				"    if tool_call[\"function\"][\"name\"] == \"multiply\":\n",
				"      multiply_call = tool_call\n",
				"  \n",
				"  if multiply_call is None:\n",
				"    raise Exception(\"No adder input found.\")\n",
				"  \n",
				"  res = multiply.invoke(\n",
				"\t\tjson.loads(multiply_call[\"function\"][\"arguments\"])\n",
				"\t)\n",
				"  \n",
				"  return prompts.ToolMessage(\n",
				"\t\ttool_call_id=multiply_call[\"id\"], content=res,\n",
				"\t)\n",
				"\n",
				"def router(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				") -> str:\n",
				"  tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
				"  \n",
				"\t# If model output has tool call, move to \"multiply\" node. Otherwise, end.\n",
				"  if len(tool_calls):\n",
				"    return NODE_MULTIPLY\n",
				"  else:\n",
				"    return ACTION_END\n",
				"\n",
				"graph.add_node(NODE_ORACLE, invoke_model)\n",
				"graph.add_node(NODE_MULTIPLY, invoke_tool)\n",
				"graph.add_edge(NODE_MULTIPLY, graphs.END)\n",
				"\n",
				"graph.set_entry_point(NODE_ORACLE)\n",
				"\n",
				"# The desired outcome: If the \"oracle\" node returns a message expecting a tool \n",
				"# call, proceed to the \"multiply\" node. Otherwise, terminate execution. \n",
				"graph.add_conditional_edges(\n",
				"\tstart_key=NODE_ORACLE, condition=router,\n",
				"\tconditional_edge_mapping={\n",
				"\t\tNODE_MULTIPLY: NODE_MULTIPLY,\n",
				"\t\tACTION_END: graphs.END,\n",
				"\t}\n",
				")\n",
				"\n",
				"runnable = graph.compile()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"runnable.invoke(prompts.HumanMessage(\"What is your name?\"))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"runnable.invoke(prompts.HumanMessage(\"Waht is 9 * 3?\"))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Cycles\n",
				"\n",
				"AgentExecutor uses chat models and function calling, representing all its state as a list of messages."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Set up the tools\n",
				"\n",
				"Define the tools to use. Wrap tools in LangGraph ToolExecutor to call and return output from ToolInvocation objects with tool and tool_input attributes.\n",
				"\n",
				"## Set up the model\n",
				"\n",
				"Any model supporting function calling can be selected.\n",
				"\n",
				"Ensure the model is aware of the available tools by converting LangChain tools to OpenAI function format and binding them to the model class.\n",
				"\n",
				"## Define Agent state\n",
				"\n",
				"StateGraph uses a state object passed to each node, which returns operations to update the state by either setting specific attributes or adding to existing ones.\n",
				"\n",
				"The state is a list of messages (internal state). Each node will add messages (returned values of a node) to the list using a TypedDict with one key (messages) annotated to always add with operator.add\n",
				"\n",
				"## Define the nodes, edges\n",
				"\n",
				"- Nodes: agent (action maker) and function (action executor for agent).\n",
				"\n",
				"- Edges: Based on the output of a node, one of several paths may be taken. The path is determined when the node is run (the LLM decides).\n",
				"\n",
				"\t- Conditional Edge: Call agent -> \n",
				"\t\t- ? Agent should take action -> Call function to invoke tools\n",
				"\t\t- ? Agent finished -> Finish\n",
				"\t- Normal Edge: Invoke tools -> Go back for next Agent decision\n",
				"\n",
				"Example: Define nodes and a function to determine the conditional edge to take.\n",
				"\n",
				"## Define Graph\n",
				"\n",
				"Put everything together.\n",
				"\n",
				"## Use it!\n",
				"\n",
				"This exposes the same interface as other LangChain runnables. This runnable accepts a list of messages.\n",
				"\n",
				"This process takes time as it makes background calls. Use streaming to see real-time intermediate results.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"tools = [\n",
				"\tagent_tools.TavilySearchResults(max_results=1),\n",
				"]\n",
				"tool_executor = graphs.ToolExecutor(tools)\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"\n",
				"model = chat_models.chat_openai\n",
				"\n",
				"functions = [\n",
				"\tagent_tools.convert_to_openai_function(t) for t in tools\n",
				"]\n",
				"model = model.bind_functions(functions)\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"\n",
				"class AgentState(typing.TypedDict):\n",
				"  messages: typing.Annotated[typing.Sequence[prompts.BaseMessage], \n",
				"                             operator.add]\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"\n",
				"def should_continue(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				") -> str:\n",
				"\t\"\"\"\n",
				"\tDetermines whether to continue or not\n",
				"\t\"\"\"\n",
				"\tlast_message = state[\"messages\"][-1]\n",
				"\n",
				"\tif \"function_call\" not in last_message.additional_kwargs:\n",
				"\t\treturn ACTION_END\n",
				"\telse:\n",
				"\t\treturn ACTION_CONTINUE\n",
				"\n",
				"def call_model(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				"):\n",
				"\t\"\"\"\n",
				"\tCalls the model\n",
				"\t\"\"\"\n",
				"\tmessages = state[\"messages\"]\n",
				"\tresponse = model.invoke(messages)\n",
				" \n",
				"\t# return a list, because this will get added to the existing list\n",
				"\treturn {\n",
				"\t\t\"messages\": [response]\n",
				"\t}\n",
				"\n",
				"def call_tool(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				"):\n",
				"\t\"\"\"\n",
				"\tExecutes tools\n",
				"\t\"\"\"\n",
				"\tmessages = state[\"messages\"]\n",
				"\tlast_message = messages[-1]\n",
				" \n",
				"\taction = graphs.ToolInvocation(\n",
				"\t\ttool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
				"\t\ttool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
				"\t)\n",
				" \n",
				"\tresponse = tool_executor.invoke(action)\n",
				"\tfunction_message = prompts.FunctionMessage(content=str(response), name=action.tool)\n",
				" \n",
				"\t# return a list, because this will get added to the existing list\n",
				"\treturn {\n",
				"\t\t\"messages\": [function_message]\n",
				"\t}\n",
				" \n",
				"#*------------------------------------------------------------------------------\n",
				"\n",
				"workflow = graphs.StateGraph(AgentState)\n",
				"\n",
				"# Define two nodes for cycling.\n",
				"NODE_AGENT = \"agent\"\n",
				"NODE_ACTION = \"action\"\n",
				"\n",
				"workflow.add_node(NODE_AGENT, call_model)\n",
				"workflow.add_node(NODE_ACTION, call_tool)\n",
				"\n",
				"# Set entrypoint as `agent`. The first node called.\n",
				"workflow.set_entry_point(NODE_AGENT)\n",
				"\n",
				"workflow.add_conditional_edges(\n",
				"\t# Define start node as `agent`, edges taken after `agent` node is called.\n",
				"\tNODE_AGENT,\n",
				"\t# Function to determine which node is called next.\n",
				"\tshould_continue,\n",
				"\t# Pass in a mapping where keys are strings and values are nodes. \n",
				"\t# END is a special node signifying the end of the graph. \n",
				"\t# Call `should_continue` and match the output against the keys in the mapping \n",
				"\t# to determine the next node.\n",
				"\t{\n",
				"\t\t# If `tools`, then call the tool node.\n",
				"\t\tACTION_CONTINUE: NODE_ACTION,\n",
				"\t\tACTION_END: graphs.END,\n",
				"\t}\n",
				")\n",
				"\n",
				"# Normal edge added from `tools` to `agent`, `agent` called after `tools`.\n",
				"workflow.add_edge(NODE_ACTION, NODE_AGENT)\n",
				"\n",
				"# Compile it into a LangChain Runnable\n",
				"app = workflow.compile()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = {\n",
				"\t\"messages\": [\n",
				"\t\tprompts.HumanMessage(content=\"What is the weather in sf?\")\n",
				"\t]\n",
				"}\n",
				"\n",
				"app.invoke(inputs)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Streaming\n",
				"\n",
				"LangGraph supports various streaming types.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming Node Output\n",
				"\n",
				"Stream output produced by each node."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = {\n",
				"\t\"messages\": [\n",
				"\t\tprompts.HumanMessage(content=\"What is the weather in sf?\")\n",
				"\t]\n",
				"}\n",
				"\n",
				"for output in app.stream(inputs):\n",
				"\t# stream() yields dictionaries with output keyed by node name\n",
				"\tfor key, value in output.items():\n",
				"\t\tprint(f\"Node: `{key}`\")\n",
				"\t\tprint(\"-\"*10)\n",
				"\t\tprint(value)\n",
				"\tprint(f\"{'-'*10}\\n\")\n",
				" "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming LLM Tokens\n",
				"\n",
				"Access LLM tokens as they are produced from each node. Only \"agent\" node produces LLM tokens. Use LLM supporting streaming and set it during construction (e.g. ChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)) for proper functionality.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = {\n",
				"\t\"messages\": [\n",
				"\t\tprompts.HumanMessage(content=\"What is the weather in sf?\")\n",
				"\t]\n",
				"}\n",
				"\n",
				"async for output in app.astream_log(inputs, include_types=[\"llm\"]):\n",
				"  # astream_log() yields the requested logs (here LLMs) in JSONPatch format\n",
				"\tfor op in output.ops:\n",
				"\t\tif op[\"path\"] == \"/streamed_output/-\":\n",
				"\t\t\t# this is the output from .stream()\n",
				"\t\t\t...\n",
				"\t\telif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n",
				"\t\t\t\"/streamed_output/-\"\n",
				"\t\t):\n",
				"\t\t\t# because we chose to only include LLMs, these are LLM tokens\n",
				"\t\t\tprint(op[\"value\"])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# How-to Guides"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Async](https://github.com/langchain-ai/langgraph/blob/main/examples/async.ipynb)\n",
				"\n",
				"If running LangGraph in async workflows, create nodes to be async by default. \n",
				"\n",
				"Example: Build a chat executor with native async implementations to take advantage of Chat Models with async clients, eliminating the need for calling the model in a separate thread."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Tools setup\n",
				"\n",
				"Define tools to use. \n",
				"\n",
				"Wrap tools in ToolExecutor: a class that takes ToolInvocation and calls the tool, returning the output. ToolInvocation has tool and tool_input attributes."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 21,
			"metadata": {},
			"outputs": [],
			"source": [
				"tools = [\n",
				"\tagent_tools.TavilySearchResults(max_results=1),\n",
				"]\n",
				"tool_executor = graphs.ToolExecutor(tools)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Model setup\n",
				"\n",
				"Load the chat model that meets two criteria:\n",
				"- Represent all agent states in messages for it to work effectively.\n",
				"- Compatible with OpenAI function calls, must be an OpenAI model or have a similar interface.\n",
				"\n",
				"Ensure the model is aware of the available tools by converting LangChain tools to OpenAI function format and binding them to the model class."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 22,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = chat_models.chat_openai\n",
				"functions = [\n",
				"\tagent_tools.convert_to_openai_function(t) for t in tools\n",
				"]\n",
				"model = model.bind_functions(functions)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Agent state definition\n",
				"\n",
				"The main type of graph in langgraph is the StatefulGraph. This graph is parameterized by a state object that is passed to each node. Nodes return operations to update the state, either SET specific attributes or ADD to existing attributes based on the annotation of the state object.\n",
				"\n",
				"Create a state to track a list of messages using a TypedDict with one key (messages) to ensure messages are always added.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 23,
			"metadata": {},
			"outputs": [],
			"source": [
				"class AgentState(typing.TypedDict):\n",
				"\tmessages: typing.Annotated[typing.Sequence[prompts.BaseMessage], operator.add]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Nodes definition\n",
				"\n",
				"Node: a function or a runnable. Two main nodes:\n",
				"- The agent: decides actions.\n",
				"- Function to invoke tools: executes actions for the agent.\n",
				"\n",
				"Edges: Based on the output of a node, one of several paths may be taken. \n",
				"- Conditional Edge: after agent call, either the function invokes tools if action instructed, or finish if agent indicates completion\n",
				"- Normal Edge: after tools invoked, always return to agent for next steps\n",
				"\n",
				"Define each node as an async function and a function to determine the conditional edge to take."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 25,
			"metadata": {},
			"outputs": [],
			"source": [
				"def should_continue(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				") -> str:\n",
				"\t\"\"\"\n",
				"\tDetermines whether to continue or not\n",
				" \t\"\"\"\n",
				"\tmessages = state[\"messages\"]\n",
				"\tlast_message = messages[-1]\n",
				"\t\n",
				"\tif ACTION_FUNCTION_CALL not in last_message.additional_kwargs:\n",
				"\t\treturn ACTION_END\n",
				"\telse:\n",
				"\t\treturn ACTION_CONTINUE\n",
				"\n",
				"\n",
				"async def call_model(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				"):\n",
				"\t\"\"\"\n",
				" \tCalls the model\n",
				"  \"\"\"\n",
				"\tmessages = state[\"messages\"]\n",
				"\tresponse = await model.ainvoke(messages)\n",
				"\t# Return a list to add to the existing list.\n",
				"\treturn {\n",
				"\t\t\"messages\": [\n",
				"\t\t\tresponse,\n",
				"\t\t],\n",
				"\t}\n",
				" \n",
				"async def call_tool(\n",
				"\tstate: typing.List[prompts.BaseMessage],\n",
				"):\n",
				"\tmessages = state[\"messages\"]\n",
				"\t# Last message involves a function call based on the continue condition.\n",
				"\tlast_message = messages[-1]\n",
				"\t# Construct ToolInvocation from function_call\n",
				"\taction = graphs.ToolInvocation(\n",
				"\t\ttool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
				"\t\ttool_input=json.loads(\n",
				"\t\t\tlast_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
				"\t\t),\n",
				"\t)\n",
				"\tresponse = await tool_executor.ainvoke(action)\n",
				"\tfunction_message = prompts.FunctionMessage(content=str(response), name=action.tool)\n",
				"\t# Return a list to add to the existing list.\n",
				"\treturn {\n",
				"\t\t\"messages\": [\n",
				"\t\t\tfunction_message,\n",
				"\t\t]\n",
				"\t}"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Graph definition\n",
				"\n",
				"Define the graph by putting everything together.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"workflow = graphs"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Use it! -> Use it!\n",
				"\n",
				"We can now use it, exposing the same interface as all other LangChain runnables.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming\n",
				"\n",
				"LangGraph supports various streaming types.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Streaming Node Output\n",
				"\n",
				"LangGraph makes it easy to stream output produced by each node.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Streamlining LLM Tokens\n",
				"\n",
				"Access LLM tokens produced by each node. Only \"agent\" node produces LLM tokens. Use LLM supporting streaming and set it when constructing LLM (e.g. ChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)) for proper functionality.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Streaming Tokens](https://github.com/langchain-ai/langgraph/blob/main/examples/streaming-tokens.ipynb)\n",
				"\n",
				"Sometimes language models take time to respond. Stream tokens to end users for faster responses.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Persistence](https://github.com/langchain-ai/langgraph/blob/main/examples/persistence.ipynb)\n",
				"\n",
				"Save and resume graph state. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Human-in-the-loop](https://github.com/langchain-ai/langgraph/blob/main/examples/human-in-the-loop.ipynb) workflows\n",
				"\n",
				"Allow for human review the current state before moving to a specific node. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Visualizing the graph](https://github.com/langchain-ai/langgraph/blob/main/examples/visualization.ipynb)\n",
				"\n",
				"Print out and visualize the graph, generating ascii art and pngs."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [\"Time Travel\"](https://github.com/langchain-ai/langgraph/blob/main/examples/time-travel.ipynb)\n",
				"\n",
				"Modify the state at any point in the graph execution and rerun from there. This feature is beneficial for debugging workflows and correcting the state in end user-facing workflows. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Examples\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## ChatAgentExecutor: function calling\n",
				"\n",
				"Takes a list of messages and outputs a list of messages, with all agent state represented as a list. Ideal for chat models supporting function calling.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Getting Started Notebook](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb)\n",
				"\n",
				"Creating executor from scratch\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [High Level Entrypoint](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/high-level.ipynb)\n",
				"\n",
				"Using high level entrypoint for chat agent executor.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Modifications\n",
				"\n",
				"How to modify the base chat agent executor. \n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Human-in-the-loop](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/human-in-the-loop.ipynb)\n",
				"\n",
				"Adding a human-in-the-loop component\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Force calling a tool first](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/force-calling-a-tool-first.ipynb)\n",
				"\n",
				"Always calling a specific tool first\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Respond in a specific format](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/respond-in-format.ipynb)\n",
				"\n",
				"Forcing the agent to respond in a specific format\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Dynamically returning tool output directly](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/dynamically-returning-directly.ipynb)\n",
				"\n",
				"Allowing the agent to choose whether to return the tool output directly\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Managing agent steps](https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/managing-agent-steps.ipynb)\n",
				"\n",
				"Explicitly managing intermediate steps taken by the agent\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Planning Agent\n",
				"\n",
				"Agent architectures in the \"plan-and-execute\" style. An LLM planner decomposes a user request into a program, an executor executes the program, and an LLM synthesizes a response (and/or dynamically replans) based on the program outputs.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Plan-and-execute](https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb)\n",
				"\n",
				"Agent with a planner generates a multi-step task list, an executor invokes the tools in the plan, and a replanner updates the plan.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Reasoning without Observation](https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb)\n",
				"\n",
				"planner generates task list with saved observations as variables to reduce re-planning\n",
				" \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [LLMCompiler](https://github.com/langchain-ai/langgraph/blob/main/examples/llm-compiler/LLMCompiler.ipynb)\n",
				"\n",
				"planner generates DAG of tasks with variable responses. Tasks are streamed and executed eagerly to minimize tool execution runtime\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Reflection / Self-Critique\n",
				"\n",
				"When focusing on output quality, use self-critique and external validation to improve system outputs. \n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Basic Reflection](https://github.com/langchain-ai/langgraph/tree/main/examples/reflection/reflection.ipynb)\n",
				"\n",
				"Add a \"reflect\" step in graph to prompt system to revise its outputs.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Reflexion](https://github.com/langchain-ai/langgraph/tree/main/examples/reflexion/reflexion.ipynb)\n",
				"\n",
				"Critique agent responses to guide next steps.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Language Agent Tree Search](https://github.com/langchain-ai/langgraph/tree/main/examples/lats/lats.ipynb)\n",
				"\n",
				"Execute multiple agents in parallel with reflection and rewards for Monte Carlo Tree Search.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Multi-agent\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Multi-agent collaboration](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb)\n",
				"\n",
				"Creating two agents to work together to solve a problem\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Multi-agent with supervisor](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb)\n",
				"\n",
				"Orchestrating individual agents using \"supervisor\" LLM to distribute work\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Hierarchical agent teams](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb)\n",
				"\n",
				"Orchestrating \"teams\" of agents as nested graphs for collaboration to solve a problem\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Web Research\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [STORM](https://github.com/langchain-ai/langgraph/tree/main/examples/storm/storm.ipynb)\n",
				"\n",
				"Writing system that generates Wikipedia-style articles on any topic, applying outline generation + multi-perspective question-answering for added breadth and reliability. Based on STORM by Shao, et. al.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Chatbot Evaluation Simulation\n",
				"\n",
				"Simulations can help evaluate chat bots. \n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Chat bot evaluation as multi-agent simulation](https://github.com/langchain-ai/langgraph/blob/main/examples/chatbot-simulation-evaluation/agent-simulation-evaluation.ipynb)\n",
				"\n",
				"simulate dialogue between a \"virtual user\" and your chat bot\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Evaluating over a dataset](https://github.com/langchain-ai/langgraph/tree/main/examples/chatbot-simulation-evaluation/langsmith-agent-simulation-evaluation.ipynb)\n",
				"\n",
				"benchmark your chatbot over a LangSmith dataset, tasks a simulated customer to red-team your chat bot.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Multimodal\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [WebVoyager](https://github.com/langchain-ai/langgraph/blob/main/examples/web-navigation/web_voyager.ipynb)\n",
				"\n",
				"Vision-enabled web browsing agent using Set-of-marks prompting for navigation and task execution.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Chain-of-Table\n",
				"\n",
				"Chain of Table framework answers questions on tabular data\n",
				"\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "llm",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.8"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
