{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import add_packages\n",
    "import json\n",
    "from toolkit.langchain import (\n",
    "\tmodels, graphs, tools, prompts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = graphs.MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_tavily_search = tools.TavilySearchResults(max_results=2)\n",
    "\n",
    "my_tools = [\n",
    "\ttool_tavily_search\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestAssistance(graphs.BaseModel):\n",
    "\t\"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "\tTo use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    "\t\"\"\"\n",
    "\trequest: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = models.create_llm(provider='openai', version='gpt-3.5-turbo-0125')\n",
    "\n",
    "llm = llm.bind_tools(my_tools + [RequestAssistance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(graphs.TypedDict):\n",
    "\t# Messages have the type \"list\". The `add_messages` function\n",
    "\t# in the annotation defines how this state key should be updated\n",
    "\t# (in this case, it appends messages to the list, rather than overwriting them)\n",
    "\tmessages: graphs.Annotated[list, graphs.add_messages]\n",
    "\task_human: bool\n",
    "\n",
    "graph_builder = graphs.StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "\tresponse: prompts.AIMessage = llm.invoke(state[\"messages\"])\n",
    "\task_human = False\n",
    "\t\n",
    "\tif response.tool_calls \\\n",
    "   \t\tand response.tool_calls[0][\"name\"] == RequestAssistance.__name__:\n",
    "\t\task_human = True\n",
    "\t\n",
    "\treturn {\n",
    "\t\t\"messages\": [response], \"ask_human\": ask_human\n",
    "\t}\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"tools\", graphs.ToolNode(tools=[tool_tavily_search]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_response(response: str, ai_msg: prompts.AIMessage):\n",
    "\treturn prompts.ToolMessage(\n",
    "\t\tcontent=response,\n",
    "\t\ttool_call_id=ai_msg.tool_calls[0][\"id\"],\n",
    "\t)\n",
    "\n",
    "def node_human(state: State):\n",
    "\tnew_msgs = []\n",
    "\t\n",
    "\tif not isinstance(state[\"messages\"][-1], prompts.ToolMessage):\n",
    "\t\t# Typically, the user will have updated the state during the interrupt.\n",
    "\t\t# If they choose not to, we will include a placeholder ToolMessage to\n",
    "\t\t# let the LLM continue.\n",
    "\t\tnew_msgs.append(\n",
    "\t\t\tcreate_tool_response(\"No response from human.\", state[\"messages\"][-1])\n",
    "\t\t)\n",
    "\t\n",
    "\treturn {\n",
    "\t\t\"messages\": new_msgs,\n",
    "\t\t\"ask_human\": False,\n",
    "\t}\n",
    "\n",
    "graph_builder.add_node(\"human\", node_human)\n",
    "\n",
    "def select_next_node(\n",
    "  state: State\n",
    ") -> graphs.Literal[\"human\", \"tools\", \"__end__\"]:\n",
    "\tif state[\"ask_human\"]:\n",
    "\t\treturn \"human\"\n",
    "\treturn graphs.tools_condition(state)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "\t\"chatbot\",\n",
    "\tselect_next_node,\n",
    "\t{\n",
    "\t\t\"human\": \"human\",\n",
    "\t\t\"tools\": \"tools\",\n",
    "\t\t\"__end__\": \"__end__\",\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(graphs.START, \"chatbot\")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"human\", \"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "  checkpointer=memory,\n",
    "\tinterrupt_before=[\"human\"],\n",
    "\t# interrupt_after=[\"tools\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.display_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# user_input = \"Hi there! My name is Will.\"\n",
    "# user_input = \"Remember my name?\"\n",
    "# user_input = \"I need some expert guidance for building this AI agent. Could you request assistance for me?\"\n",
    "# user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
    "user_input = \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\"\n",
    "\n",
    "events = graph.stream(\n",
    "\t{\"messages\": (\"user\", user_input)}, config, stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "\tif \"messages\" in event:\n",
    "\t\tevent[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "states = graph.get_state_history(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "\tNone, config, stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "\tevent[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Customer Support](https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prompt Generation from User Requirements](https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Collaboration](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/)\n",
    "\n",
    "Enable two agents to collaborate on a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Supervision](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/)\n",
    "\n",
    "Use an LLM to orchestrate and delegate to individual agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hierarchical Teams](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/)\n",
    "\n",
    "Orchestrate nested teams of agents to solve problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Adaptive RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agentic RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Corrective RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Self-RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [SQL Agent](https://langchain-ai.github.io/langgraph/tutorials/sql-agent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plan-and-Execute](https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute/)\n",
    "\n",
    "Implement a basic planning and execution agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Reasoning without Observation](https://langchain-ai.github.io/langgraph/tutorials/rewoo/rewoo/)\n",
    "\n",
    "Reduce re-planning by saving observations as variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LLMCompiler](https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/)\n",
    "\n",
    "Stream and eagerly execute a DAG of tasks from a planner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection & Critique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Basic Reflection](https://langchain-ai.github.io/langgraph/tutorials/reflection/reflection/)\n",
    "\n",
    "Prompt the agent to reflect on and revise its outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Reflexion](https://langchain-ai.github.io/langgraph/tutorials/reflexion/reflexion/)\n",
    "\n",
    "Critique missing and superfluous details to guide next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Language Agent Tree Search](https://langchain-ai.github.io/langgraph/tutorials/lats/lats/)\n",
    "\n",
    "Use reflection and rewards to drive a tree search over agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Self-Discover Agent](https://langchain-ai.github.io/langgraph/tutorials/self-discover/self-discover/)\n",
    "\n",
    "Analyze an agent that learns about its own capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent-based](https://langchain-ai.github.io/langgraph/tutorials/chatbot-simulation-evaluation/agent-simulation-evaluation/)\n",
    "\n",
    "Evaluate chatbots via simulated user interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [In LangSmith](https://langchain-ai.github.io/langgraph/tutorials/chatbot-simulation-evaluation/langsmith-agent-simulation-evaluation/)\n",
    "\n",
    "Evaluate chatbots in LangSmith over a dialog dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
