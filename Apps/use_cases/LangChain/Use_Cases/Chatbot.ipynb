{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 57,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import add_packages\n",
				"import config\n",
				"from pprint import pprint\n",
				"import os, typing\n",
				"\n",
				"from toolkit.langchain import (\n",
				"  document_loaders, text_splitters, text_embedding_models, vectorstores, \n",
				"  chat_models, prompts, utils, output_parsers, agents, memories, chains,\n",
				"  runnables, agent_tools\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# LangChain"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Chatbot (Usecase)\n",
				"\n",
				"Design and implement an LLM-powered chatbot. Key components include Chat Models, Prompt Templates, Chat History, and Retrievers. These elements work together to create a conversational chatbot."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Start\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Retrievers incorporating domain-specific knowledge to model\n",
				"\n",
				"# Use a document loader to pull data from a webpage.\n",
				"loader = document_loaders.WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
				"document = loader.load()\n",
				"\n",
				"# Split text into smaller chunks for LLM's context window\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=500, chunk_overlap=0,\n",
				")\n",
				"docs = text_splitter.split_documents(document)\n",
				"\n",
				"# Embed and store chunks in a vector database.\n",
				"vectorstore = vectorstores.chroma.Chroma.from_documents(\n",
				"  documents=docs, embedding=text_embedding_models.OpenAIEmbeddings()\n",
				")\n",
				"# Create a retriever from the initialized vectorstore.\n",
				"retriever = vectorstore.as_retriever(k=4)\n",
				"\n",
				"# Initialize the chat model for the chatbot's brain.\n",
				"# The model lacks a concept of state, doesn't consider previous conversation context\n",
				"# Must input the entire conversation history\n",
				"model = chat_models.chat_openai\n",
				"\n",
				"# In-memory ChatMessageHistory saves and loads chat messages.\n",
				"chat_history = memories.ChatMessageHistory()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Prompt template for easier formatting. Pipe a chain into the model.\n",
				"# Accept documents as context to \"stuff\" input documents into prompt, \n",
				"# handling formatting and passing other arguments directly.\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    \"Answer the user's questions based on the below context:\\n\\n{context}\"\n",
				"  ),\n",
				"  # Inserts chat messages into chain's input as chat_history to the prompt\n",
				"  prompts.MessagesPlaceholder(variable_name=\"messages\")\n",
				"])\n",
				"\n",
				"# Accepts dict as input, parses string for retriever.\n",
				"chain_documents = chains.create_stuff_documents_chain(model, prompt)\n",
				"\n",
				"#*-----------------------------------------------------------------------------\n",
				"\n",
				"# When a follow-up question is asked, the retrieved docs may not include \n",
				"# information from the previous question. The query is passed verbatim to the \n",
				"# retriever, so adding a query transformation step to remove references from the\n",
				"# input can help retrieve more informative documents.\n",
				"prompt_query_transform = prompts.ChatPromptTemplate.from_messages([\n",
				"  prompts.MessagesPlaceholder(variable_name=\"messages\"),\n",
				"  (\n",
				"    \"user\",\n",
				"    (\"Given the above conversation, generate a search query to look up in order \"\n",
				"     \"to get information relevant to the conversation. Only response with the \"\n",
				"     \"query, nothing else.\")\n",
				"  )\n",
				"])\n",
				"\n",
				"chain_query_transforming_retriever = runnables.RunnableBranch(\n",
				"  (\n",
				"    lambda x: len(x.get(\"messages\", [])) == 1,\n",
				"    # Pass the message's to the retriever if there is only one message.\n",
				"    (lambda x: x[\"messages\"][-1].content) | retriever,\n",
				"  ),\n",
				"  # Messages are passed to LLM chain to transform the query before passing it to\n",
				"  # the retriever\n",
				"  prompt_query_transform | model | output_parsers.StrOutputParser() | retriever,\n",
				").with_config(run_name=\"chain_chat_retriever\")\n",
				"\n",
				"#*-----------------------------------------------------------------------------\n",
				"\n",
				"chain_conversational_retrieval = runnables.RunnablePassthrough.assign(\n",
				"  context=chain_query_transforming_retriever\n",
				").assign(\n",
				"  answer=chain_documents,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"chat_history.add_user_message(\"how can langsmith help with testing?\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"response = chain_conversational_retrieval.invoke({\n",
				"  \"messages\": chat_history.messages, \n",
				"})\n",
				"chat_history.add_ai_message(response[\"answer\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"chat_history.add_user_message(\"tell me more about that\")\n",
				"response = chain_conversational_retrieval.invoke({\n",
				"  \"messages\": chat_history.messages, \n",
				"})\n",
				"chat_history.add_ai_message(response[\"answer\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Memory management\n",
				"\n",
				"A key feature of chatbots is their ability to use previous conversation turns as context. This state management can take various forms:\n",
				"\n",
				"- Stuffing previous messages into a chat model prompt.\n",
				"- Trimming old messages to reduce distracting information.\n",
				"- Synthesizing summaries for long conversations.\n",
				"  \n",
				"Use LangChainâ€™s message history class stores and loads chat messages from persistent storage.\n",
				"\n",
				"Store conversation turns directly for chain."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Automatic history management\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = chat_models.chat_openai\n",
				"\n",
				"\n",
				"# Prompt includes final input variable populating HumanMessage template after \n",
				"# chat history. Expect chat_history parameter with messages BEFORE current messages.\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    \"You are a helpful assistant. Answer all question to the best of your ability.\"\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"chat_history\"),\n",
				"  (\n",
				"    \"human\",\n",
				"    \"{input}\"\n",
				"  ),\n",
				"])\n",
				"\n",
				"chain = prompt | model\n",
				"\n",
				"# Chain wrapper RunnableWithMessageHistory handle process history automatically.\n",
				"# Pass latest input to conversation, RunnableWithMessageHistory appends input\n",
				"# variable to chat history.\n",
				"chat_history = memories.ChatMessageHistory()\n",
				"\n",
				"chain_memorizable = runnables.RunnableWithMessageHistory(\n",
				"  chain,\n",
				"  # Factory function returns message history for session id, enabling handling\n",
				"  # multiple users at once with different messages for each conversation.\n",
				"  lambda session_id: chat_history,\n",
				"  # specifies the tracked input stored in chat history.\n",
				"  input_messages_key=\"input\",\n",
				"  # specifies previous messages injected into prompt as. \n",
				"  # Prompt has MessagesPlaceholder named chat_history\n",
				"  history_messages_key=\"chat_history\",\n",
				"  # specifies which output to store as history for chains with multiple outputs, \n",
				"  # output_messages_key=\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Invoke chain with an additional configurable field specifying the session_id\n",
				"# to pass to the factory function. In real-world chains, return a chat history \n",
				"# corresponding to the passed session.\n",
				"\n",
				"# Initial user input\n",
				"inputs = [\n",
				"  \"Translate this sentence from English to French: I love programming.\",\n",
				"  \"What did I just ask you?\"\n",
				"]\n",
				"\n",
				"for input in inputs:\n",
				"  response = chain_memorizable.invoke(\n",
				"    {\"input\": input},\n",
				"    {\"configurable\": {\"session_id\": \"unused\"}},\n",
				"  )\n",
				"\n",
				"  print(response.content) "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Modifying chat history\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"##### Trimming messages\n",
				"\n",
				"LLMs and chat models have limited context windows, limit distractions by only storing the most recent n messages."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = chat_models.chat_openai\n",
				"\n",
				"\n",
				"# Prompt includes final input variable populating HumanMessage template after \n",
				"# chat history. Expect chat_history parameter with messages BEFORE current messages.\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    \"You are a helpful assistant. Answer all question to the best of your ability.\"\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"chat_history\"),\n",
				"  (\n",
				"    \"human\",\n",
				"    \"{input}\"\n",
				"  ),\n",
				"])\n",
				"\n",
				"chain = prompt | model\n",
				"\n",
				"# Chain wrapper RunnableWithMessageHistory handle process history automatically.\n",
				"# Pass latest input to conversation, RunnableWithMessageHistory appends input\n",
				"# variable to chat history.\n",
				"chat_history = memories.ChatMessageHistory()\n",
				"\n",
				"chain_memorizable = runnables.RunnableWithMessageHistory(\n",
				"  chain,\n",
				"  # Factory function returns message history for session id, enabling handling\n",
				"  # multiple users at once with different messages for each conversation.\n",
				"  lambda session_id: chat_history,\n",
				"  # specifies the tracked input stored in chat history.\n",
				"  input_messages_key=\"input\",\n",
				"  # specifies previous messages injected into prompt as. \n",
				"  # Prompt has MessagesPlaceholder named chat_history\n",
				"  history_messages_key=\"chat_history\",\n",
				"  # specifies which output to store as history for chains with multiple outputs, \n",
				"  # output_messages_key=\n",
				")\n",
				"\n",
				"\n",
				"# Use the message history with the RunnableWithMessageHistory chain.\n",
				"chat_history = memories.ChatMessageHistory()\n",
				"\n",
				"# Reduce context window, trim messages to 2 recent ones using clear method, \n",
				"# add back to history. Place method at front of chain for consistent calling.\n",
				"def trim_messages(chain_input, max_messages: int = 2):\n",
				"  stored_messages = chat_history.messages\n",
				"  \n",
				"  if len(stored_messages) <= max_messages:\n",
				"    return False\n",
				"  \n",
				"  chat_history.clear()\n",
				"  \n",
				"  # Our history keeps the most recent conversation. Next time the chain is \n",
				"  # called, only the `max_messages` most recent messages will be passed to the model. \n",
				"  for message in stored_messages[-max_messages:]:\n",
				"    chat_history.add_message(message)\n",
				"  \n",
				"  return True\n",
				"\n",
				"chain_memorizable_with_trimming = (\n",
				"  runnables.RunnablePassthrough.assign(trimmed_messages=trim_messages)\n",
				"  | chain_memorizable\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = [\n",
				"  \"My name is Bob\",\n",
				"  \"What is 1 + 1\",\n",
				"  \"Add one more\",\n",
				"  \"Add one more\",\n",
				"  \"Add one more\",\n",
				"  \"What is my name?\"\n",
				"  \"Add one more\",\n",
				"]\n",
				"\n",
				"for input in inputs:\n",
				"  response = chain_memorizable_with_trimming.invoke(\n",
				"    {\"input\": input},\n",
				"    {\"configurable\": {\"session_id\": \"unused\"}},\n",
				"  )\n",
				"\n",
				"  print(response.content) "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"##### Summary memory"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Call LLM to generate a conversation summary before calling the chain.\n",
				"# Modify prompt for LLM to expect condensed summary instead of chat history.\n",
				"\n",
				"model = chat_models.chat_openai\n",
				"\n",
				"\n",
				"# Prompt includes final input variable populating HumanMessage template after \n",
				"# chat history. Expect chat_history parameter with messages BEFORE current messages.\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    (\"You are a helpful assistant. Answer all question to the best of your \"\n",
				"     \"ability. The provided chat history includes facts about the user you \"\n",
				"     \"are speaking with.\")\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"chat_history\"),\n",
				"  (\n",
				"    \"human\",\n",
				"    \"{input}\"\n",
				"  ),\n",
				"])\n",
				"\n",
				"chain = prompt | model\n",
				"\n",
				"# Chain wrapper RunnableWithMessageHistory handle process history automatically.\n",
				"# Pass latest input to conversation, RunnableWithMessageHistory appends input\n",
				"# variable to chat history.\n",
				"chat_history = memories.ChatMessageHistory()\n",
				"\n",
				"chain_memorizable = runnables.RunnableWithMessageHistory(\n",
				"  chain,\n",
				"  # Factory function returns message history for session id, enabling handling\n",
				"  # multiple users at once with different messages for each conversation.\n",
				"  lambda session_id: chat_history,\n",
				"  # specifies the tracked input stored in chat history.\n",
				"  input_messages_key=\"input\",\n",
				"  # specifies previous messages injected into prompt as. \n",
				"  # Prompt has MessagesPlaceholder named chat_history\n",
				"  history_messages_key=\"chat_history\",\n",
				"  # specifies which output to store as history for chains with multiple outputs, \n",
				"  # output_messages_key=\n",
				")\n",
				"\n",
				"def summarize_message(chain_input):\n",
				"  stored_messages = chat_history.messages\n",
				"  if len(stored_messages) == 0:\n",
				"    return False\n",
				"  \n",
				"  prompt_summarization = prompts.ChatPromptTemplate.from_messages([\n",
				"    prompts.MessagesPlaceholder(variable_name=\"chat_history\"),\n",
				"    (\n",
				"      \"user\",\n",
				"      (\"Distill the above chat messages into a single summary message. Include \"\n",
				"       \"as many specific details as you can.\")\n",
				"    )\n",
				"  ])\n",
				"  \n",
				"  chain_summarization = prompt_summarization | model\n",
				"  \n",
				"  summarized_message = chain_summarization.invoke({\"chat_history\": stored_messages})\n",
				"  \n",
				"  chat_history.clear()\n",
				"  chat_history.add_message(summarized_message)\n",
				"  \n",
				"  return True\n",
				"\n",
				"chain_memorizable_with_summarization = (\n",
				"  runnables.RunnablePassthrough.assign(summarized_message=summarize_message)\n",
				"  | chain_memorizable\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = [\n",
				"  \"What is 1 + 1\",\n",
				"  \"Add one more\",\n",
				"  \"Add one more\",\n",
				"  \"Add one more\",\n",
				"  \"Add one more\",\n",
				"]\n",
				"\n",
				"for input in inputs:\n",
				"  response = chain_memorizable_with_summarization.invoke(\n",
				"    {\"input\": input},\n",
				"    {\"configurable\": {\"session_id\": \"unused\"}},\n",
				"  )\n",
				"\n",
				"  print(response.content) "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Retrieval using Query transformation\n",
				"\n",
				"Retrieval enhances chatbot responses with external data."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Use document loader to extract text from documents.\n",
				"loader = document_loaders.WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
				"document = loader.load()\n",
				"\n",
				"# Split text into smaller chunks for LLM's context window and store in vector database.\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=500, chunk_overlap=0,\n",
				")\n",
				"documents = text_splitter.split_documents(document)\n",
				"\n",
				"# Embed and store chunks in a vector database.\n",
				"vectorstore = vectorstores.chroma.Chroma.from_documents(\n",
				"  documents=documents, embedding=text_embedding_models.OpenAIEmbeddings(),\n",
				")\n",
				"# Create a retriever from the initialized vectorstore.\n",
				"retriever = vectorstore.as_retriever(k=4)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Chatbots must handle follow-up questions from users.\n",
				"# The retriever only pulls documents most similar to the query because it has no \n",
				"# innate concept of state. \n",
				"# Transform the query into a standalone query without any external references.\n",
				"prompt_query_transform = prompts.ChatPromptTemplate.from_messages([\n",
				"  prompts.MessagesPlaceholder(variable_name=\"messages\"),\n",
				"  (\n",
				"    \"user\",\n",
				"    (\"Given the above conversation, generate a search query to look up in order \"\n",
				"     \"to get information relevant to the conversation. Only response to the query, \"\n",
				"     \"nothing else.\")\n",
				"  )\n",
				"])\n",
				"\n",
				"# Transformed query pulls up context documents\n",
				"chain_query_transformation = runnables.RunnableBranch(\n",
				"  (\n",
				"    lambda x: len(x.get(\"messages\", [])) == 1,\n",
				"    # If only one message, then we just pass that message's content to retriever\n",
				"    (lambda x: x[\"messages\"][-1].content) | retriever,\n",
				"  ),\n",
				"  # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
				"  prompt_query_transform | model | output_parsers.StrOutputParser() | retriever,\n",
				").with_config(run_name=\"chat_retriever_chain\")\n",
				"\n",
				"# Use query transformation chain to improve retrieval chain for followup questions.\n",
				"SYSTEM_TEMPLATE = \"\"\"\\\n",
				"Answer the user's questions based on the below context. \n",
				"If the context doesn't contain any relevant information to the question, don't \\\n",
				"make something up and just say \"I don't know\":\n",
				"\n",
				"<context>\n",
				"{context}\n",
				"</context>\n",
				"\"\"\"\n",
				"prompt_qa = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\"system\", SYSTEM_TEMPLATE),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"messages\"),\n",
				"])\n",
				"\n",
				"chain_documents = chains.create_stuff_documents_chain(model, prompt_qa)\n",
				"\n",
				"chain_conversational_retrieval = runnables.RunnablePassthrough.assign(\n",
				"  context=chain_query_transformation,\n",
				").assign(\n",
				"  answer=chain_documents,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"chain_conversational_retrieval.invoke({\n",
				"  \"messages\": [\n",
				"    prompts.HumanMessage(\n",
				"        content=\"Can LangSmith help test my LLM applications?\"\n",
				"    ),\n",
				"    prompts.AIMessage(\n",
				"        content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
				"    ),\n",
				"    prompts.HumanMessage(content=\"Tell me more!\"),\n",
				"  ]\n",
				"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Tool usage\n",
				"\n",
				"Create conversational agents (chatbots) to interact with systems and APIs using tools.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 58,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create an agent that can chat with users and search for information.\n",
				"\n",
				"# Initialize Tavily and an OpenAI chat model capable of tool calling.\n",
				"tool_search_tavily = agent_tools.TavilySearchResults(max_results=1)\n",
				"tools = [tool_search_tavily]\n",
				"\n",
				"model = chat_models.chat_openai\n",
				"\n",
				"# Make agent conversational by choosing a prompt with a placeholder for storing \n",
				"# chat history.\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    (\"You are a helpful assistant. You may not need to use tools for every \" \n",
				"     \"quer - the user may just want to chat!\")\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"chat_history\"),\n",
				"  (\n",
				"    \"human\",\n",
				"    \"{input}\",\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
				"])\n",
				"\n",
				"# Assemble the agent.\n",
				"agent = agents.create_openai_tools_agent(\n",
				"  llm=model, tools=tools, prompt=prompt,\n",
				") \n",
				"\n",
				"agent_executor = agents.AgentExecutor(\n",
				"  agent=agent, tools=tools, verbose=True,\n",
				")\n",
				"\n",
				"# Wrap the agent executor in a RunnableWithMessageHistory class to manage \n",
				"# history messages. \n",
				"# Set the output_messages_key property when initializing the wrapper for agent\n",
				"# executor with multiple outputs.\n",
				"chat_history = memories.ChatMessageHistory()\n",
				"agent_executor_conversable = runnables.RunnableWithMessageHistory(\n",
				"  agent_executor,\n",
				"  lambda session_id: chat_history,\n",
				"  input_messages_key=\"input\",\n",
				"  output_messages_key=\"output\",\n",
				"  history_messages_key=\"chat_history\",\n",
				")\n",
				"config_agent_executor = {\n",
				"  \"configurable\": {\"session_id\": \"unused\"},\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 60,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
						"\u001b[32;1m\u001b[1;3mHello Nemo! How can I assist you today?\u001b[0m\n",
						"\n",
						"\u001b[1m> Finished chain.\u001b[0m\n",
						"\n",
						"\n",
						"\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
						"\u001b[32;1m\u001b[1;3mYour name is Nemo!\u001b[0m\n",
						"\n",
						"\u001b[1m> Finished chain.\u001b[0m\n"
					]
				}
			],
			"source": [
				"queries = [\n",
				"  \"I'm Nemo!\",\n",
				"  \"What is my name?\",\n",
				"]\n",
				"\n",
				"for query in queries:\n",
				"  agent_executor_conversable.invoke({\"input\": query}, config_agent_executor)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.9.18"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
