{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
					]
				}
			],
			"source": [
				"import add_packages\n",
				"import os\n",
				"from pprint import pprint\n",
				"\n",
				"from my_configs import constants\n",
				"\n",
				"from toolkit.langchain import (\n",
				"\ttools, document_loaders, text_splitters, text_embedding_models, stores,\n",
				"\tprompts, utils, output_parsers, agents, documents, runnables,\n",
				"\thistories, models\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Quickstart\n",
				"\n",
				"Build an agent with two tools: one for online searches and one for specific data retrieval from an index."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Define tools"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create the tools needed: Tavily for online search and a retriever for local index.\n",
				"tool_tavily_search = tools.TavilySearchResults()\n",
				"\n",
				"# Create a retriever over data. \n",
				"loader = document_loaders.WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
				"document = loader.load()\n",
				"documents = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=1000, chunk_overlap=200,\n",
				").split_documents(document)\n",
				"\n",
				"embeddings = text_embedding_models.OpenAIEmbeddings()\n",
				"vectorstore = stores.chroma.Chroma.from_documents(documents, embeddings)\n",
				"retriever = vectorstore.as_retriever()\n",
				"tool_retriever = stores.create_retriever_tool(\n",
				"  retriever=retriever,\n",
				"  name=\"langsmith_search\",\n",
				"  description=\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
				")\n",
				"\n",
				"# List of tools will use downstream.\n",
				"my_tools = [\n",
				"  tool_tavily_search, \n",
				"  tool_retriever,\n",
				"]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Create agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# Choose LLM guiding agent.\n",
				"llm = models.chat_openai\n",
				"\n",
				"# Choose the prompt to guide the agent.\n",
				"prompt = prompts.hub.pull(\"hwchase17/openai-functions-agent\")\n",
				"\n",
				"# Initialize the agent with the LLM, prompt, and tools. \n",
				"# The agent takes in input and decides on actions. \n",
				"# AgentExecutor execute actions for Agent\n",
				"agent = agents.create_openai_functions_agent(llm=llm, tools=my_tools, prompt=prompt)\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=my_tools, verbose=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Run agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# Run agent on stateless queries.\n",
				"agent_executor.invoke({\"input\": \"hi\"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Adding in memory\n",
				"\n",
				"This agent is stateless, does not remember previous interactions. To give it memory, pass in previous chat_history. It needs to be called chat_history because of the prompt used. If a different prompt is used, the variable name could be changed.\n",
				"\n",
				"Keep track of messages automatically by wrapping in a RunnableWithMessageHistory. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Chat history is stored in memory using a global Python dictionary.\n",
				"store = {}\n",
				"\n",
				"def get_session_history(\n",
				"  user_id: str, conversation_id: str\n",
				") -> histories.BaseChatMessageHistory:\n",
				"  \"\"\"\n",
				"  Callable references a dict to return an instance of ChatMessageHistory. \n",
				"  \n",
				"  The arguments can be specified by passing a configuration to the \n",
				"  RunnableWithMessageHistory at runtime. \n",
				"  \n",
				"  The configuration parameters for tracking message histories can be customized \n",
				"  by passing a list of ConfigurableFieldSpec objects to the \n",
				"  history_factory_config parameter. \n",
				"  \n",
				"  Two parameters used are user_id and conversation_id.\n",
				"  \"\"\"\n",
				"  if (user_id, conversation_id) not in store:\n",
				"    store[(user_id, conversation_id)] = histories.ChatMessageHistory()\n",
				"  return store[(user_id, conversation_id)]\n",
				"\n",
				"agent_with_memory = runnables.RunnableWithMessageHistory(\n",
				"  agent_executor,\n",
				"  get_session_history,\n",
				"  input_messages_key=\"input\",  # latest input message\n",
				"  history_messages_key=\"history\",  # key to add historical messages to\n",
				"  history_factory_config=[\n",
				"    runnables.ConfigurableFieldSpec(\n",
				"      id=\"user_id\", annotation=str, name=\"User ID\", default=\"\",\n",
				"      description=\"Unique identifier for the user.\", is_shared=True,\n",
				"    ),\n",
				"    runnables.ConfigurableFieldSpec(\n",
				"      id=\"conversation_id\", annotation=str, name=\"Conversation ID\", default=\"\", \n",
				"      description=\"Unique identifier for the conversation.\", is_shared=True,\n",
				"    ),\n",
				"  ]\n",
				")\n",
				"\n",
				"print(agent_with_memory.invoke(\n",
				"    {\"input\": \"Hi, I'm Bob\"},\n",
				"    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
				"))\n",
				"\n",
				"print(agent_with_memory.invoke(\n",
				"    {\"input\": \"What is my name?\"},\n",
				"    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
				"))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"message_history = histories.ChatMessageHistory()\n",
				"\n",
				"agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
				"    agent_executor,\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    lambda session_id: message_history,\n",
				"    input_messages_key=\"input\",\n",
				"    history_messages_key=\"chat_history\",\n",
				")\n",
				"\n",
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": \"hi! I'm bob\"},\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")\n",
				"\n",
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": \"what's my name?\"},\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Agent Types\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Tool Calling"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"\n",
				"llm = models.chat_openai\n",
				"my_tools = [\n",
				"\ttools.TavilySearchResults(max_results=3)\n",
				"]\n",
				"\n",
				"prompt = prompts.create_prompt_tool_calling_agent()\n",
				"\n",
				"agent = agents.MyStatelessAgent(\n",
				"\tllm=llm, tools=my_tools, prompt=prompt, agent_type='tool_calling',\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = [\n",
				"\t\"hi! my name is bob\",\n",
				"\t\"what's my name? Don't use tools to look this up unless you NEED to\",\n",
				"\t\"tell me a super long story about an apple\",\n",
				"\t\"What is LangChain\",\n",
				"\t\"What I have just ask you?\"\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"input_message = questions[1]\n",
				"result = await agent.stream_agent(input_message)\n",
				"pprint(result)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"input_message = questions[1]\n",
				"result = agent.invoke_agent(input_message)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### OpenAI tools\n",
				"\n",
				"OpenAI models detect function calls and provide input for API calls. Model outputs JSON object with function arguments for more reliable and useful function calls.\n",
				"\n",
				"OpenAI termed capability to invoke single function as functions, capability to invoke one or more functions as tools.\n",
				"\n",
				"Using tools allows the model to request multiple functions to be called when needed."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize Tools\n",
				"tool_tavily_search = tools.TavilySearchResults(max_results=1)\n",
				"my_tools = [\n",
				"  tool_tavily_search\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/openai-tools-agent\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = models.chat_openai\n",
				"\n",
				"# Construct the OpenAI Tools agent\n",
				"agent = agents.create_openai_tools_agent(llm, my_tools, prompt)\n",
				"\n",
				"# Run Agent\n",
				"# Create an agent executor by passing in the agent and tools\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=my_tools, verbose=True)\n",
				"agent_executor.invoke({\"input\": \"What is LangcChain?\"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### ReAct\n",
				"\n",
				"Using an agent to implement the [ReAct](https://react-lm.github.io/) logic."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize tools\n",
				"tool_tavily_search = tools.TavilySearchResults(max_results=1)\n",
				"my_tools = [\n",
				"  tool_tavily_search,\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/react\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = models.chat_openai\n",
				"\n",
				"# Construct the ReAct agent\n",
				"agent = agents.create_react_agent(llm=llm, tools=my_tools, prompt=prompt)\n",
				"\n",
				"# Create an agent executor by passing in the agent and tools\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=my_tools, verbose=True)\n",
				"\n",
				"# Run Agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_executor.invoke({\"input\": \"What is LangChain?\"})"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Using with chat history\n",
				"prompt = prompts.hub.pull(\"hwchase17/react-chat\")\n",
				"\n",
				"# Construct the ReAct agent\n",
				"agent = agents.create_react_agent(llm=llm, tools=my_tools, prompt=prompt)\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=my_tools, verbose=True)\n",
				"\n",
				"message_history = histories.ChatMessageHistory()\n",
				"\n",
				"agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
				"    agent_executor,\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    lambda session_id: message_history,\n",
				"    input_messages_key=\"input\",\n",
				"    history_messages_key=\"chat_history\",\n",
				")\n",
				"\n",
				"questions = [\n",
				"  \"hi! I'm bob\",\n",
				"  \"what's my name?\",\n",
				"  \"Is there any actor with the same name as me?\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": questions[2]},\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Self-ask with search"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize Tools\n",
				"tool_tavily_answer = tools.TavilyAnswer(max_results=1, name=\"Intermediate Answer\")\n",
				"my_tools = [\n",
				"  tool_tavily_answer\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/self-ask-with-search\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = models.chat_openai\n",
				"\n",
				"# Construct the Self Ask With Search Agent\n",
				"agent = agents.create_self_ask_with_search_agent(llm, my_tools, prompt)\n",
				"\n",
				"# Create an agent executor by passing in the agent and tools\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=my_tools, verbose=True,\n",
				"                                      handle_parsing_errors=True)\n",
				"\n",
				"# Run Agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_executor.invoke(\n",
				"    {\"input\": \"What is the hometown of the reigning men's U.S. Open champion?\"})"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### XML Agent\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### JSON Chat Agent\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Structured chat\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [OpenAI assistants](https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants)\n",
				"\n",
				"The Assistants API enables building AI assistants in applications. Assistants have instructions and use models, my_tools, and knowledge to answer user queries. The API supports Code Interpreter, Retrieval, and Function calling tools.\n",
				"\n",
				"Interact with OpenAI Assistants using OpenAI tools or custom tools. With OpenAI tools, invoke the assistant directly for final answers. With custom tools, run the assistant and tool execution loop using the built-in AgentExecutor or write your own executor.\n",
				"\n",
				"Different ways to interact with Assistants.\n",
				"\n",
				"The OpenAIAssistantRunnable is compatible with the AgentExecutor. Pass it in as an agent directly to the executor. The AgentExecutor calls the invoked tools and uploads the tool outputs back to the Assistants API. Includes built-in LangSmith tracing.\n",
				"\n",
				"Example: Building a math tutor that can write and run code."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import Sequence, Union\n",
				"from loguru import logger\n",
				"\n",
				"my_tools = [\n",
				"  # {\"type\": \"code_interpreter\"},\n",
				"  tools.DuckDuckGoSearchRun(),\n",
				"  tools.E2BDataAnalysisTool(),\n",
				"]\n",
				"\n",
				"\n",
				"class OpenAIAssistant:\n",
				"  def __init__(\n",
				"    self,\n",
				"    name: str,\n",
				"    instructions: str,\n",
				"    tools: list[tools.BaseTool],\n",
				"    model: str,\n",
				"    assistant_id: Union[str, None] = None,\n",
				"  ) -> None:\n",
				"    self.name = name\n",
				"    self.instructions = instructions\n",
				"    self.my_tools = tools\n",
				"    self.model = model\n",
				"    self.assistant_id = assistant_id\n",
				"    self.agent = None\n",
				"    \n",
				"    self._create_assistant()\n",
				"    \n",
				"    self.agent_executor = agents.AgentExecutor(\n",
				"      agent=self.agent, tools=self.my_tools,\n",
				"    )\n",
				"\n",
				"  def _create_assistant(\n",
				"    self,\n",
				"  ):\n",
				"    if self.assistant_id is None:\n",
				"      self.agent = agents.OpenAIAssistantRunnable.create_assistant(\n",
				"        name=self.name,\n",
				"        instructions=self.instructions,\n",
				"        tools=self.my_tools,\n",
				"        model=self.model,\n",
				"        as_agent=True,\n",
				"      )\n",
				"      self.assistant_id = self.agent.assistant_id\n",
				"      logger.info(f\"Created: Assistant ID `{self.assistant_id}`\")\n",
				"      return\n",
				"\n",
				"    logger.info(f\"Found: Assistant ID `{self.assistant_id}`\")\n",
				"    self.agent = agents.OpenAIAssistantRunnable(\n",
				"      assistant_id=self.assistant_id,\n",
				"      as_agent=True,\n",
				"    )\n",
				"    return\n",
				"  \n",
				"  def execute_agent(self, input):\n",
				"    tool_map = {tool.name: tool for tool in self.my_tools}\n",
				"    response = self.agent.invoke(input)\n",
				"    while not isinstance(response, agents.AgentFinish):\n",
				"      tool_outputs = []\n",
				"      for action in response:\n",
				"        tool_output = tool_map[action.tool].invoke(action.tool_input)\n",
				"        print(action.tool, action.tool_input, tool_output, end=\"\\n\\n\")\n",
				"        tool_outputs.append(\n",
				"          {\"output\": tool_output, \"tool_call_id\": action.tool_call_id}\n",
				"        )\n",
				"      response = self.agent.invoke(\n",
				"        {\n",
				"          \"tool_outputs\": tool_outputs,\n",
				"          \"run_id\": action.run_id,\n",
				"          \"thread_id\": action.thread_id,\n",
				"        }\n",
				"      )\n",
				"\n",
				"    return response\n",
				"\n",
				"my_openai_assistant = OpenAIAssistant(\n",
				"  name=\"langchain assistant\",\n",
				"  instructions=(\"You are a personal math tutor. Write and run code to answer \"\n",
				"                \"math questions. You can also search the internet.\"),\n",
				"  tools=my_tools,\n",
				"  model=constants.MODELS[\"OPENAI\"][\"GPT-3.5-TURBO-0125\"],\n",
				"  assistant_id=\"asst_SYdC8LwpTHu0fedq102coNyo\",  # None, asst_SYdC8LwpTHu0fedq102coNyo\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# query = {\"content\": \"What is typical color of dogs?\"}\n",
				"query = {\"content\": \"husky's color\"}\n",
				"result = my_openai_assistant.agent_executor.invoke(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# How-to\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Custom agent\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create agent using OpenAI Tool Calling for reliability.\n",
				"# Create it without memory, then add memory for conversation.\n",
				"\n",
				"#* Load LLM\n",
				"# Load the language model used to control the agent.\n",
				"llm = models.chat_openai\n",
				"\n",
				"#* Define Tools\n",
				"# Function docstring is important.\n",
				"# Python function to calculate word length.\n",
				"@tools.tool\n",
				"def get_word_length(word: str) -> int:\n",
				"  \"\"\"Returns the length of a word.\"\"\"\n",
				"  return len(word)\n",
				"\n",
				"my_tools = [\n",
				"  get_word_length,\n",
				"]\n",
				"\n",
				"#* Create Prompt for OpenAI Function Calling.\n",
				"# Input variables: \n",
				"# - input: user objective, string\n",
				"# - agent_scratchpad: sequence of previous agent tool invocations and outputs (messages)\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    \"You are very powerful assistant, but don't know current events.\"\n",
				"  ),\n",
				"  (\n",
				"    \"user\",\n",
				"    \"{input}\"\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
				"])\n",
				"\n",
				"#* Bind tools to LLM\n",
				"# How agent knows tools can be used by relying on OpenAI tool calling LLMs. \n",
				"# Tools are passed in OpenAI tool format to the model by binding functions to \n",
				"# ensure they are passed each time the model is invoked.\n",
				"llm_with_tools = llm.bind_tools(my_tools)\n",
				"\n",
				"# * Create Agent & Adding memory\n",
				"# Utility functions: \n",
				"# - Component for formatting intermediate steps (agent action, tool output\n",
				"# pairs) to input messages sent to model\n",
				"# - Component for converting output message into agent action/agent finish.\n",
				"agent = (\n",
				"  {\n",
				"    \"input\": lambda x: x[\"input\"],\n",
				"    \"agent_scratchpad\": lambda x: agents.format_to_openai_tool_messages(\n",
				"      x[\"intermediate_steps\"]\n",
				"    ),\n",
				"  }\n",
				"  | prompt \n",
				"  | llm_with_tools\n",
				"  | agents.OpenAIToolsAgentOutputParser()\n",
				")\n",
				"\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=my_tools, verbose=True)\n",
				"\n",
				"message_history = histories.ChatMessageHistory()\n",
				"agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
				"  agent_executor,\n",
				"  lambda session_id: message_history,\n",
				"  input_messages_key=\"input\",\n",
				"  history_messages_key=\"chat_history\",\n",
				")\n",
				"\n",
				"questions = [\n",
				"  \"Hello\",\n",
				"  \"My name is Bob\",\n",
				"  \"What is my name?\",\n",
				"  \"What is the length of word bob?\"\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": questions[3]},\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Returning Structured Output\n",
				"\n",
				"Agent return a structured output instead of a single string.\n",
				"\n",
				"Example, agent doing question-answering over sources. Output should include answer and list of sources used."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import List\n",
				"from langchain_core.pydantic_v1 import BaseModel, Field\n",
				"import json\n",
				"\n",
				"#* Retriever\n",
				"\n",
				"# Create a retriever over mock data.\n",
				"loader = document_loaders.TextLoader(\"../data/state_of_the_union.txt\")\n",
				"document = loader.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=1000, chunk_overlap=0,\n",
				")\n",
				"docs = text_splitter.split_documents(document)\n",
				"\n",
				"# add in the fake source information\n",
				"# Add a “page_chunk” tag to the metadata of each document.\n",
				"for i, doc in enumerate(docs):\n",
				"  doc.metadata[\"page_chunk\"] = i\n",
				"  \n",
				"vectorstore = stores.chroma.Chroma.from_documents(\n",
				"  docs, text_embedding_models.OpenAIEmbeddings(), collection_name=\"state-of-union\"\n",
				")\n",
				"retriever = vectorstore.as_retriever()\n",
				"\n",
				"#* Tools\n",
				"# Create tools for the agent, specifically one tool to wrap the retriever.\n",
				"retriever_tool = stores.create_retriever_tool(\n",
				"  retriever=retriever,\n",
				"  name=\"state-of-union-retriever\",\n",
				"  description=\"Create tools for the agent, specifically one tool to wrap the retriever.\",\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"_.\n",
				"\n",
				"Create custom parsing logic by passing the Response schema to the LLM via functions parameter, similar to passing tools for the agent to use.\n",
				"\n",
				"When Response function called by LLM, use as signal to return to user. \n",
				"When any other function called by LLM, treat as tool invocation.\n",
				"\n",
				"Parsing logic:\n",
				"- If no function is called, assume response to user is AgentFinish\n",
				"- If Response function is called, respond to user with inputs (structured output) \n",
				"and return AgentFinish\n",
				"- If any other function is called, treat as tool invocation and return AgentActionMessageLog\n",
				"\n",
				"Using AgentActionMessageLog allows to attach a log of messages for future use in passing back to the agent prompt.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"#* Response schema\n",
				"# Two fields: answer and list of sources.\n",
				"class Response(BaseModel):\n",
				"  \"\"\"Final response to the question being asked\"\"\"\n",
				"  \n",
				"  answer: str = Field(\n",
				"    description=\"The final response to the user\"\n",
				"  )\n",
				"  sources: List[int] = Field(\n",
				"      description=(\"List of page chunks that contain answer to the question. \"\n",
				"                   \"Only include a page chunk if it contains relevant information\")\n",
				"  )\n",
				"  \n",
				"#* Custom parsing logic\n",
				"def parse(output):\n",
				"  # If no function was invoked, return to user\n",
				"  if \"function_call\" not in output.additional_kwargs:\n",
				"    return agents.AgentFinish(\n",
				"      return_values={\"output\": output.content}, log=output.content\n",
				"    )\n",
				"  \n",
				"  # Parse out the function call\n",
				"  function_call = output.additional_kwargs[\"function_call\"]\n",
				"  name = function_call[\"name\"]\n",
				"  inputs = json.loads((function_call[\"arguments\"]))\n",
				"  \n",
				"  # If the Response function was invoked, return to the user with the function inputs\n",
				"  if name == \"Response\":\n",
				"    return agents.AgentFinish(return_values=inputs, log=str(function_call)) \n",
				"  # Return an agent action\n",
				"  else:\n",
				"    return agents.AgentActionMessageLog(\n",
				"      tool=name, tool_input=inputs, log=\"\", message_log=[output]\n",
				"    )\n",
				"\n",
				"#* Create Agent\n",
				"# prompt: placeholders for user's question and agent_scratchpad (intermediate steps)\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\"system\", \"You are a helpful assistant\"),\n",
				"  (\"user\", \"{input}\"),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
				"])\n",
				"\n",
				"# tools: attach tools and Response format to LLM as functions\n",
				"llm = models.chat_openai\n",
				"llm_with_tools = llm.bind_functions([retriever_tool, Response])\n",
				"\n",
				"agent = (\n",
				"  {\n",
				"    \"input\": lambda x: x[\"input\"],\n",
				"    # format agent_scratchpad from intermediate steps (AIMessages, FunctionMessages)\n",
				"    \"agent_scratchpad\": lambda x: agents.format_to_openai_function_messages(\n",
				"      x[\"intermediate_steps\"]\n",
				"    )\n",
				"  }\n",
				"  | prompt\n",
				"  | llm_with_tools\n",
				"  # custom output parser: parse LLM response\n",
				"  | parse\n",
				")\n",
				"\n",
				"# AgentExecutor: run agent-tool loop\n",
				"agent_executor = agents.AgentExecutor(\n",
				"  tools=[retriever_tool], agent=agent, verbose=True,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#* Run agent\n",
				"# It responds with a dictionary answer and sources keys\n",
				"agent_executor.invoke(\n",
				"  {\"input\": \"what did the president say about ketanji brown jackson\"},\n",
				"  return_only_outputs=True,\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Handle parsing errors, Access intermediate steps\n",
				"\n",
				"Occasionally LLM cannot determine step to take because outputs are not correctly formatted for output parser. Default agent errors easily control functionality with handle_parsing_errors.\n",
				"\n",
				"Include intermediary steps as a list of (action, observation) pairs in the return value to enhance agent insight\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"wikipedia_tool = tools.wikipedia\n",
				"my_tools = [\n",
				"  wikipedia_tool,\n",
				"]\n",
				"\n",
				"prompt = prompts.hub.pull(\"hwchase17/react\")\n",
				"llm = models.chat_openai\n",
				"agent = agents.create_react_agent(llm, my_tools, prompt)\n",
				"agent_executor = agents.AgentExecutor(\n",
				"  agent=agent, tools=my_tools, verbose=True, handle_parsing_errors=True,\n",
				"  return_intermediate_steps=True,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"#* Error\n",
				"# The agent will error due to failing to output an Action string caused by a\n",
				"# malicious input.\n",
				"agent_executor.invoke(\n",
				"  {\"input\": \"What is Leo DiCaprio's middle name?\"}\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Cap the max number of iterations\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Timeouts for agents"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from langchain import hub\n",
				"from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
				"from langchain.tools import tool\n",
				"from langchain_core.callbacks import Callbacks\n",
				"from langchain_core.prompts import ChatPromptTemplate\n",
				"from langchain_openai import ChatOpenAI\n",
				"import random\n",
				"import pprint\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = ChatOpenAI(temperature=0, streaming=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
				"from uuid import UUID\n",
				"\n",
				"from langchain_core.callbacks.base import AsyncCallbackHandler\n",
				"from langchain_core.messages import BaseMessage\n",
				"from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
				"\n",
				"# Get the prompt to use - you can modify this!\n",
				"prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
				"# print(prompt.messages) -- to see the prompt\n",
				"my_tools = [\n",
				"\ttools.TavilySearchResults(max_results=3)\n",
				"]\n",
				"agent = create_openai_tools_agent(\n",
				"    model.with_config({\"tags\": [\"agent_llm\"]}), my_tools, prompt\n",
				")\n",
				"agent_executor = AgentExecutor(agent=agent, tools=my_tools).with_config(\n",
				"    {\"run_name\": \"Agent\"}\n",
				")\n",
				"\n",
				"# Here is a custom handler that will print the tokens to stdout.\n",
				"# Instead of printing to stdout you can send the data elsewhere; e.g., to a streaming API response\n",
				"class TokenByTokenHandler(AsyncCallbackHandler):\n",
				"\tdef __init__(self, tags_of_interest: List[str]) -> None:\n",
				"\t\t\"\"\"A custom call back handler.\n",
				"\n",
				"\t\tArgs:\n",
				"\t\t\t\ttags_of_interest: Only LLM tokens from models with these tags will be\n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\tprinted.\n",
				"\t\t\"\"\"\n",
				"\t\tself.tags_of_interest = tags_of_interest\n",
				"\n",
				"\tasync def on_chain_start(\n",
				"\t\tself,\n",
				"\t\tserialized: Dict[str, Any],\n",
				"\t\tinputs: Dict[str, Any],\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> None:\n",
				"\t\t\"\"\"Run when chain starts running.\"\"\"\n",
				"\t\t# print(\"on chain start: \")\n",
				"\t\t# print(inputs)\n",
				"\n",
				"\tasync def on_chain_end(\n",
				"\t\tself,\n",
				"\t\toutputs: Dict[str, Any],\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> None:\n",
				"\t\t\"\"\"Run when chain ends running.\"\"\"\n",
				"\t\t# print(\"On chain end\")\n",
				"\t\t# print(outputs)\n",
				"\n",
				"\tasync def on_chat_model_start(\n",
				"\t\tself,\n",
				"\t\tserialized: Dict[str, Any],\n",
				"\t\tmessages: List[List[BaseMessage]],\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> Any:\n",
				"\t\t\"\"\"Run when a chat model starts running.\"\"\"\n",
				"\t\toverlap_tags = self.get_overlap_tags(tags)\n",
				"\n",
				"\t\t# if overlap_tags:\n",
				"\t\t# \tprint(\",\".join(overlap_tags), end=\": \", flush=True)\n",
				"\n",
				"\tdef on_tool_start(\n",
				"\t\tself,\n",
				"\t\tserialized: Dict[str, Any],\n",
				"\t\tinput_str: str,\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\tinputs: Optional[Dict[str, Any]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> Any:\n",
				"\t\t\"\"\"Run when tool starts running.\"\"\"\n",
				"\t\tprint(f\"Tool: {serialized}\")\n",
				"\n",
				"\tdef on_tool_end(\n",
				"\t\tself,\n",
				"\t\toutput: Any,\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> Any:\n",
				"\t\t\"\"\"Run when tool ends running.\"\"\"\n",
				"\t\tprint(f\"Result: {str(output)}\")\n",
				"\n",
				"\tasync def on_llm_end(\n",
				"\t\tself,\n",
				"\t\tresponse: LLMResult,\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> None:\n",
				"\t\t\"\"\"Run when LLM ends running.\"\"\"\n",
				"\t\toverlap_tags = self.get_overlap_tags(tags)\n",
				"\n",
				"\t\tif overlap_tags:\n",
				"\t\t\t# Who can argue with beauty?\n",
				"\t\t\tprint()\n",
				"\t\t\t# print()\n",
				"\n",
				"\tdef get_overlap_tags(self, tags: Optional[List[str]]) -> List[str]:\n",
				"\t\t\"\"\"Check for overlap with filtered tags.\"\"\"\n",
				"\t\tif not tags:\n",
				"\t\t\treturn []\n",
				"\t\treturn sorted(set(tags or []) & set(self.tags_of_interest or []))\n",
				"\n",
				"\tasync def on_llm_new_token(\n",
				"\t\tself,\n",
				"\t\ttoken: str,\n",
				"\t\t*,\n",
				"\t\tchunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\t**kwargs: Any,\n",
				") -> None:\n",
				"\t\t\"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
				"\t\toverlap_tags = self.get_overlap_tags(tags)\n",
				"\n",
				"\t\tif token and overlap_tags:\n",
				"\t\t\tprint(token, end=\"\", flush=True)\n",
				"\n",
				"\n",
				"handler = TokenByTokenHandler(tags_of_interest=[\n",
				"  \"tool_llm\", \"agent_llm\"\n",
				"])\n",
				"\n",
				"result = await agent_executor.ainvoke(\n",
				"\t{\"input\": \"tell me a super long story about a dog\"},\n",
				"\t{\"callbacks\": [handler]},\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Structured Tools\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Running Agent as an Iterator\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tools"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Toolkits\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Defining Custom Tools\n",
				"\n",
				"Construct agent with list of Tools:\n",
				"\n",
				"- Tool components:\n",
				"    - name (str): required, must be unique within set of tools\n",
				"    - description (str): optional but recommended for tool use determination\n",
				"    - args_schema (Pydantic BaseModel): optional but recommended for more information or validation.\n",
				"\n",
				"There are multiple ways to define a tool.\n",
				"\n",
				"Many agents only work with functions that require single inputs\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\"\"\"\n",
				"Two functions: \n",
				"- A made up search function that always returns the string “LangChain” , \n",
				"requires one input\n",
				"- A multiplier function that will multiply two numbers by eachother, requires \n",
				"multiple inputs\n",
				"\"\"\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### @tool decorator\n",
				"\n",
				"\n",
				"\n",
				"Defines a custom tool. \n",
				"\n",
				"The function name is the tool name, can be overridden by passing a string as the first argument. Pass tool name and JSON args into the tool decorator for customization.\n",
				"\n",
				"The function’s docstring is used as the tool’s description - a docstring MUST be provided."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SearchInput(tools.BaseModel):\n",
				"  query: str = tools.Field(description=\"should be a search query\")\n",
				"\n",
				"@tools.tool(\"search_tool\", args_schema=SearchInput, return_direct=True)\n",
				"def search(query: str) -> str:\n",
				"  \"\"\"Look up things online.\"\"\"\n",
				"  return \"LangChain\"\n",
				"\n",
				"@tools.tool\n",
				"def multiply(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Subclass BaseTool\n",
				"\n",
				"Define a custom tool by subclassing the BaseTool class for maximal control over the tool definition.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SearchInput(tools.BaseModel):\n",
				"  query: str = tools.Field(description=\"should be a search query\")\n",
				"  \n",
				"class CustomSearchTool(tools.BaseTool):\n",
				"  name = \"custom_search\"\n",
				"  description = \"useful for when you need to answer questions about current events\"\n",
				"  args_schema: tools.typing.Type[tools.BaseModel] = SearchInput\n",
				"  \n",
				"  def _run(\n",
				"    self, query: str, \n",
				"    run_manager: tools.typing.Optional[tools.CallbackManagerForToolRun] = None,\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool.\"\"\"\n",
				"    return \"LangChain\"\n",
				"  \n",
				"  async def _arun(\n",
				"    self, query: str,\n",
				"    run_manager: tools.typing.Optional[tools.AsyncCallbackManagerForToolRun] = None,\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool asynchronously.\"\"\"\n",
				"    raise NotImplementedError(\"custom_search does not support async\")\n",
				"  \n",
				"\n",
				"tool_search = CustomSearchTool()\n",
				"\n",
				"print(tool_search.name)\n",
				"print(tool_search.description)\n",
				"print(tool_search.args)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class CalculatorInput(tools.BaseModel):\n",
				"  a: int = tools.Field(description=\"first number\")\n",
				"  b: int = tools.Field(description=\"second number\")\n",
				"  \n",
				"class CustomCalculatorTool(tools.BaseTool):\n",
				"  name = \"Calculator\"\n",
				"  description = \"useful for when you need to answer questions about math\"\n",
				"  args_schema: tools.typing.Type[tools.BaseModel] = CalculatorInput\n",
				"  return_direct: bool = True\n",
				"  \n",
				"  def _run(\n",
				"    self, a: int, b: int,\n",
				"    run_manager: tools.typing.Optional[tools.CallbackManagerForToolRun] = None\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool.\"\"\"\n",
				"    return a * b\n",
				"\n",
				"  async def _arun(\n",
				"    self, a: int, b: int,\n",
				"    run_manager: tools.typing.Optional[tools.AsyncCallbackManagerForToolRun] = None\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool asynchronously.\"\"\"\n",
				"    return NotImplementedError(\"Calculator does not support async.\")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### StructuredTool dataclass\n",
				"\n",
				"Use a StructuredTool dataclass for a mix of convenience and functionality.\n",
				"\n",
				"Can Define a custom args_schema to provide more information about inputs."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def search_function(query: str):\n",
				"  return \"LangChain\"\n",
				"\n",
				"tool_search = tools.StructuredTool.from_function(\n",
				"  func=search_function,\n",
				"  name=\"Search\",\n",
				"  description=\"useful for when you need to answer questions about current events\",\n",
				"  # coroutine=\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SchemaCalculator(tools.BaseModel):\n",
				"  a: int = tools.Field(description=\"first number\")\n",
				"  b: int = tools.Field(description=\"second number\")\n",
				"  \n",
				"def fn_calculator(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b\n",
				"\n",
				"tool_calculator = tools.StructuredTool.from_function(\n",
				"  func=fn_calculator,\n",
				"  name=\"Calculator\",\n",
				"  description=\"multiply numbers\",\n",
				"  args_schema=SchemaCalculator,\n",
				"  return_direct=True,\n",
				"  \n",
				"  # coroutine=\n",
				")\n",
				"\n",
				"print(tool_calculator.name)\n",
				"print(tool_calculator.description)\n",
				"print(tool_calculator.args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Handling Tool Errors\n",
				"\n",
				"When tool encounters error and exception is not caught, agent will stop executing. To continue execution, raise ToolException and set handle_tool_error accordingly.\n",
				"\n",
				"When ToolException is thrown, the agent will handle the exception based on the handle_tool_error variable of the tool, and the processing result will be returned to the agent as observation, and printed in red.\n",
				"\n",
				"Set handle_tool_error to True, a unified string value, or a function. If set as a function, the function should take a ToolException as a parameter and return a str value.\n",
				"\n",
				"Only raising a ToolException won’t be effective. Set the handle_tool_error of the tool because its default value is False.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SchemaCalculator(tools.BaseModel):\n",
				"  a: int = tools.Field(description=\"first number\")\n",
				"  b: int = tools.Field(description=\"second number\")\n",
				"  \n",
				"def fn_calculator(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b\n",
				"\n",
				"tool_calculator = tools.StructuredTool.from_function(\n",
				"  func=fn_calculator,\n",
				"  name=\"Calculator\",\n",
				"  description=\"multiply numbers\",\n",
				"  args_schema=SchemaCalculator,\n",
				"  return_direct=True,\n",
				"  handle_tool_error=tools._handle_error\n",
				"  # coroutine=\n",
				")\n",
				"\n",
				"print(tool_calculator.name)\n",
				"print(tool_calculator.description)\n",
				"print(tool_calculator.args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### My way"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SchemaCalculator(tools.BaseModel):\n",
				"  a: int = tools.Field(description=\"first number\")\n",
				"  b: int = tools.Field(description=\"second number\")\n",
				"  \n",
				"def fn_calculator(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b\n",
				"\n",
				"tool_calculator = tools.StructuredTool.from_function(\n",
				"  func=fn_calculator,\n",
				"  name=\"Calculator\",\n",
				"  description=\"multiply numbers\",\n",
				"  args_schema=SchemaCalculator,\n",
				"  return_direct=True,\n",
				"  handle_tool_error=tools._handle_error\n",
				"  # coroutine=\n",
				")\n",
				"\n",
				"print(tool_calculator.name)\n",
				"print(tool_calculator.description)\n",
				"print(tool_calculator.args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Tools as OpenAI Functions\n",
				"\n",
				"LangChain tools are used as OpenAI functions\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = models.chat_openai\n",
				"\n",
				"tool_move_file = tools.MoveFileTool()\n",
				"my_tools = [\n",
				"  tool_move_file,\n",
				"]\n",
				"\n",
				"fns = [tools.convert_to_openai_function(t) for t in my_tools]\n",
				"\n",
				"model_with_fns = model.bind_functions(fns)\n",
				"model_with_tools = model.bind_tools(my_tools)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model_with_tools.invoke([\n",
				"  prompts.HumanMessage(content=\"move file foo to bar\"),\n",
				"])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# Initialize tools\n",
				"tool_tavily_search = tools.TavilySearchResults(max_results=1)\n",
				"my_tools = [\n",
				"  tool_tavily_search,\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/react-chat\")\n",
				"# prompt = prompts.hub.pull(\"hwchase17/react\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = models.chat_openai\n",
				"\n",
				"my_agent = agents.MyAgent(prompt=prompt, tools=my_tools, agent_type=\"react\", llm=llm)\n",
				"\n",
				"questions = [\n",
				"\t\"Hello\",\n",
				"\t\"My name is Bob\",\n",
				"\t\"what's my name?\",\n",
				"\t\"Is there any actor with the same name as me?\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"response = my_agent.invoke_agent(questions[2])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Agent Class"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = [\n",
				"\t\"Hello\",\n",
				"\t\"My name is Bob\",\n",
				"\t\"What is my name?\"\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-07-07 18:10:17.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.agents\u001b[0m:\u001b[36m_create_agent\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mAgent type: tool_calling\u001b[0m\n"
					]
				}
			],
			"source": [
				"llm = models.chat_openai\n",
				"my_tools = [\n",
				"\ttools.TavilySearchResults(max_results=3)\n",
				"]\n",
				"prompt = prompts.create_prompt_tool_calling_agent()\n",
				"\n",
				"agent = agents.MyStatelessAgent(\n",
				"\tllm=llm,\n",
				"\ttools=my_tools,\n",
				"\tprompt=prompt,\n",
				"\tagent_type='tool_calling',\n",
				"\tagent_verbose=False,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-07-07 18:11:31.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.agents\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mUser Id: user-ip\u001b[0m\n",
						"\u001b[32m2024-07-07 18:11:31.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.agents\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSession Id: default\u001b[0m\n",
						"\u001b[32m2024-07-07 18:11:31.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.agents\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mHistory Type: mongodb\u001b[0m\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"`[TOOL - CALLING]` \n",
						"Invoking: `tavily_search_results_json` with `{'query': 'Apple Inc'}`\n",
						"\n",
						"\n",
						"`[TOOL - RESULT]` [{\"url\": \"https://www.apple.com/\", \"content\": \"Discover the innovative world of Apple and shop everything iPhone, iPad, Apple Watch, Mac, and Apple TV, plus explore accessories, entertainment, ...SupportStoreMacBusinessCareers\"}, {\"url\": \"https://en.wikipedia.org/wiki/Apple_Inc.\", \"content\": \"In total, Apple occupies almost 40% of the available office space in the city.[278]\\nApple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland, called the Hollyhill campus.[279] The facility, which opened in 1980, houses 5,500 people and was Apple's first location outside of the United States.[280] Apple's international sales and distribution arms operate out of the campus in Cork.[281]\\nApple has two campuses near Austin, Texas: a 216,000-square-foot (20,100 m2) campus opened in 2014 houses 500 engineers who work on Apple silicon[282] and a 1.1-million-square-foot (100,000 m2) campus opened in 2021 where 6,000 people work in technical support, supply chain management, online store curation, and Apple Maps data management.\\n As of the end of 2021[update], this broad line of products comprises about 11% of the company's revenues.[1]\\nAt WWDC 2023, Apple introduced its new VR headset, Vision Pro, along with visionOS.[188] Apple announced that it will be partnering with Unity to bring existing 3D apps to Vision Pro using Unity's PolySpatial technology.[189][190][191][192]\\nServices\\nApple offers a broad line of services, including advertising in the App Store and Apple News app, the AppleCare+ extended warranty plan, the iCloud+ cloud-based data storage service, payment services through the Apple Card credit card and the Apple Pay processing platform, digital content services including Apple Books, Apple Fitness+, Apple Music, Apple News+, Apple TV+, and the iTunes Store.\\n According to Bloomberg, the headset may come out in 2023.[173] Further insider reports state that the device uses iris scanning for payment confirmation and signing into accounts.[174]\\nOn June 18, 2022, the Apple Store in Towson, Maryland became the first to unionize in the U.S., with the employees voting to join the International Association of Machinists and Aerospace Workers.[175]\\nOn July 7, 2022, Apple added Lockdown Mode to macOS 13 and iOS 16, as a response to the earlier Pegasus revelations; the mode increases security protections for high-risk users against targeted zero-day malware.[176]\\nApple launched a buy now, pay later service called 'Apple Pay Later' for its Apple Wallet users in March 2023. While the Apple I and early Apple II models used ordinary audio cassette tapes as storage devices, they were superseded by the introduction of a 5+1⁄4-inch floppy disk drive and interface called the Disk II in 1978.[26][27]\\nThe Apple II was chosen to be the desktop platform for the first \\\"killer application\\\" of the business world: VisiCalc, a spreadsheet program released in 1979.[26] VisiCalc created a business market for the Apple II and gave home users an additional reason to buy an Apple II: compatibility with the office,[26] but Apple II market share remained behind home computers made by competitors such as Atari, Commodore, and Tandy.[28][29]\\n The Christmas season of 1989 was the first in the company's history to have declining sales, which led to a 20% drop in Apple's stock price.[55]: 117–129  During this period, the relationship between Sculley and Gassée deteriorated, leading Sculley to effectively demote Gassée in January 1990 by appointing Michael Spindler as the chief operating officer.[56] Gassée left the company later that year.[57]\\n1990–1997: Decline and restructuring\\nThe company pivoted strategy and in October 1990 introduced three lower-cost models, the Macintosh Classic, the Macintosh LC, and the Macintosh IIsi, all of which saw significant sales due to pent-up demand.[58]\"}]\n",
						"\n",
						"Apple Inc. is a renowned technology company known for its innovative products. It offers a wide range of products including iPhone, iPad, Apple Watch, Mac, and Apple TV. Apple also provides various services such as advertising in the App Store, AppleCare+ extended warranty plan, iCloud+ cloud-based data storage service, payment services through Apple Card and Apple Pay, digital content services like Apple Books, Apple Fitness+, Apple Music, Apple News+, Apple TV+, and the iTunes Store.\n",
						"\n",
						"If you'd like to explore more about Apple Inc., you can visit their official website [here](https://www.apple.com/)."
					]
				}
			],
			"source": [
				"async for chunk in agent.astream_events_basic(\n",
				"\tinput_message=\"Tell me a about apple inc\",\n",
				"\t# input_message=questions[2],\n",
				"\tshow_tool_call=True,\n",
				"\thistory_type=\"mongodb\",\n",
				"\tuser_id=\"user-ip\",\n",
				"\tsession_id=\"default\",\n",
				"):\n",
				"\tprint(chunk, end=\"\", flush=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"await agent.invoke_agent(\n",
				"\tinput_message=\"Tell me about apple inc\",\n",
				"\thistory_type=\"mongodb\",\n",
				"\tsession_id=\"default\",\n",
				"\tuser_id=\"user-ip\", # no need for mongodb\n",
				")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.9"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
