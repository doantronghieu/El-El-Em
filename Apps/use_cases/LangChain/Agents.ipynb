{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import add_packages\n",
				"import config\n",
				"import os\n",
				"from pprint import pprint\n",
				"\n",
				"from my_configs import constants\n",
				"\n",
				"from toolkit.langchain import (\n",
				"\tagent_tools, document_loaders, text_splitters, text_embedding_models, vectorstores,\n",
				"\tchat_models, prompts, utils, output_parsers, agents, documents, runnables,\n",
				"\tllms, histories\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Quickstart\n",
				"\n",
				"Build an agent with two tools: one for online searches and one for specific data retrieval from an index."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Define tools"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create the tools needed: Tavily for online search and a retriever for local index.\n",
				"\n",
				"tool_tavily_search = agent_tools.tavily_search_results()\n",
				"\n",
				"# Create a retriever over data. \n",
				"loader = document_loaders.WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
				"document = loader.load()\n",
				"documents = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=1000, chunk_overlap=200,\n",
				").split_documents(document)\n",
				"\n",
				"embeddings = text_embedding_models.OpenAIEmbeddings()\n",
				"vectorstore = vectorstores.chroma.Chroma.from_documents(documents, embeddings)\n",
				"retriever = vectorstore.as_retriever()\n",
				"tool_retriever = vectorstores.create_retriever_tool(\n",
				"  retriever=retriever,\n",
				"  name=\"langsmith_search\",\n",
				"  description=\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
				")\n",
				"\n",
				"# List of tools will use downstream.\n",
				"tools = [\n",
				"  tool_tavily_search, \n",
				"  tool_retriever,\n",
				"]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Create agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# Choose LLM guiding agent.\n",
				"llm = chat_models.chat_openai\n",
				"\n",
				"# Choose the prompt to guide the agent.\n",
				"prompt = prompts.hub.pull(\"hwchase17/openai-functions-agent\")\n",
				"\n",
				"# Initialize the agent with the LLM, prompt, and tools. \n",
				"# The agent takes in input and decides on actions. \n",
				"# AgentExecutor execute actions for Agent\n",
				"agent = agents.create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Run agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# Run agent on stateless queries.\n",
				"agent_executor.invoke({\"input\": \"hi\"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Adding in memory\n",
				"\n",
				"This agent is stateless, does not remember previous interactions. To give it memory, pass in previous chat_history. It needs to be called chat_history because of the prompt used. If a different prompt is used, the variable name could be changed.\n",
				"\n",
				"Keep track of messages automatically by wrapping in a RunnableWithMessageHistory. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Chat history is stored in memory using a global Python dictionary.\n",
				"store = {}\n",
				"\n",
				"def get_session_history(\n",
				"  user_id: str, conversation_id: str\n",
				") -> histories.BaseChatMessageHistory:\n",
				"  \"\"\"\n",
				"  Callable references a dict to return an instance of ChatMessageHistory. \n",
				"  \n",
				"  The arguments can be specified by passing a configuration to the \n",
				"  RunnableWithMessageHistory at runtime. \n",
				"  \n",
				"  The configuration parameters for tracking message histories can be customized \n",
				"  by passing a list of ConfigurableFieldSpec objects to the \n",
				"  history_factory_config parameter. \n",
				"  \n",
				"  Two parameters used are user_id and conversation_id.\n",
				"  \"\"\"\n",
				"  if (user_id, conversation_id) not in store:\n",
				"    store[(user_id, conversation_id)] = histories.ChatMessageHistory()\n",
				"  return store[(user_id, conversation_id)]\n",
				"\n",
				"agent_with_memory = runnables.RunnableWithMessageHistory(\n",
				"  agent_executor,\n",
				"  get_session_history,\n",
				"  input_messages_key=\"input\",  # latest input message\n",
				"  history_messages_key=\"history\",  # key to add historical messages to\n",
				"  history_factory_config=[\n",
				"    runnables.ConfigurableFieldSpec(\n",
				"      id=\"user_id\", annotation=str, name=\"User ID\", default=\"\",\n",
				"      description=\"Unique identifier for the user.\", is_shared=True,\n",
				"    ),\n",
				"    runnables.ConfigurableFieldSpec(\n",
				"      id=\"conversation_id\", annotation=str, name=\"Conversation ID\", default=\"\", \n",
				"      description=\"Unique identifier for the conversation.\", is_shared=True,\n",
				"    ),\n",
				"  ]\n",
				")\n",
				"\n",
				"print(agent_with_memory.invoke(\n",
				"    {\"input\": \"Hi, I'm Bob\"},\n",
				"    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
				"))\n",
				"\n",
				"print(agent_with_memory.invoke(\n",
				"    {\"input\": \"What is my name?\"},\n",
				"    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
				"))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"message_history = histories.ChatMessageHistory()\n",
				"\n",
				"agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
				"    agent_executor,\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    lambda session_id: message_history,\n",
				"    input_messages_key=\"input\",\n",
				"    history_messages_key=\"chat_history\",\n",
				")\n",
				"\n",
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": \"hi! I'm bob\"},\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")\n",
				"\n",
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": \"what's my name?\"},\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Agent Types\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Tool Calling"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"\n",
				"llm = chat_models.chat_openai\n",
				"tools = [\n",
				"\tagent_tools.TavilySearchResults(max_results=3)\n",
				"]\n",
				"\n",
				"prompt = prompts.create_prompt_tool_calling_agent()\n",
				"\n",
				"agent = agents.MyAgent(\n",
				"\tllm=llm, tools=tools, prompt=prompt, agent_type='tool_calling',\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = [\n",
				"\t\"hi! my name is bob\",\n",
				"\t\"what's my name? Don't use tools to look this up unless you NEED to\",\n",
				"\t\"tell me a super long story about an apple\",\n",
				"\t\"What is LangChain\",\n",
				"\t\"What I have just ask you?\"\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"input_message = questions[1]\n",
				"result = await agent.stream_agent(input_message)\n",
				"pprint(result)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"input_message = questions[1]\n",
				"result = agent.invoke_agent(input_message)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### OpenAI tools\n",
				"\n",
				"OpenAI models detect function calls and provide input for API calls. Model outputs JSON object with function arguments for more reliable and useful function calls.\n",
				"\n",
				"OpenAI termed capability to invoke single function as functions, capability to invoke one or more functions as tools.\n",
				"\n",
				"Using tools allows the model to request multiple functions to be called when needed."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize Tools\n",
				"tool_tavily_search = agent_tools.TavilySearchResults(max_results=1)\n",
				"tools = [\n",
				"  tool_tavily_search\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/openai-tools-agent\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = chat_models.chat_openai\n",
				"\n",
				"# Construct the OpenAI Tools agent\n",
				"agent = agents.create_openai_tools_agent(llm, tools, prompt)\n",
				"\n",
				"# Run Agent\n",
				"# Create an agent executor by passing in the agent and tools\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
				"agent_executor.invoke({\"input\": \"What is LangcChain?\"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### ReAct\n",
				"\n",
				"Using an agent to implement the [ReAct](https://react-lm.github.io/) logic."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize tools\n",
				"tool_tavily_search = agent_tools.TavilySearchResults(max_results=1)\n",
				"tools = [\n",
				"  tool_tavily_search,\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/react\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = chat_models.chat_openai\n",
				"\n",
				"# Construct the ReAct agent\n",
				"agent = agents.create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
				"\n",
				"# Create an agent executor by passing in the agent and tools\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
				"\n",
				"# Run Agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_executor.invoke({\"input\": \"What is LangChain?\"})"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Using with chat history\n",
				"prompt = prompts.hub.pull(\"hwchase17/react-chat\")\n",
				"\n",
				"# Construct the ReAct agent\n",
				"agent = agents.create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
				"\n",
				"message_history = histories.ChatMessageHistory()\n",
				"\n",
				"agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
				"    agent_executor,\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    lambda session_id: message_history,\n",
				"    input_messages_key=\"input\",\n",
				"    history_messages_key=\"chat_history\",\n",
				")\n",
				"\n",
				"questions = [\n",
				"  \"hi! I'm bob\",\n",
				"  \"what's my name?\",\n",
				"  \"Is there any actor with the same name as me?\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": questions[2]},\n",
				"    # This is needed because in most real world scenarios, a session id is needed\n",
				"    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Self-ask with search"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize Tools\n",
				"tool_tavily_answer = agent_tools.TavilyAnswer(max_results=1, name=\"Intermediate Answer\")\n",
				"tools = [\n",
				"  tool_tavily_answer\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/self-ask-with-search\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = chat_models.chat_openai\n",
				"\n",
				"# Construct the Self Ask With Search Agent\n",
				"agent = agents.create_self_ask_with_search_agent(llm, tools, prompt)\n",
				"\n",
				"# Create an agent executor by passing in the agent and tools\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True,\n",
				"                                      handle_parsing_errors=True)\n",
				"\n",
				"# Run Agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_executor.invoke(\n",
				"    {\"input\": \"What is the hometown of the reigning men's U.S. Open champion?\"})"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### XML Agent\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### JSON Chat Agent\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Structured chat\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [OpenAI assistants](https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants)\n",
				"\n",
				"The Assistants API enables building AI assistants in applications. Assistants have instructions and use models, tools, and knowledge to answer user queries. The API supports Code Interpreter, Retrieval, and Function calling tools.\n",
				"\n",
				"Interact with OpenAI Assistants using OpenAI tools or custom tools. With OpenAI tools, invoke the assistant directly for final answers. With custom tools, run the assistant and tool execution loop using the built-in AgentExecutor or write your own executor.\n",
				"\n",
				"Different ways to interact with Assistants.\n",
				"\n",
				"The OpenAIAssistantRunnable is compatible with the AgentExecutor. Pass it in as an agent directly to the executor. The AgentExecutor calls the invoked tools and uploads the tool outputs back to the Assistants API. Includes built-in LangSmith tracing.\n",
				"\n",
				"Example: Building a math tutor that can write and run code."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import Sequence, Union\n",
				"from loguru import logger\n",
				"\n",
				"tools = [\n",
				"  # {\"type\": \"code_interpreter\"},\n",
				"  agent_tools.DuckDuckGoSearchRun(),\n",
				"  agent_tools.E2BDataAnalysisTool(),\n",
				"]\n",
				"\n",
				"\n",
				"class OpenAIAssistant:\n",
				"  def __init__(\n",
				"    self,\n",
				"    name: str,\n",
				"    instructions: str,\n",
				"    tools: list[agent_tools.BaseTool],\n",
				"    model: str,\n",
				"    assistant_id: Union[str, None] = None,\n",
				"  ) -> None:\n",
				"    self.name = name\n",
				"    self.instructions = instructions\n",
				"    self.tools = tools\n",
				"    self.model = model\n",
				"    self.assistant_id = assistant_id\n",
				"    self.agent = None\n",
				"    \n",
				"    self._create_assistant()\n",
				"    \n",
				"    self.agent_executor = agents.AgentExecutor(\n",
				"      agent=self.agent, tools=self.tools,\n",
				"    )\n",
				"\n",
				"  def _create_assistant(\n",
				"    self,\n",
				"  ):\n",
				"    if self.assistant_id is None:\n",
				"      self.agent = agents.OpenAIAssistantRunnable.create_assistant(\n",
				"        name=self.name,\n",
				"        instructions=self.instructions,\n",
				"        tools=self.tools,\n",
				"        model=self.model,\n",
				"        as_agent=True,\n",
				"      )\n",
				"      self.assistant_id = self.agent.assistant_id\n",
				"      logger.info(f\"Created: Assistant ID `{self.assistant_id}`\")\n",
				"      return\n",
				"\n",
				"    logger.info(f\"Found: Assistant ID `{self.assistant_id}`\")\n",
				"    self.agent = agents.OpenAIAssistantRunnable(\n",
				"      assistant_id=self.assistant_id,\n",
				"      as_agent=True,\n",
				"    )\n",
				"    return\n",
				"  \n",
				"  def execute_agent(self, input):\n",
				"    tool_map = {tool.name: tool for tool in self.tools}\n",
				"    response = self.agent.invoke(input)\n",
				"    while not isinstance(response, agents.AgentFinish):\n",
				"      tool_outputs = []\n",
				"      for action in response:\n",
				"        tool_output = tool_map[action.tool].invoke(action.tool_input)\n",
				"        print(action.tool, action.tool_input, tool_output, end=\"\\n\\n\")\n",
				"        tool_outputs.append(\n",
				"          {\"output\": tool_output, \"tool_call_id\": action.tool_call_id}\n",
				"        )\n",
				"      response = self.agent.invoke(\n",
				"        {\n",
				"          \"tool_outputs\": tool_outputs,\n",
				"          \"run_id\": action.run_id,\n",
				"          \"thread_id\": action.thread_id,\n",
				"        }\n",
				"      )\n",
				"\n",
				"    return response\n",
				"\n",
				"my_openai_assistant = OpenAIAssistant(\n",
				"  name=\"langchain assistant\",\n",
				"  instructions=(\"You are a personal math tutor. Write and run code to answer \"\n",
				"                \"math questions. You can also search the internet.\"),\n",
				"  tools=tools,\n",
				"  model=constants.MODELS[\"OPENAI\"][\"GPT-3.5-TURBO-0125\"],\n",
				"  assistant_id=\"asst_SYdC8LwpTHu0fedq102coNyo\",  # None, asst_SYdC8LwpTHu0fedq102coNyo\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# query = {\"content\": \"What is typical color of dogs?\"}\n",
				"query = {\"content\": \"husky's color\"}\n",
				"result = my_openai_assistant.agent_executor.invoke(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# How-to\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Custom agent\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create agent using OpenAI Tool Calling for reliability.\n",
				"# Create it without memory, then add memory for conversation.\n",
				"\n",
				"#* Load LLM\n",
				"# Load the language model used to control the agent.\n",
				"llm = chat_models.chat_openai\n",
				"\n",
				"#* Define Tools\n",
				"# Function docstring is important.\n",
				"# Python function to calculate word length.\n",
				"@agent_tools.tool\n",
				"def get_word_length(word: str) -> int:\n",
				"  \"\"\"Returns the length of a word.\"\"\"\n",
				"  return len(word)\n",
				"\n",
				"tools = [\n",
				"  get_word_length,\n",
				"]\n",
				"\n",
				"#* Create Prompt for OpenAI Function Calling.\n",
				"# Input variables: \n",
				"# - input: user objective, string\n",
				"# - agent_scratchpad: sequence of previous agent tool invocations and outputs (messages)\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\n",
				"    \"system\",\n",
				"    \"You are very powerful assistant, but don't know current events.\"\n",
				"  ),\n",
				"  (\n",
				"    \"user\",\n",
				"    \"{input}\"\n",
				"  ),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
				"])\n",
				"\n",
				"#* Bind tools to LLM\n",
				"# How agent knows tools can be used by relying on OpenAI tool calling LLMs. \n",
				"# Tools are passed in OpenAI tool format to the model by binding functions to \n",
				"# ensure they are passed each time the model is invoked.\n",
				"llm_with_tools = llm.bind_tools(tools)\n",
				"\n",
				"# * Create Agent & Adding memory\n",
				"# Utility functions: \n",
				"# - Component for formatting intermediate steps (agent action, tool output\n",
				"# pairs) to input messages sent to model\n",
				"# - Component for converting output message into agent action/agent finish.\n",
				"agent = (\n",
				"  {\n",
				"    \"input\": lambda x: x[\"input\"],\n",
				"    \"agent_scratchpad\": lambda x: agents.format_to_openai_tool_messages(\n",
				"      x[\"intermediate_steps\"]\n",
				"    ),\n",
				"  }\n",
				"  | prompt \n",
				"  | llm_with_tools\n",
				"  | agents.OpenAIToolsAgentOutputParser()\n",
				")\n",
				"\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
				"\n",
				"message_history = histories.ChatMessageHistory()\n",
				"agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
				"  agent_executor,\n",
				"  lambda session_id: message_history,\n",
				"  input_messages_key=\"input\",\n",
				"  history_messages_key=\"chat_history\",\n",
				")\n",
				"\n",
				"questions = [\n",
				"  \"Hello\",\n",
				"  \"My name is Bob\",\n",
				"  \"What is my name?\",\n",
				"  \"What is the length of word bob?\"\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"agent_with_chat_history.invoke(\n",
				"    {\"input\": questions[3]},\n",
				"    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Returning Structured Output\n",
				"\n",
				"Agent return a structured output instead of a single string.\n",
				"\n",
				"Example, agent doing question-answering over sources. Output should include answer and list of sources used."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import List\n",
				"from langchain_core.pydantic_v1 import BaseModel, Field\n",
				"import json\n",
				"\n",
				"#* Retriever\n",
				"\n",
				"# Create a retriever over mock data.\n",
				"loader = document_loaders.TextLoader(\"../data/state_of_the_union.txt\")\n",
				"document = loader.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=1000, chunk_overlap=0,\n",
				")\n",
				"docs = text_splitter.split_documents(document)\n",
				"\n",
				"# add in the fake source information\n",
				"# Add a “page_chunk” tag to the metadata of each document.\n",
				"for i, doc in enumerate(docs):\n",
				"  doc.metadata[\"page_chunk\"] = i\n",
				"  \n",
				"vectorstore = vectorstores.chroma.Chroma.from_documents(\n",
				"  docs, text_embedding_models.OpenAIEmbeddings(), collection_name=\"state-of-union\"\n",
				")\n",
				"retriever = vectorstore.as_retriever()\n",
				"\n",
				"#* Tools\n",
				"# Create tools for the agent, specifically one tool to wrap the retriever.\n",
				"retriever_tool = vectorstores.create_retriever_tool(\n",
				"  retriever=retriever,\n",
				"  name=\"state-of-union-retriever\",\n",
				"  description=\"Create tools for the agent, specifically one tool to wrap the retriever.\",\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"_.\n",
				"\n",
				"Create custom parsing logic by passing the Response schema to the LLM via functions parameter, similar to passing tools for the agent to use.\n",
				"\n",
				"When Response function called by LLM, use as signal to return to user. \n",
				"When any other function called by LLM, treat as tool invocation.\n",
				"\n",
				"Parsing logic:\n",
				"- If no function is called, assume response to user is AgentFinish\n",
				"- If Response function is called, respond to user with inputs (structured output) \n",
				"and return AgentFinish\n",
				"- If any other function is called, treat as tool invocation and return AgentActionMessageLog\n",
				"\n",
				"Using AgentActionMessageLog allows to attach a log of messages for future use in passing back to the agent prompt.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"#* Response schema\n",
				"# Two fields: answer and list of sources.\n",
				"class Response(BaseModel):\n",
				"  \"\"\"Final response to the question being asked\"\"\"\n",
				"  \n",
				"  answer: str = Field(\n",
				"    description=\"The final response to the user\"\n",
				"  )\n",
				"  sources: List[int] = Field(\n",
				"      description=(\"List of page chunks that contain answer to the question. \"\n",
				"                   \"Only include a page chunk if it contains relevant information\")\n",
				"  )\n",
				"  \n",
				"#* Custom parsing logic\n",
				"def parse(output):\n",
				"  # If no function was invoked, return to user\n",
				"  if \"function_call\" not in output.additional_kwargs:\n",
				"    return agents.AgentFinish(\n",
				"      return_values={\"output\": output.content}, log=output.content\n",
				"    )\n",
				"  \n",
				"  # Parse out the function call\n",
				"  function_call = output.additional_kwargs[\"function_call\"]\n",
				"  name = function_call[\"name\"]\n",
				"  inputs = json.loads((function_call[\"arguments\"]))\n",
				"  \n",
				"  # If the Response function was invoked, return to the user with the function inputs\n",
				"  if name == \"Response\":\n",
				"    return agents.AgentFinish(return_values=inputs, log=str(function_call)) \n",
				"  # Return an agent action\n",
				"  else:\n",
				"    return agents.AgentActionMessageLog(\n",
				"      tool=name, tool_input=inputs, log=\"\", message_log=[output]\n",
				"    )\n",
				"\n",
				"#* Create Agent\n",
				"# prompt: placeholders for user's question and agent_scratchpad (intermediate steps)\n",
				"prompt = prompts.ChatPromptTemplate.from_messages([\n",
				"  (\"system\", \"You are a helpful assistant\"),\n",
				"  (\"user\", \"{input}\"),\n",
				"  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
				"])\n",
				"\n",
				"# tools: attach tools and Response format to LLM as functions\n",
				"llm = chat_models.chat_openai\n",
				"llm_with_tools = llm.bind_functions([retriever_tool, Response])\n",
				"\n",
				"agent = (\n",
				"  {\n",
				"    \"input\": lambda x: x[\"input\"],\n",
				"    # format agent_scratchpad from intermediate steps (AIMessages, FunctionMessages)\n",
				"    \"agent_scratchpad\": lambda x: agents.format_to_openai_function_messages(\n",
				"      x[\"intermediate_steps\"]\n",
				"    )\n",
				"  }\n",
				"  | prompt\n",
				"  | llm_with_tools\n",
				"  # custom output parser: parse LLM response\n",
				"  | parse\n",
				")\n",
				"\n",
				"# AgentExecutor: run agent-tool loop\n",
				"agent_executor = agents.AgentExecutor(\n",
				"  tools=[retriever_tool], agent=agent, verbose=True,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#* Run agent\n",
				"# It responds with a dictionary answer and sources keys\n",
				"agent_executor.invoke(\n",
				"  {\"input\": \"what did the president say about ketanji brown jackson\"},\n",
				"  return_only_outputs=True,\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Handle parsing errors, Access intermediate steps\n",
				"\n",
				"Occasionally LLM cannot determine step to take because outputs are not correctly formatted for output parser. Default agent errors easily control functionality with handle_parsing_errors.\n",
				"\n",
				"Include intermediary steps as a list of (action, observation) pairs in the return value to enhance agent insight\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"wikipedia_tool = agent_tools.wikipedia\n",
				"tools = [\n",
				"  wikipedia_tool,\n",
				"]\n",
				"\n",
				"prompt = prompts.hub.pull(\"hwchase17/react\")\n",
				"llm = llms.llm_openai\n",
				"agent = agents.create_react_agent(llm, tools, prompt)\n",
				"agent_executor = agents.AgentExecutor(\n",
				"  agent=agent, tools=tools, verbose=True, handle_parsing_errors=True,\n",
				"  return_intermediate_steps=True,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"#* Error\n",
				"# The agent will error due to failing to output an Action string caused by a\n",
				"# malicious input.\n",
				"agent_executor.invoke(\n",
				"  {\"input\": \"What is Leo DiCaprio's middle name?\"}\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Cap the max number of iterations\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Timeouts for agents"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from langchain import hub\n",
				"from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
				"from langchain.tools import tool\n",
				"from langchain_core.callbacks import Callbacks\n",
				"from langchain_core.prompts import ChatPromptTemplate\n",
				"from langchain_openai import ChatOpenAI\n",
				"import random\n",
				"import pprint\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = ChatOpenAI(temperature=0, streaming=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
				"from uuid import UUID\n",
				"\n",
				"from langchain_core.callbacks.base import AsyncCallbackHandler\n",
				"from langchain_core.messages import BaseMessage\n",
				"from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
				"\n",
				"# Get the prompt to use - you can modify this!\n",
				"prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
				"# print(prompt.messages) -- to see the prompt\n",
				"tools = [\n",
				"\tagent_tools.TavilySearchResults(max_results=3)\n",
				"]\n",
				"agent = create_openai_tools_agent(\n",
				"    model.with_config({\"tags\": [\"agent_llm\"]}), tools, prompt\n",
				")\n",
				"agent_executor = AgentExecutor(agent=agent, tools=tools).with_config(\n",
				"    {\"run_name\": \"Agent\"}\n",
				")\n",
				"\n",
				"# Here is a custom handler that will print the tokens to stdout.\n",
				"# Instead of printing to stdout you can send the data elsewhere; e.g., to a streaming API response\n",
				"class TokenByTokenHandler(AsyncCallbackHandler):\n",
				"\tdef __init__(self, tags_of_interest: List[str]) -> None:\n",
				"\t\t\"\"\"A custom call back handler.\n",
				"\n",
				"\t\tArgs:\n",
				"\t\t\t\ttags_of_interest: Only LLM tokens from models with these tags will be\n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\tprinted.\n",
				"\t\t\"\"\"\n",
				"\t\tself.tags_of_interest = tags_of_interest\n",
				"\n",
				"\tasync def on_chain_start(\n",
				"\t\tself,\n",
				"\t\tserialized: Dict[str, Any],\n",
				"\t\tinputs: Dict[str, Any],\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> None:\n",
				"\t\t\"\"\"Run when chain starts running.\"\"\"\n",
				"\t\t# print(\"on chain start: \")\n",
				"\t\t# print(inputs)\n",
				"\n",
				"\tasync def on_chain_end(\n",
				"\t\tself,\n",
				"\t\toutputs: Dict[str, Any],\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> None:\n",
				"\t\t\"\"\"Run when chain ends running.\"\"\"\n",
				"\t\t# print(\"On chain end\")\n",
				"\t\t# print(outputs)\n",
				"\n",
				"\tasync def on_chat_model_start(\n",
				"\t\tself,\n",
				"\t\tserialized: Dict[str, Any],\n",
				"\t\tmessages: List[List[BaseMessage]],\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> Any:\n",
				"\t\t\"\"\"Run when a chat model starts running.\"\"\"\n",
				"\t\toverlap_tags = self.get_overlap_tags(tags)\n",
				"\n",
				"\t\t# if overlap_tags:\n",
				"\t\t# \tprint(\",\".join(overlap_tags), end=\": \", flush=True)\n",
				"\n",
				"\tdef on_tool_start(\n",
				"\t\tself,\n",
				"\t\tserialized: Dict[str, Any],\n",
				"\t\tinput_str: str,\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\tinputs: Optional[Dict[str, Any]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> Any:\n",
				"\t\t\"\"\"Run when tool starts running.\"\"\"\n",
				"\t\tprint(f\"Tool: {serialized}\")\n",
				"\n",
				"\tdef on_tool_end(\n",
				"\t\tself,\n",
				"\t\toutput: Any,\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> Any:\n",
				"\t\t\"\"\"Run when tool ends running.\"\"\"\n",
				"\t\tprint(f\"Result: {str(output)}\")\n",
				"\n",
				"\tasync def on_llm_end(\n",
				"\t\tself,\n",
				"\t\tresponse: LLMResult,\n",
				"\t\t*,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\t**kwargs: Any,\n",
				"\t) -> None:\n",
				"\t\t\"\"\"Run when LLM ends running.\"\"\"\n",
				"\t\toverlap_tags = self.get_overlap_tags(tags)\n",
				"\n",
				"\t\tif overlap_tags:\n",
				"\t\t\t# Who can argue with beauty?\n",
				"\t\t\tprint()\n",
				"\t\t\t# print()\n",
				"\n",
				"\tdef get_overlap_tags(self, tags: Optional[List[str]]) -> List[str]:\n",
				"\t\t\"\"\"Check for overlap with filtered tags.\"\"\"\n",
				"\t\tif not tags:\n",
				"\t\t\treturn []\n",
				"\t\treturn sorted(set(tags or []) & set(self.tags_of_interest or []))\n",
				"\n",
				"\tasync def on_llm_new_token(\n",
				"\t\tself,\n",
				"\t\ttoken: str,\n",
				"\t\t*,\n",
				"\t\tchunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n",
				"\t\trun_id: UUID,\n",
				"\t\tparent_run_id: Optional[UUID] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t\t**kwargs: Any,\n",
				") -> None:\n",
				"\t\t\"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
				"\t\toverlap_tags = self.get_overlap_tags(tags)\n",
				"\n",
				"\t\tif token and overlap_tags:\n",
				"\t\t\tprint(token, end=\"\", flush=True)\n",
				"\n",
				"\n",
				"handler = TokenByTokenHandler(tags_of_interest=[\n",
				"  \"tool_llm\", \"agent_llm\"\n",
				"])\n",
				"\n",
				"result = await agent_executor.ainvoke(\n",
				"\t{\"input\": \"tell me a super long story about a dog\"},\n",
				"\t{\"callbacks\": [handler]},\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Structured Tools\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Running Agent as an Iterator\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tools"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Toolkits\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Defining Custom Tools\n",
				"\n",
				"Construct agent with list of Tools:\n",
				"\n",
				"- Tool components:\n",
				"    - name (str): required, must be unique within set of tools\n",
				"    - description (str): optional but recommended for tool use determination\n",
				"    - args_schema (Pydantic BaseModel): optional but recommended for more information or validation.\n",
				"\n",
				"There are multiple ways to define a tool.\n",
				"\n",
				"Many agents only work with functions that require single inputs\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\"\"\"\n",
				"Two functions: \n",
				"- A made up search function that always returns the string “LangChain” , \n",
				"requires one input\n",
				"- A multiplier function that will multiply two numbers by eachother, requires \n",
				"multiple inputs\n",
				"\"\"\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### @tool decorator\n",
				"\n",
				"\n",
				"\n",
				"Defines a custom tool. \n",
				"\n",
				"The function name is the tool name, can be overridden by passing a string as the first argument. Pass tool name and JSON args into the tool decorator for customization.\n",
				"\n",
				"The function’s docstring is used as the tool’s description - a docstring MUST be provided."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SearchInput(agent_tools.BaseModel):\n",
				"  query: str = agent_tools.Field(description=\"should be a search query\")\n",
				"\n",
				"@agent_tools.tool(\"search_tool\", args_schema=SearchInput, return_direct=True)\n",
				"def search(query: str) -> str:\n",
				"  \"\"\"Look up things online.\"\"\"\n",
				"  return \"LangChain\"\n",
				"\n",
				"@agent_tools.tool\n",
				"def multiply(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Subclass BaseTool\n",
				"\n",
				"Define a custom tool by subclassing the BaseTool class for maximal control over the tool definition.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SearchInput(agent_tools.BaseModel):\n",
				"  query: str = agent_tools.Field(description=\"should be a search query\")\n",
				"  \n",
				"class CustomSearchTool(agent_tools.BaseTool):\n",
				"  name = \"custom_search\"\n",
				"  description = \"useful for when you need to answer questions about current events\"\n",
				"  args_schema: agent_tools.typing.Type[agent_tools.BaseModel] = SearchInput\n",
				"  \n",
				"  def _run(\n",
				"    self, query: str, \n",
				"    run_manager: agent_tools.typing.Optional[agent_tools.CallbackManagerForToolRun] = None,\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool.\"\"\"\n",
				"    return \"LangChain\"\n",
				"  \n",
				"  async def _arun(\n",
				"    self, query: str,\n",
				"    run_manager: agent_tools.typing.Optional[agent_tools.AsyncCallbackManagerForToolRun] = None,\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool asynchronously.\"\"\"\n",
				"    raise NotImplementedError(\"custom_search does not support async\")\n",
				"  \n",
				"\n",
				"tool_search = CustomSearchTool()\n",
				"\n",
				"print(tool_search.name)\n",
				"print(tool_search.description)\n",
				"print(tool_search.args)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class CalculatorInput(agent_tools.BaseModel):\n",
				"  a: int = agent_tools.Field(description=\"first number\")\n",
				"  b: int = agent_tools.Field(description=\"second number\")\n",
				"  \n",
				"class CustomCalculatorTool(agent_tools.BaseTool):\n",
				"  name = \"Calculator\"\n",
				"  description = \"useful for when you need to answer questions about math\"\n",
				"  args_schema: agent_tools.typing.Type[agent_tools.BaseModel] = CalculatorInput\n",
				"  return_direct: bool = True\n",
				"  \n",
				"  def _run(\n",
				"    self, a: int, b: int,\n",
				"    run_manager: agent_tools.typing.Optional[agent_tools.CallbackManagerForToolRun] = None\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool.\"\"\"\n",
				"    return a * b\n",
				"\n",
				"  async def _arun(\n",
				"    self, a: int, b: int,\n",
				"    run_manager: agent_tools.typing.Optional[agent_tools.AsyncCallbackManagerForToolRun] = None\n",
				"  ) -> str:\n",
				"    \"\"\"Use the tool asynchronously.\"\"\"\n",
				"    return NotImplementedError(\"Calculator does not support async.\")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### StructuredTool dataclass\n",
				"\n",
				"Use a StructuredTool dataclass for a mix of convenience and functionality.\n",
				"\n",
				"Can Define a custom args_schema to provide more information about inputs."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def search_function(query: str):\n",
				"  return \"LangChain\"\n",
				"\n",
				"tool_search = agent_tools.StructuredTool.from_function(\n",
				"  func=search_function,\n",
				"  name=\"Search\",\n",
				"  description=\"useful for when you need to answer questions about current events\",\n",
				"  # coroutine=\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SchemaCalculator(agent_tools.BaseModel):\n",
				"  a: int = agent_tools.Field(description=\"first number\")\n",
				"  b: int = agent_tools.Field(description=\"second number\")\n",
				"  \n",
				"def fn_calculator(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b\n",
				"\n",
				"tool_calculator = agent_tools.StructuredTool.from_function(\n",
				"  func=fn_calculator,\n",
				"  name=\"Calculator\",\n",
				"  description=\"multiply numbers\",\n",
				"  args_schema=SchemaCalculator,\n",
				"  return_direct=True,\n",
				"  \n",
				"  # coroutine=\n",
				")\n",
				"\n",
				"print(tool_calculator.name)\n",
				"print(tool_calculator.description)\n",
				"print(tool_calculator.args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Handling Tool Errors\n",
				"\n",
				"When tool encounters error and exception is not caught, agent will stop executing. To continue execution, raise ToolException and set handle_tool_error accordingly.\n",
				"\n",
				"When ToolException is thrown, the agent will handle the exception based on the handle_tool_error variable of the tool, and the processing result will be returned to the agent as observation, and printed in red.\n",
				"\n",
				"Set handle_tool_error to True, a unified string value, or a function. If set as a function, the function should take a ToolException as a parameter and return a str value.\n",
				"\n",
				"Only raising a ToolException won’t be effective. Set the handle_tool_error of the tool because its default value is False.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SchemaCalculator(agent_tools.BaseModel):\n",
				"  a: int = agent_tools.Field(description=\"first number\")\n",
				"  b: int = agent_tools.Field(description=\"second number\")\n",
				"  \n",
				"def fn_calculator(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b\n",
				"\n",
				"tool_calculator = agent_tools.StructuredTool.from_function(\n",
				"  func=fn_calculator,\n",
				"  name=\"Calculator\",\n",
				"  description=\"multiply numbers\",\n",
				"  args_schema=SchemaCalculator,\n",
				"  return_direct=True,\n",
				"  handle_tool_error=agent_tools._handle_error\n",
				"  # coroutine=\n",
				")\n",
				"\n",
				"print(tool_calculator.name)\n",
				"print(tool_calculator.description)\n",
				"print(tool_calculator.args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### My way"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class SchemaCalculator(agent_tools.BaseModel):\n",
				"  a: int = agent_tools.Field(description=\"first number\")\n",
				"  b: int = agent_tools.Field(description=\"second number\")\n",
				"  \n",
				"def fn_calculator(a: int, b: int) -> int:\n",
				"  \"\"\"Multiply two numbers.\"\"\"\n",
				"  return a * b\n",
				"\n",
				"tool_calculator = agent_tools.StructuredTool.from_function(\n",
				"  func=fn_calculator,\n",
				"  name=\"Calculator\",\n",
				"  description=\"multiply numbers\",\n",
				"  args_schema=SchemaCalculator,\n",
				"  return_direct=True,\n",
				"  handle_tool_error=agent_tools._handle_error\n",
				"  # coroutine=\n",
				")\n",
				"\n",
				"print(tool_calculator.name)\n",
				"print(tool_calculator.description)\n",
				"print(tool_calculator.args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Tools as OpenAI Functions\n",
				"\n",
				"LangChain tools are used as OpenAI functions\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = chat_models.chat_openai\n",
				"\n",
				"tool_move_file = agent_tools.MoveFileTool()\n",
				"tools = [\n",
				"  tool_move_file,\n",
				"]\n",
				"\n",
				"fns = [agent_tools.convert_to_openai_function(t) for t in tools]\n",
				"\n",
				"model_with_fns = model.bind_functions(fns)\n",
				"model_with_tools = model.bind_tools(tools)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model_with_tools.invoke([\n",
				"  prompts.HumanMessage(content=\"move file foo to bar\"),\n",
				"])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# Initialize tools\n",
				"tool_tavily_search = agent_tools.TavilySearchResults(max_results=1)\n",
				"tools = [\n",
				"  tool_tavily_search,\n",
				"]\n",
				"\n",
				"# Create Agent\n",
				"prompt = prompts.hub.pull(\"hwchase17/react-chat\")\n",
				"# prompt = prompts.hub.pull(\"hwchase17/react\")\n",
				"\n",
				"# Choose the LLM that will drive the agent\n",
				"llm = chat_models.chat_openai\n",
				"\n",
				"my_agent = agents.MyAgent(prompt=prompt, tools=tools, agent_type=\"react\", llm=llm)\n",
				"\n",
				"questions = [\n",
				"\t\"Hello\",\n",
				"\t\"My name is Bob\",\n",
				"\t\"what's my name?\",\n",
				"\t\"Is there any actor with the same name as me?\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"response = my_agent.invoke_agent(questions[2])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Agent Class"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 82,
			"metadata": {},
			"outputs": [],
			"source": [
				"from torch import is_neg\n",
				"import add_packages\n",
				"import boto3\n",
				"from typing import AsyncGenerator\n",
				"from loguru import logger\n",
				"from typing import Union, Optional, List, Literal\n",
				"\n",
				"from toolkit.langchain import histories, runnables\n",
				"from toolkit import utils\n",
				"\n",
				"from langchain.agents import (\n",
				"\tcreate_openai_tools_agent, create_openai_functions_agent, \n",
				"\tcreate_react_agent, create_self_ask_with_search_agent,\n",
				"\tcreate_xml_agent, create_tool_calling_agent,\n",
				"\tAgentExecutor\n",
				")\n",
				"from langchain_core.language_models.chat_models import BaseChatModel\n",
				"from langchain_core.prompts.chat import BaseChatPromptTemplate\n",
				"from langchain_core.runnables import Runnable\n",
				"from langchain_core.tools import BaseTool\n",
				"from langchain_core.agents import (\n",
				"\tAgentActionMessageLog, AgentFinish, AgentAction\n",
				")\n",
				"from langchain_core.messages import AIMessage, HumanMessage, ChatMessage\n",
				"\n",
				"from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
				"from langchain.agents.format_scratchpad.openai_tools import (\n",
				"\tformat_to_openai_tool_messages, \n",
				")\n",
				"from langchain.agents.format_scratchpad import (\n",
				"\tformat_to_openai_function_messages,\n",
				")\n",
				"from langchain.agents.output_parsers.openai_tools import (\n",
				"\tOpenAIToolsAgentOutputParser,\n",
				")\n",
				"\n",
				"from langchain_community.chat_message_histories.dynamodb import DynamoDBChatMessageHistory\n",
				"\n",
				"#*==============================================================================\n",
				"\n",
				"dynamodb = boto3.resource(\"dynamodb\")\n",
				"\n",
				"#*==============================================================================\n",
				"\n",
				"class ChatHistory:\n",
				"\tdef __init__(\n",
				"\t\tself,\n",
				"\t\thistory_type: Literal[\"in_memory\", \"dynamodb\"] = \"in_memory\",\n",
				"\t\tuser_id: str = \"admin\",\n",
				"\t\tsession_id: str = None,\n",
				"\t):\n",
				"\t\tself.history_type = history_type\n",
				"\n",
				"\t\tself.user_id = user_id\n",
				"\t\tself.is_new_session = not bool(session_id)\n",
				"\t\tself.session_id = session_id if session_id else utils.generate_unique_id(\"uuid_name\")\n",
				"  \n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tself.chat_history = []\n",
				"\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\tself.chat_history = DynamoDBChatMessageHistory(\n",
				"\t\t\t\ttable_name=\"LangChainSessionTable\", session_id=self.session_id,\n",
				"\t\t\t\tkey={\n",
				"\t\t\t\t\t\"SessionId\": self.session_id,\n",
				"\t\t\t\t\t\"UserId\": self.user_id,\n",
				"\t\t\t\t}\n",
				"\t\t\t)\n",
				"\n",
				"\t\tif self.is_new_session:\n",
				"\t\t\twelcome_msg = \"Hello! How can I help you today?\"\n",
				"   \n",
				"\t\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\t\tself.chat_history.append(AIMessage(welcome_msg))\n",
				"\t\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\t\tself.chat_history.add_ai_message(welcome_msg)\n",
				"\n",
				"\t\tlogger.info(f\"User Id: {self.user_id}\")\n",
				"\t\tlogger.info(f\"Session Id: {self.session_id}\")\n",
				"\t\tlogger.info(f\"History Type: {self.history_type}\")\n",
				"  \n",
				"\tasync def _add_messages_to_history(\n",
				"\t\tself,\n",
				"\t\tmsg_user: str,\n",
				"\t\tmsg_ai: str,\n",
				"\t):\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tself.chat_history.append(HumanMessage(msg_user))\n",
				"\t\t\tself.chat_history.append(AIMessage(msg_ai))\n",
				"\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\tawait self.chat_history.aadd_messages(messages=[HumanMessage(msg_user)])\n",
				"\t\t\tawait self.chat_history.aadd_messages(messages=[AIMessage(msg_ai)])\n",
				"\n",
				"\tasync def _get_chat_history(self):\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\treturn self.chat_history\n",
				"\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\treturn self.chat_history.messages\n",
				"\n",
				"\tasync def clear_chat_history(self):\n",
				"\t\tif self.history_type == \"in_memory\":\n",
				"\t\t\tself.chat_history = []\n",
				"\t\telif self.history_type == \"dynamodb\":\n",
				"\t\t\tawait self.chat_history.aclear()\n",
				"\n",
				"class MyAgent:\n",
				"\tdef __init__(\n",
				"\t\tself,\n",
				"\t\tllm: Union[BaseChatModel, None],\n",
				"\t\ttools: list[BaseTool],\n",
				"\t\tprompt: Union[BaseChatPromptTemplate, None],\n",
				"\t\thistory: ChatHistory,\n",
				"\t\tagent_type: Literal[\n",
				"\t\t\t\"tool_calling\", \"openai_tools\", \"react\", \"anthropic\"\n",
				"\t\t] = \"tool_calling\",\n",
				"\t\tagent_verbose: bool = False,\n",
				"\t):\n",
				"\t\tself.llm = llm\n",
				"\t\tself.tools = tools\n",
				"\t\tself.prompt = prompt\n",
				"\n",
				"\t\tself.agent_type = agent_type\n",
				"\t\tself.agent_verbose = agent_verbose\n",
				"\n",
				"\t\tself.history = history\n",
				"\n",
				"\t\tself.agent = self._create_agent()\n",
				"\t\tself.agent_executor = AgentExecutor(\n",
				"\t\t\tagent=self.agent, tools=self.tools, verbose=self.agent_verbose,\n",
				"\t\t\thandle_parsing_errors=True,\n",
				"\t\t\treturn_intermediate_steps=False,\n",
				"\t\t)\n",
				"\n",
				"\tdef _create_agent(self) -> Runnable:\n",
				"\t\tlogger.info(f\"Agent type: {self.agent_type}\")\n",
				"  \n",
				"\t\tif self.agent_type == \"tool_calling\":\n",
				"\t\t\treturn create_tool_calling_agent(self.llm, self.tools, self.prompt)\n",
				"\t\telif self.agent_type == \"openai_tools\":\n",
				"\t\t\treturn create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
				"\t\telif self.agent_type == \"react\":\n",
				"\t\t\treturn create_react_agent(llm=self.llm, tools=self.tools, prompt=self.prompt)\n",
				"\t\telif self.agent_type == \"anthropic\": # todo\n",
				"\t\t\treturn create_xml_agent(llm=self.llm, tools=self.tools, prompt=self.prompt)\n",
				"\t\telse:\n",
				"\t\t\traise ValueError(\n",
				"\t\t\t\t\t\"Invalid agent type. Supported types are 'openai_tools' and 'react'.\")\n",
				"\n",
				"\tasync def invoke_agent(\n",
				"\t\tself,\n",
				"\t\tinput_message: str,\n",
				"\t\tcallbacks: Optional[List] = None,\n",
				"\t\tmode: Literal[\"sync\", \"async\"] = \"async\",\n",
				"\t):\n",
				"\t\tresult = None\n",
				"\n",
				"\t\tinput_data = {\n",
				"\t\t\t\"input\": input_message, \"chat_history\": await self.history._get_chat_history()\n",
				"\t\t}\n",
				"\n",
				"\t\tconfigs = {}\n",
				"\t\tconfigs[\"callbacks\"] = callbacks if callbacks else []\n",
				"\n",
				"\t\tif mode == \"sync\":\n",
				"\t\t\tresult = self.agent_executor.invoke(input_data, configs)\n",
				"\t\telif mode == \"async\":\n",
				"\t\t\tresult = await self.agent_executor.ainvoke(input_data, configs)\n",
				"\n",
				"\t\tresult = result[\"output\"]\n",
				"\n",
				"\t\tawait self.history._add_messages_to_history(input_message, result)\n",
				"\n",
				"\t\treturn result\n",
				"\n",
				"\tasync def astream_events_basic(\n",
				"\t\tself,\n",
				"\t\tinput_message: str,\n",
				"\t\tshow_tool_call: bool = False,\n",
				"\t) -> AsyncGenerator[str, None]:\n",
				"\t\t\"\"\"\n",
				"\t\tasync for chunk in agent.astream_events_basic(\"Hello\"):\n",
				"\t\t\tprint(chunk, end=\"\", flush=True)\n",
				"\t\t\"\"\"\n",
				"\n",
				"\t\tresult = \"\"\n",
				"\t\tasync for event in self.agent_executor.astream_events(\n",
				"\t\t\tinput={\"input\": input_message, \"chat_history\": await self.history._get_chat_history()},\n",
				"\t\t\tversion=\"v1\",\n",
				"\t\t):\n",
				"\t\t\tevent_event = event[\"event\"]\n",
				"\t\t\tevent_name = event[\"name\"]\n",
				"\n",
				"\t\t\tif event[\"event\"] == \"on_chat_model_stream\":\n",
				"\t\t\t\tchunk = dict(event[\"data\"][\"chunk\"])[\"content\"]\n",
				"\t\t\t\tresult += chunk\n",
				"\t\t\t\tyield chunk\n",
				"\n",
				"\t\t\tif show_tool_call and event_event == \"on_chain_stream\" and event_name == \"Agent\":\n",
				"\t\t\t\tif 'actions' in event['data']['chunk']:\n",
				"\t\t\t\t\tevent_log = dict(list(event['data']['chunk']['actions'])[0])['log']\n",
				"\t\t\t\t\tchunk = event_log\n",
				"\t\t\t\t\tresult += chunk\n",
				"\t\t\t\t\tyield chunk\n",
				"\n",
				"\t\tawait self.history._add_messages_to_history(input_message, result)\n",
				"\n",
				"\tasync def astream_events_basic_wrapper(\n",
				"\t\tself,\n",
				"\t\tinput_message: str,\n",
				"\t):\n",
				"\t\tresult = \"\"\n",
				"\t\tasync for chunk in self.astream_events_basic(input_message):\n",
				"\t\t\tresult += chunk\n",
				"\t\t\tprint(chunk, end=\"\", flush=True)\n",
				"\t\treturn result\n",
				"\n",
				"\tdef hello():\n",
				"\t\t...\n",
				"  "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 96,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-05-16 13:37:23.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mUser Id: admin\u001b[0m\n",
						"\u001b[32m2024-05-16 13:37:23.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mSession Id: Ashley Martinez-4abc8215\u001b[0m\n",
						"\u001b[32m2024-05-16 13:37:23.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mHistory Type: in_memory\u001b[0m\n",
						"\u001b[32m2024-05-16 13:37:23.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_create_agent\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mAgent type: tool_calling\u001b[0m\n"
					]
				}
			],
			"source": [
				"llm = chat_models.chat_openai\n",
				"prompt = prompts.create_prompt_tool_calling_agent()\n",
				"tools = [\n",
				"\tagent_tools.TavilySearchResults(max_results=3)\n",
				"]\n",
				"\n",
				"history = ChatHistory(\n",
				"\thistory_type='in_memory',\n",
				"\tuser_id=\"admin\",\n",
				"\t# session_id=\"1\",\n",
				")\n",
				"\n",
				"agent = MyAgent(\n",
				"\tllm=llm,\n",
				"\ttools=tools,\n",
				"\tprompt=prompt,\n",
				"\thistory=history,\n",
				"\tagent_verbose=False,\n",
				"\tagent_type='tool_calling',\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 84,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = [\n",
				"\t\"Hello\",\n",
				"\t\"My name is Bob\",\n",
				"\t\"What is my name?\"\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 97,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"'Hi there! How can I assist you today?'"
						]
					},
					"execution_count": 97,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"await agent.invoke_agent(questions[0])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 99,
			"metadata": {},
			"outputs": [],
			"source": [
				"res = await agent.history._get_chat_history()\n",
				"res = [dict(msg) for msg in result]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 105,
			"metadata": {},
			"outputs": [],
			"source": [
				"import json"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 106,
			"metadata": {},
			"outputs": [],
			"source": [
				"res_json = json.dumps(res)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 108,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"'[{\"content\": \"Hello! How can I help you today?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}, {\"content\": \"Hello\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}, {\"content\": \"Hi there! How can I assist you today?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"ai\", \"name\": null, \"id\": null, \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}]'"
						]
					},
					"execution_count": 108,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"res_json"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 109,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"[{'content': 'Hello! How can I help you today?',\n",
							"  'additional_kwargs': {},\n",
							"  'response_metadata': {},\n",
							"  'type': 'ai',\n",
							"  'name': None,\n",
							"  'id': None,\n",
							"  'example': False,\n",
							"  'tool_calls': [],\n",
							"  'invalid_tool_calls': []},\n",
							" {'content': 'Hello',\n",
							"  'additional_kwargs': {},\n",
							"  'response_metadata': {},\n",
							"  'type': 'human',\n",
							"  'name': None,\n",
							"  'id': None,\n",
							"  'example': False},\n",
							" {'content': 'Hi there! How can I assist you today?',\n",
							"  'additional_kwargs': {},\n",
							"  'response_metadata': {},\n",
							"  'type': 'ai',\n",
							"  'name': None,\n",
							"  'id': None,\n",
							"  'example': False,\n",
							"  'tool_calls': [],\n",
							"  'invalid_tool_calls': []}]"
						]
					},
					"execution_count": 109,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"json.loads(res_json)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 88,
			"metadata": {},
			"outputs": [],
			"source": [
				"await agent.history.clear_chat_history()"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.8"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
