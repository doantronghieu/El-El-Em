{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import add_packages\n",
				"import config\n",
				"from pprint import pprint\t\n",
				"import os\n",
				"import bs4\n",
				"\n",
				"import ast\n",
				"\n",
				"from toolkit.langchain import (\n",
				"\tdocument_loaders, text_splitters, text_embedding_models, stores, \n",
				"\tmodels, prompts, utils, output_parsers, agents, output_parsers, documents,\n",
				"\trunnables, chains\n",
				"\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Usecase"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q&A with RAG "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Base"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"import bs4\n",
				"from langchain import hub\n",
				"from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
				"from langchain_core.messages import AIMessage, HumanMessage"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Indexing: Load\n",
				"bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
				"loader = document_loaders.web_base_loader(\n",
				"  web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"  bs_kwargs={\"parse_only\": bs4_strainer}\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"# Indexing: Split\n",
				"text_splitter = text_splitters.recursive_character_text_splitter(\n",
				"  chunk_size=1000, chunk_overlap=200, add_start_index=True,\n",
				")\n",
				"all_splits = text_splitter.split_documents(docs)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-03-07 10:45:10.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mFound collection: `my-rag`.\u001b[0m\n"
					]
				}
			],
			"source": [
				"qdrant_instance = stores.QdrantWrapper(\n",
				"  collection_name=\"my-rag\",\n",
				"  qdrant_host=os.getenv(\"QDRANT_HOST\"),\n",
				"  qdrant_api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
				"  default_search_type=\"similarity\",\n",
				"  default_search_kwargs={\"k\": 6},\n",
				"  retriever_tool_name=\"search_state_of_union\",\n",
				"  retriever_tool_description=\"Searches and returns excerpts from the 2022 State of the Union.\",\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Indexing: Store\n",
				"qdrant_instance.vector_store.add_documents(documents=all_splits)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Retrieval and Generation: Retrieve\n",
				"query = \"What are the approaches to Task Decomposition?\"\n",
				"retrieved_docs = qdrant_instance.invoke_retriever(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Retrieval and Generation: Generate\n",
				"\n",
				"chat = models.chat_openai\n",
				"\n",
				"prompt = prompts.rag_prompt\n",
				"\n",
				"rag_chain = (\n",
				"  {\n",
				"    \"context\": qdrant_instance.retriever | utils.format_docs, \n",
				"    \"question\": RunnablePassthrough()\n",
				"  }\n",
				"  | prompt\n",
				"  | chat\n",
				"  | output_parsers.StrOutputParser()\n",
				")\n",
				"\n",
				"example_messages = prompt.invoke(\n",
				"  {\n",
				"    \"context\": \"filter context\",\n",
				"    \"question\": \"filter question\"\n",
				"  }\n",
				").to_messages()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Task decomposition involves breaking down complex tasks into smaller and simpler steps to enhance model performance. Techniques like Chain of Thought and Tree of Thoughts help in transforming big tasks into manageable ones by exploring multiple reasoning possibilities at each step. Task decomposition can be done using simple prompting, task-specific instructions, or human inputs."
					]
				}
			],
			"source": [
				"for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
				"  print(chunk, end=\"\", flush=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Add Source"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Adding sources\n",
				"rag_chain_from_dos = (\n",
				"  RunnablePassthrough.assign(context=(lambda x: utils.format_docs(x[\"context\"])))\n",
				"  | prompt\n",
				"  | chat\n",
				"  | output_parsers.str_output_parser()\n",
				")\n",
				"rag_chain_with_source = RunnableParallel(\n",
				"  {\n",
				"    \"context\": qdrant_instance.retriever,\n",
				"    \"question\": RunnablePassthrough()\n",
				"  }\n",
				").assign(answer=rag_chain_from_dos)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"{'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585, '_id': '3e36d9d5-a5f2-48cd-9bf9-0e1d127bf068', '_collection_name': 'my-rag'}),\n",
							"  Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192, '_id': '355aa980-55bd-40ba-b72c-6ab47ea9c0d9', '_collection_name': 'my-rag'}),\n",
							"  Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630, '_id': 'f0933cc3-36d0-4a29-a6d7-a8835b8a5366', '_collection_name': 'my-rag'}),\n",
							"  Document(page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373, '_id': 'e2e9d02e-6329-4dd3-b47a-7252e542be88', '_collection_name': 'my-rag'}),\n",
							"  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17804, '_id': 'e5f959e3-1b52-4678-8394-75d6e2baca87', '_collection_name': 'my-rag'}),\n",
							"  Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414, '_id': '6e433cf8-7094-476d-b2a6-6527aa9fc4ce', '_collection_name': 'my-rag'})],\n",
							" 'question': 'What is Task Decomposition?',\n",
							" 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This process allows agents to plan ahead and tackle each step individually. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or with task-specific instructions.'}"
						]
					},
					"execution_count": 17,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"rag_chain_with_source.invoke(\"What is Task Decomposition?\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Add chat history"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {},
			"outputs": [],
			"source": [
				"contextualize_q_chain = (\n",
				"  prompts.contextualize_q_prompt \n",
				"  | chat\n",
				"  | output_parsers.str_output_parser()\n",
				")\n",
				"\n",
				"def contextualized_question(input: dict):\n",
				"  if input.get(\"chat_history\"):\n",
				"    return contextualize_q_chain\n",
				"  else:\n",
				"    return input[\"question\"]\n",
				"\n",
				"rag_chain = (\n",
				"  RunnablePassthrough.assign(\n",
				"    context=contextualized_question | qdrant_instance.retriever | utils.format_docs\n",
				"  )\n",
				"  | prompts.qa_prompt\n",
				"  | chat\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {},
			"outputs": [],
			"source": [
				"chat_history = []\n",
				"\n",
				"questions = [\n",
				"  \"What is Task Decomposition?\",\n",
				"  \"What are common ways of doing it?\"\n",
				"]\n",
				"\n",
				"for question in questions:\n",
				"  ai_msg = rag_chain.invoke({\n",
				"    \"question\": question, \"chat_history\": chat_history\n",
				"  })\n",
				"  chat_history.extend([HumanMessage(content=question), ai_msg])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 19,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable for an agent or model. Techniques like Chain of Thought (CoT) and Tree of Thoughts help in decomposing hard tasks into multiple manageable tasks by guiding the model to think step by step or explore multiple reasoning possibilities at each step. Task decomposition can be done using simple prompting, task-specific instructions, or human inputs to guide the agent or model in achieving the overall task goal."
					]
				}
			],
			"source": [
				"for chunk in rag_chain.stream({\n",
				"  \"question\": \"What is Task Decomposition\", \"chat_history\": []\n",
				"}):\n",
				"  print(chunk.content, flush=True, end='')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Per-User Retrieval"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Citations"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Use Agents"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"metadata": {},
			"outputs": [],
			"source": [
				"tools = [\n",
				"  qdrant_instance.retriever_tool\n",
				"]\n",
				"\n",
				"agent_prompt = prompts.prompt_agent_openai_tools\n",
				"\n",
				"agent = agents.create_openai_tools_agent(\n",
				"  llm=models.chat_openai,\n",
				"  tools=tools,\n",
				"  prompt=agent_prompt,\n",
				")\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 25,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"\n",
						"\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
						"\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
						"\n",
						"\u001b[1m> Finished chain.\u001b[0m\n"
					]
				},
				{
					"data": {
						"text/plain": [
							"{'input': 'hi, i am Bob', 'output': 'Hello Bob! How can I assist you today?'}"
						]
					},
					"execution_count": 25,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"agent_executor.invoke({\"input\": \"hi, i am Bob\"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Use Local Models"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tutorials"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Build a RAG App](https://python.langchain.com/v0.2/docs/tutorials/rag/)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [],
			"source": [
				"llm = models.chat_openai"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"\tbs_kwargs=dict(\n",
				"\t\tparse_only=bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t),\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=1000, chunk_overlap=200,\n",
				")\n",
				"splits = splitter.split_documents(docs)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 49,
			"metadata": {},
			"outputs": [],
			"source": [
				"vectorstore = stores.chroma.Chroma.from_documents(\n",
				"\tdocuments=splits, embedding=text_embedding_models.OpenAIEmbeddings(),\n",
				")\n",
				"retriever = vectorstore.as_retriever(\n",
				"\tsearch_type=\"similarity\",\n",
				"\tsearch_kwargs={\n",
				"\t\t\"k\": 6,\n",
				"\t}\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 98,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompt_tpl_filter_context = \"\"\"\\\n",
				"You are an AI assistant tasked with filtering a list of retrieved text chunks to only the most relevant ones for answering a given question.\n",
				"\n",
				"Here is the question:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the retrieved text chunks:\n",
				"<retrieved_chunks>\n",
				"{context}\n",
				"</retrieved_chunks>\n",
				"\n",
				"Carefully analyze each chunk for how relevant it is to answering the question. Consider the following:\n",
				"- Does the chunk contain information that helps answer the question?\n",
				"- Does the chunk provide important context or background for the question?\n",
				"- Is the chunk focused on the key concepts and entities mentioned in the question?\n",
				"\n",
				"Create a Python list containing only the most relevant chunks. The list should be a subset of the original retrieved_chunks list. Omit any chunks that are not directly helpful for answering the question. \n",
				"\n",
				"Output this filtered list of the most relevant chunks. The format should be a valid Python list, like:\n",
				"['chunk 1 text', \n",
				"'chunk 2 text',\n",
				"'chunk 3 text']\n",
				"\n",
				"Remember, the goal is to create a focused list of only the most relevant chunks for answering the original question. Do not include any irrelevant or tangential chunks in your final result list.\n",
				"\"\"\"\n",
				"prompt_filter_context = prompts.ChatPromptTemplate.from_template(prompt_tpl_filter_context)\n",
				"\n",
				"prompt_tpl_rag = \"\"\"\\\n",
				"Here is the question to answer:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the relevant pieces of context that may help answer the question:\n",
				"<context>\n",
				"{context_filtered}\n",
				"</context>\n",
				"\n",
				"Carefully read the question and context. Think through how the context can be used to answer the question in the <scratchpad> area below:\n",
				"\n",
				"Now provide your final answer to the question. If the question cannot be answered based on the provided context, simply say \"I don't know.\" Keep your answer to 3 sentences maximum and prioritize conciseness.\n",
				"\n",
				"Helpful Answer:\\\n",
				"\"\"\"\n",
				"prompt_rag = prompts.ChatPromptTemplate.from_template(prompt_tpl_rag)\n",
				"\n",
				"def format_docs_to_str(docs: list[document_loaders.Document]):\n",
				"\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
				"\n",
				"def format_docs_to_list(docs: list[document_loaders.Document]):\n",
				"\treturn [doc.page_content for doc in docs]\n",
				"\n",
				"chain_rag = runnables.RunnableParallel(\n",
				"\t{\n",
				"\t\t\"question\": runnables.RunnablePassthrough(),\n",
				"\t\t\"context\": retriever | format_docs_to_list,\n",
				"\t}\n",
				")\\\n",
				"  .assign(context_filtered=(\n",
				"\t\tprompt_filter_context | llm | output_parsers.StrOutputParser() | ast.literal_eval\n",
				"  )).with_retry()\\\n",
				"  .assign(output=(\n",
				"\t\tprompt_rag\t| llm\t| output_parsers.StrOutputParser()\n",
				"\t)).with_retry()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 99,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = chain_rag.invoke(\n",
				"  \"What is Task Decomposition?\"\n",
				")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [RAG From Scratch](https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&si=dE6TOhGs5KMC1zc7)\n",
				"\n",
				"[Git](https://github.com/langchain-ai/rag-from-scratch/tree/main)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.OpenAIEmbeddings()\n",
				"llm = models.chat_openai"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Basic Flow\n",
				"\n",
				"- [Indexing](https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&si=dE6TOhGs5KMC1zc7), [Slide](https://docs.google.com/presentation/d/1MhsCqZs7wTX6P19TFnA9qRSlxH3u-1-0gWkhBiDG9lQ/edit#slide=id.p)\n",
				"- [Retrieval](https://youtu.be/LxNVgdIz9sU?si=rmu8kYV1BH_hwEvo), [Slide](https://docs.google.com/presentation/d/124I8jlBRCbb0LAUhdmDwbn4nREqxSxZU1RF_eTGXUGc/edit#slide=id.g267060cc54f_0_0)\n",
				"- [Generation](https://youtu.be/Vw52xyyFsB8?si=pQqUluFZUrxTnwZP), [Slide](https://docs.google.com/presentation/d/1eRJwzbdSv71e9Ou9yeqziZrz1UagwX8B1kL4TbL5_Gc/edit#slide=id.g2b46f2cb556_0_0)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"\tbs_kwargs=dict(\n",
				"\t\tparse_only=bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t),\n",
				")\n",
				"doc = loader.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=300, chunk_overlap=50,\n",
				")\n",
				"docs = text_splitter.split_documents(doc)\n",
				"\n",
				"vectorstore = stores.chroma.Chroma.from_documents(docs, embeddings)\n",
				"retriever = vectorstore.as_retriever()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [],
			"source": [
				"template = \"\"\"\\\n",
				"Answer the question based only on the following context:\n",
				"{context}\n",
				"\n",
				"Question: {question}\n",
				"\"\"\"\n",
				"prompt = prompts.ChatPromptTemplate.from_template(template)\n",
				"\n",
				"chain_rag: runnables.Runnable = (\n",
				"\t{\n",
				"\t\t\"context\": retriever,\n",
				"\t\t\"question\": runnables.RunnablePassthrough()\n",
				"\t}\n",
				"\t| prompt\n",
				"\t| llm\n",
				"\t| output_parsers.StrOutputParser()\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps agents to plan and execute tasks more efficiently by transforming big tasks into manageable subtasks. Task decomposition can be achieved through various methods such as prompting with specific instructions or utilizing human inputs.'"
						]
					},
					"execution_count": 15,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"chain_rag.invoke(\"What is Task Decomposition?\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Query Translation\n",
				"\n",
				"Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Multi Query](https://youtu.be/JChPi0CRnDY?si=wEgjcc0NHINTvQVh), [Slide](https://docs.google.com/presentation/d/15pWydIszbQG3Ipur9COfTduutTZm6ULdkkyX-MNry8I/edit#slide=id.g268cd4ba153_0_0), [LangChain](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever/)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [RAG Fusion](https://youtu.be/77qELPbNgxA?si=uyfzuemn02ktS2xe), [Slide](https://docs.google.com/presentation/d/1EwykmdVSQqlh6XpGt8APOMYp4q1CZqqeclAx61pUcjI/edit#slide=id.g268cfa48f45_0_0), [LangChain](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker/)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Decomposition](https://youtu.be/h0OPWlEOank?si=jU3DecxsmxDWi9az), [Slide](https://docs.google.com/presentation/d/1O97KYrsmYEmhpQ6nkvOVAqQYMJvIaZulGFGmz4cuuVE/edit#slide=id.g268fdc1fda2_0_0)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Step Back](https://youtu.be/xn1jEjRyJ2U?si=63WfDLTQwBKmUsdW), [Slide](https://docs.google.com/presentation/d/1L0MRGVDxYA1eLOR0L_6Ze1l2YV8AhN1QKUtmNA-fJlU/edit#slide=id.g268cfa65240_0_0)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [HyDE](https://youtu.be/SaDzIVkYqyY?si=7tFx5bpTiBpy5KkV), [Slide](https://docs.google.com/presentation/d/10MmB_QEiS4m00xdyu-92muY-8jC3CdaMpMXbXjzQXsM/edit#slide=id.g2b872e9a17e_0_0)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Routing](https://youtu.be/pfpIndq7Fi8?si=m6SerpLuJdKzIV6A), [Slide](https://docs.google.com/presentation/d/1kC6jFj8C_1ZXDYcFaJ8vhJvCYEwxwsVqk2VVeKKuyx4/edit#slide=id.g26bc3116f45_0_0)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### [Query Structuring](https://youtu.be/kl6NwWYxvbM?si=Vm0MiQL13kI0nr-Q), [Blog](https://blog.langchain.dev/query-construction/)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Indexing"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Multi-Representation Indexing](https://youtu.be/gTCU9I6QqCE?si=jQ3Aj9ko3DYVQ1vU), [Slide](https://blog.langchain.dev/semi-structured-multi-modal-rag/)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [RAPTOR](https://youtu.be/z_6EeA2LDSw?si=E09-N68W93TgBNBC), [Code](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)\n",
				"\n",
				"### [ColBERT](https://youtu.be/cN6S0Ehm7_8?si=LGBLo-VUonJMXnmR)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"llm = models.chat_openai"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"\tbs_kwargs=dict(\n",
				"\t\tparse_only=bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t),\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=1000, chunk_overlap=200,\n",
				")\n",
				"splits = splitter.split_documents(docs)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-06-11 11:39:50.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.langchain.stores\u001b[0m:\u001b[36mcreate_retriever\u001b[0m:\u001b[36m267\u001b[0m - \u001b[1mRetrievers: ['base', 'MultiQueryRetriever', 'RePhraseQueryRetriever']\u001b[0m\n"
					]
				}
			],
			"source": [
				"\n",
				"vectorstore = stores.chroma.Chroma.from_documents(\n",
				"\tdocuments=splits, embedding=text_embedding_models.OpenAIEmbeddings(),\n",
				")\n",
				"\n",
				"retriever = vectorstore.as_retriever(\n",
				"\tsearch_type=\"similarity\",\n",
				"\tsearch_kwargs={\n",
				"\t\t\"k\": 6,\n",
				"\t}\n",
				")\n",
				"\n",
				"my_retriever = stores.create_retriever(\n",
				"\tvectorstore=vectorstore,\n",
				"\tllm=llm,\n",
				"\tretriever_types=['base', 'MultiQueryRetriever', 'RePhraseQueryRetriever'],\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompt_tpl_filter_context = \"\"\"\\\n",
				"You are an AI assistant tasked with filtering a list of retrieved text chunks to only the most relevant ones for answering a given question.\n",
				"\n",
				"Here is the question:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the retrieved text chunks:\n",
				"<retrieved_chunks>\n",
				"{context}\n",
				"</retrieved_chunks>\n",
				"\n",
				"Carefully analyze each chunk for how relevant it is to answering the question. Consider the following:\n",
				"- Does the chunk contain information that helps answer the question?\n",
				"- Does the chunk provide important context or background for the question?\n",
				"- Is the chunk focused on the key concepts and entities mentioned in the question?\n",
				"\n",
				"Create a Python list containing only the most relevant chunks. The list should be a subset of the original retrieved_chunks list. Omit any chunks that are not directly helpful for answering the question. \n",
				"\n",
				"Output this filtered list of the most relevant chunks. The format should be a valid Python list, like:\n",
				"['chunk 1 text', \n",
				"'chunk 2 text',\n",
				"'chunk 3 text']\n",
				"\n",
				"Remember, the goal is to create a focused list of only the most relevant chunks for answering the original question. Do not include any irrelevant or tangential chunks in your final result list.\n",
				"\"\"\"\n",
				"prompt_filter_context = prompts.ChatPromptTemplate.from_template(prompt_tpl_filter_context)\n",
				"\n",
				"prompt_tpl_rag = \"\"\"\\\n",
				"Here is the question to answer:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the relevant pieces of context that may help answer the question:\n",
				"<context>\n",
				"{context_filtered}\n",
				"</context>\n",
				"\n",
				"Carefully read the question and context. Think through how the context can be used to answer the question. If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\".\n",
				"\n",
				"Provide your final answer to the question. If the question cannot be answered based on the provided context, simply say \"I don't know.\" Keep your answer to 3 sentences maximum and prioritize conciseness.\n",
				"\n",
				"Helpful Answer:\\\n",
				"\"\"\"\n",
				"prompt_rag = prompts.ChatPromptTemplate.from_template(prompt_tpl_rag)\n",
				"\n",
				"def format_docs_to_str(docs: list[document_loaders.Document]):\n",
				"\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
				"\n",
				"def format_docs_to_list(docs: list[document_loaders.Document]):\n",
				"\treturn [doc.page_content for doc in docs]\n",
				"\n",
				"chain_rag = runnables.RunnableParallel(\n",
				"\t{\n",
				"\t\t\"question\": runnables.RunnablePassthrough(),\n",
				"\t\t\"context\": my_retriever | format_docs_to_list,\n",
				"\t}\n",
				")\\\n",
				"  .assign(context_filtered=(\n",
				"\t\tprompt_filter_context | llm | output_parsers.StrOutputParser() | ast.literal_eval\n",
				"  )).with_retry()\\\n",
				"  .assign(output=(\n",
				"\t\tprompt_rag\t| llm\t| output_parsers.StrOutputParser()\n",
				"\t)).with_retry()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = chain_rag.invoke(\n",
				"  \"What is Task Decomposition?\"\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"{'question': 'What is Task Decomposition?',\n",
							" 'context': ['The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.',\n",
							"  'Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.',\n",
							"  'Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.',\n",
							"  'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.',\n",
							"  'Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.',\n",
							"  'inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.',\n",
							"  'FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.',\n",
							"  'They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.',\n",
							"  '}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:',\n",
							"  '\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease',\n",
							"  'Fig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.',\n",
							"  'This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.',\n",
							"  'You always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:',\n",
							"  'Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.',\n",
							"  'You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.'],\n",
							" 'context_filtered': ['Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.',\n",
							"  'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'],\n",
							" 'output': 'Task Decomposition is a technique used to break down complex tasks into smaller, more manageable steps. This process allows agents or models to tackle difficult tasks by dividing them into simpler components, aiding in problem-solving and decision-making. Task decomposition can be facilitated through methods like Chain of Thought or Tree of Thoughts, which guide the breakdown of tasks into sequential or branching steps.'}"
						]
					},
					"execution_count": 11,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"result"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.9"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
