{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import add_packages\n",
				"import config\n",
				"from pprint import pprint\t\n",
				"import os\n",
				"import bs4\n",
				"\n",
				"import ast\n",
				"from langchain_core.retrievers import BaseRetriever\n",
				"\n",
				"from toolkit.langchain import (\n",
				"\tdocument_loaders, text_splitters, text_embedding_models, stores, \n",
				"\tmodels, prompts, utils, output_parsers, agents, output_parsers, documents,\n",
				"\trunnables, chains\n",
				"\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from operator import itemgetter\n",
				"from toolkit import sql \n",
				"from typing import Any, Awaitable, Callable, Optional, Type, Union, Dict, List\n",
				"\n",
				"from langchain.chains.combine_documents import create_stuff_documents_chain\n",
				"from langchain.chains.conversation.base import ConversationChain\n",
				"from langchain.chains.llm import LLMChain\n",
				"from langchain.chains.query_constructor.schema import AttributeInfo\n",
				"from langchain.chains.retrieval_qa.base import RetrievalQA\n",
				"from langchain.chains.sql_database.query import create_sql_query_chain\n",
				"from langchain.pydantic_v1 import BaseModel, Field\n",
				"from langchain.tools import BaseTool, tool\n",
				"from langchain.tools import StructuredTool\n",
				"from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
				"from langchain_community.utilities import SQLDatabase\n",
				"from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
				"from langchain_core.embeddings import Embeddings\n",
				"from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
				"from langchain_core.language_models.chat_models import  BaseChatModel\n",
				"from langchain_core.output_parsers import StrOutputParser\n",
				"from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
				"from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
				"from langchain_core.pydantic_v1 import BaseModel, Field\n",
				"from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
				"from langchain_core.tools import ToolException, ValidationError\n",
				"from langchain_core.vectorstores import VectorStore"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Usecase"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Q&A with RAG "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Base"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import bs4\n",
				"from langchain import hub\n",
				"from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
				"from langchain_core.messages import AIMessage, HumanMessage"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Indexing: Load\n",
				"bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
				"loader = document_loaders.web_base_loader(\n",
				"  web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"  bs_kwargs={\"parse_only\": bs4_strainer}\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"# Indexing: Split\n",
				"text_splitter = text_splitters.recursive_character_text_splitter(\n",
				"  chunk_size=1000, chunk_overlap=200, add_start_index=True,\n",
				")\n",
				"all_splits = text_splitter.split_documents(docs)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"qdrant_instance = stores.QdrantWrapper(\n",
				"  collection_name=\"my-rag\",\n",
				"  qdrant_host=os.getenv(\"QDRANT_HOST\"),\n",
				"  qdrant_api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
				"  default_search_type=\"similarity\",\n",
				"  default_search_kwargs={\"k\": 6},\n",
				"  retriever_tool_name=\"search_state_of_union\",\n",
				"  retriever_tool_description=\"Searches and returns excerpts from the 2022 State of the Union.\",\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Indexing: Store\n",
				"qdrant_instance.vector_store.add_documents(documents=all_splits)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Retrieval and Generation: Retrieve\n",
				"query = \"What are the approaches to Task Decomposition?\"\n",
				"retrieved_docs = qdrant_instance.invoke_retriever(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Retrieval and Generation: Generate\n",
				"\n",
				"chat = models.chat_openai\n",
				"\n",
				"prompt = prompts.rag_prompt\n",
				"\n",
				"rag_chain = (\n",
				"  {\n",
				"    \"context\": qdrant_instance.retriever | utils.format_docs, \n",
				"    \"question\": RunnablePassthrough()\n",
				"  }\n",
				"  | prompt\n",
				"  | chat\n",
				"  | output_parsers.StrOutputParser()\n",
				")\n",
				"\n",
				"example_messages = prompt.invoke(\n",
				"  {\n",
				"    \"context\": \"filter context\",\n",
				"    \"question\": \"filter question\"\n",
				"  }\n",
				").to_messages()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
				"  print(chunk, end=\"\", flush=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Add Source"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Adding sources\n",
				"rag_chain_from_dos = (\n",
				"  RunnablePassthrough.assign(context=(lambda x: utils.format_docs(x[\"context\"])))\n",
				"  | prompt\n",
				"  | chat\n",
				"  | output_parsers.str_output_parser()\n",
				")\n",
				"rag_chain_with_source = RunnableParallel(\n",
				"  {\n",
				"    \"context\": qdrant_instance.retriever,\n",
				"    \"question\": RunnablePassthrough()\n",
				"  }\n",
				").assign(answer=rag_chain_from_dos)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"rag_chain_with_source.invoke(\"What is Task Decomposition?\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Add chat history"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"contextualize_q_chain = (\n",
				"  prompts.contextualize_q_prompt \n",
				"  | chat\n",
				"  | output_parsers.str_output_parser()\n",
				")\n",
				"\n",
				"def contextualized_question(input: dict):\n",
				"  if input.get(\"chat_history\"):\n",
				"    return contextualize_q_chain\n",
				"  else:\n",
				"    return input[\"question\"]\n",
				"\n",
				"rag_chain = (\n",
				"  RunnablePassthrough.assign(\n",
				"    context=contextualized_question | qdrant_instance.retriever | utils.format_docs\n",
				"  )\n",
				"  | prompts.qa_prompt\n",
				"  | chat\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"chat_history = []\n",
				"\n",
				"questions = [\n",
				"  \"What is Task Decomposition?\",\n",
				"  \"What are common ways of doing it?\"\n",
				"]\n",
				"\n",
				"for question in questions:\n",
				"  ai_msg = rag_chain.invoke({\n",
				"    \"question\": question, \"chat_history\": chat_history\n",
				"  })\n",
				"  chat_history.extend([HumanMessage(content=question), ai_msg])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Streaming"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for chunk in rag_chain.stream({\n",
				"  \"question\": \"What is Task Decomposition\", \"chat_history\": []\n",
				"}):\n",
				"  print(chunk.content, flush=True, end='')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Per-User Retrieval"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Citations"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Use Agents"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"tools = [\n",
				"  qdrant_instance.retriever_tool\n",
				"]\n",
				"\n",
				"agent_prompt = prompts.prompt_agent_openai_tools\n",
				"\n",
				"agent = agents.create_openai_tools_agent(\n",
				"  llm=models.chat_openai,\n",
				"  tools=tools,\n",
				"  prompt=agent_prompt,\n",
				")\n",
				"agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_executor.invoke({\"input\": \"hi, i am Bob\"})"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Use Local Models"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tutorials"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Build a RAG App](https://python.langchain.com/v0.2/docs/tutorials/rag/)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"llm = models.chat_openai"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"\tbs_kwargs=dict(\n",
				"\t\tparse_only=bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t),\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=1000, chunk_overlap=200,\n",
				")\n",
				"splits = splitter.split_documents(docs)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"vectorstore = stores.chroma.Chroma.from_documents(\n",
				"\tdocuments=splits, embedding=text_embedding_models.OpenAIEmbeddings(),\n",
				")\n",
				"retriever = vectorstore.as_retriever(\n",
				"\tsearch_type=\"similarity\",\n",
				"\tsearch_kwargs={\n",
				"\t\t\"k\": 6,\n",
				"\t}\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompt_tpl_filter_context = \"\"\"\\\n",
				"You are an AI assistant tasked with filtering a list of retrieved text chunks to only the most relevant ones for answering a given question.\n",
				"\n",
				"Here is the question:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the retrieved text chunks:\n",
				"<retrieved_chunks>\n",
				"{context}\n",
				"</retrieved_chunks>\n",
				"\n",
				"Carefully analyze each chunk for how relevant it is to answering the question. Consider the following:\n",
				"- Does the chunk contain information that helps answer the question?\n",
				"- Does the chunk provide important context or background for the question?\n",
				"- Is the chunk focused on the key concepts and entities mentioned in the question?\n",
				"\n",
				"Create a Python list containing only the most relevant chunks. The list should be a subset of the original retrieved_chunks list. Omit any chunks that are not directly helpful for answering the question. \n",
				"\n",
				"Output this filtered list of the most relevant chunks. The format should be a valid Python list, like:\n",
				"['chunk 1 text', \n",
				"'chunk 2 text',\n",
				"'chunk 3 text']\n",
				"\n",
				"Remember, the goal is to create a focused list of only the most relevant chunks for answering the original question. Do not include any irrelevant or tangential chunks in your final result list.\n",
				"\"\"\"\n",
				"prompt_filter_context = prompts.ChatPromptTemplate.from_template(prompt_tpl_filter_context)\n",
				"\n",
				"prompt_tpl_rag = \"\"\"\\\n",
				"Here is the question to answer:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the relevant pieces of context that may help answer the question:\n",
				"<context>\n",
				"{context_filtered}\n",
				"</context>\n",
				"\n",
				"Carefully read the question and context. Think through how the context can be used to answer the question in the <scratchpad> area below:\n",
				"\n",
				"Now provide your final answer to the question. If the question cannot be answered based on the provided context, simply say \"I don't know.\" Keep your answer to 3 sentences maximum and prioritize conciseness.\n",
				"\n",
				"Helpful Answer:\\\n",
				"\"\"\"\n",
				"prompt_rag = prompts.ChatPromptTemplate.from_template(prompt_tpl_rag)\n",
				"\n",
				"def format_docs_to_str(docs: list[document_loaders.Document]):\n",
				"\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
				"\n",
				"def format_docs_to_list(docs: list[document_loaders.Document]):\n",
				"\treturn [doc.page_content for doc in docs]\n",
				"\n",
				"chain_rag = runnables.RunnableParallel(\n",
				"\t{\n",
				"\t\t\"question\": runnables.RunnablePassthrough(),\n",
				"\t\t\"context\": retriever | format_docs_to_list,\n",
				"\t}\n",
				")\\\n",
				"  .assign(context_filtered=(\n",
				"\t\tprompt_filter_context | llm | output_parsers.StrOutputParser() | ast.literal_eval\n",
				"  )).with_retry()\\\n",
				"  .assign(output=(\n",
				"\t\tprompt_rag\t| llm\t| output_parsers.StrOutputParser()\n",
				"\t)).with_retry()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = chain_rag.invoke(\n",
				"  \"What is Task Decomposition?\"\n",
				")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [RAG From Scratch](https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&si=dE6TOhGs5KMC1zc7)\n",
				"\n",
				"[Git](https://github.com/langchain-ai/rag-from-scratch/tree/main)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.OpenAIEmbeddings()\n",
				"llm = models.chat_openai"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Basic Flow\n",
				"\n",
				"- [Indexing](https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&si=dE6TOhGs5KMC1zc7), [Slide](https://docs.google.com/presentation/d/1MhsCqZs7wTX6P19TFnA9qRSlxH3u-1-0gWkhBiDG9lQ/edit#slide=id.p)\n",
				"- [Retrieval](https://youtu.be/LxNVgdIz9sU?si=rmu8kYV1BH_hwEvo), [Slide](https://docs.google.com/presentation/d/124I8jlBRCbb0LAUhdmDwbn4nREqxSxZU1RF_eTGXUGc/edit#slide=id.g267060cc54f_0_0)\n",
				"- [Generation](https://youtu.be/Vw52xyyFsB8?si=pQqUluFZUrxTnwZP), [Slide](https://docs.google.com/presentation/d/1eRJwzbdSv71e9Ou9yeqziZrz1UagwX8B1kL4TbL5_Gc/edit#slide=id.g2b46f2cb556_0_0)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"\tbs_kwargs=dict(\n",
				"\t\tparse_only=bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t),\n",
				")\n",
				"doc = loader.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=300, chunk_overlap=50,\n",
				")\n",
				"docs = text_splitter.split_documents(doc)\n",
				"\n",
				"vectorstore = stores.chroma.Chroma.from_documents(docs, embeddings)\n",
				"retriever = vectorstore.as_retriever()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"template = \"\"\"\\\n",
				"Answer the question based only on the following context:\n",
				"{context}\n",
				"\n",
				"Question: {question}\n",
				"\"\"\"\n",
				"prompt = prompts.ChatPromptTemplate.from_template(template)\n",
				"\n",
				"chain_rag: runnables.Runnable = (\n",
				"\t{\n",
				"\t\t\"context\": retriever,\n",
				"\t\t\"question\": runnables.RunnablePassthrough()\n",
				"\t}\n",
				"\t| prompt\n",
				"\t| llm\n",
				"\t| output_parsers.StrOutputParser()\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"chain_rag.invoke(\"What is Task Decomposition?\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Query Translation\n",
				"\n",
				"Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Multi Query](https://youtu.be/JChPi0CRnDY?si=wEgjcc0NHINTvQVh), [Slide](https://docs.google.com/presentation/d/15pWydIszbQG3Ipur9COfTduutTZm6ULdkkyX-MNry8I/edit#slide=id.g268cd4ba153_0_0), [LangChain](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever/)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [RAG Fusion](https://youtu.be/77qELPbNgxA?si=uyfzuemn02ktS2xe), [Slide](https://docs.google.com/presentation/d/1EwykmdVSQqlh6XpGt8APOMYp4q1CZqqeclAx61pUcjI/edit#slide=id.g268cfa48f45_0_0), [LangChain](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker/)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Decomposition](https://youtu.be/h0OPWlEOank?si=jU3DecxsmxDWi9az), [Slide](https://docs.google.com/presentation/d/1O97KYrsmYEmhpQ6nkvOVAqQYMJvIaZulGFGmz4cuuVE/edit#slide=id.g268fdc1fda2_0_0)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Step Back](https://youtu.be/xn1jEjRyJ2U?si=63WfDLTQwBKmUsdW), [Slide](https://docs.google.com/presentation/d/1L0MRGVDxYA1eLOR0L_6Ze1l2YV8AhN1QKUtmNA-fJlU/edit#slide=id.g268cfa65240_0_0)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [HyDE](https://youtu.be/SaDzIVkYqyY?si=7tFx5bpTiBpy5KkV), [Slide](https://docs.google.com/presentation/d/10MmB_QEiS4m00xdyu-92muY-8jC3CdaMpMXbXjzQXsM/edit#slide=id.g2b872e9a17e_0_0)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Routing](https://youtu.be/pfpIndq7Fi8?si=m6SerpLuJdKzIV6A), [Slide](https://docs.google.com/presentation/d/1kC6jFj8C_1ZXDYcFaJ8vhJvCYEwxwsVqk2VVeKKuyx4/edit#slide=id.g26bc3116f45_0_0)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### [Query Structuring](https://youtu.be/kl6NwWYxvbM?si=Vm0MiQL13kI0nr-Q), [Blog](https://blog.langchain.dev/query-construction/)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Indexing"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [Multi-Representation Indexing](https://youtu.be/gTCU9I6QqCE?si=jQ3Aj9ko3DYVQ1vU), [Slide](https://blog.langchain.dev/semi-structured-multi-modal-rag/)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### [RAPTOR](https://youtu.be/z_6EeA2LDSw?si=E09-N68W93TgBNBC), [Code](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)\n",
				"\n",
				"### [ColBERT](https://youtu.be/cN6S0Ehm7_8?si=LGBLo-VUonJMXnmR)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"llm = models.chat_openai"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
				"\tbs_kwargs=dict(\n",
				"\t\tparse_only=bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t),\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=1000, chunk_overlap=200,\n",
				")\n",
				"splits = splitter.split_documents(docs)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"vectorstore = stores.chroma.Chroma.from_documents(\n",
				"\tdocuments=splits, embedding=text_embedding_models.OpenAIEmbeddings(),\n",
				")\n",
				"\n",
				"retriever = vectorstore.as_retriever(\n",
				"\tsearch_type=\"similarity\",\n",
				"\tsearch_kwargs={\n",
				"\t\t\"k\": 6,\n",
				"\t}\n",
				")\n",
				"\n",
				"my_retriever = stores.create_retriever(\n",
				"\tvectorstore=vectorstore,\n",
				"\tllm=llm,\n",
				"\tretriever_types=['base', 'MultiQueryRetriever', 'RePhraseQueryRetriever'],\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompt_tpl_filter_context = \"\"\"\\\n",
				"You are an AI assistant tasked with filtering a list of retrieved text chunks to only the most relevant ones for answering a given question.\n",
				"\n",
				"Here is the question:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the retrieved text chunks:\n",
				"<retrieved_chunks>\n",
				"{context}\n",
				"</retrieved_chunks>\n",
				"\n",
				"Carefully analyze each chunk for how relevant it is to answering the question. Consider the following:\n",
				"- Does the chunk contain information that helps answer the question?\n",
				"- Does the chunk provide important context or background for the question?\n",
				"- Is the chunk focused on the key concepts and entities mentioned in the question?\n",
				"\n",
				"Create a Python list containing only the most relevant chunks. The list should be a subset of the original retrieved_chunks list. Omit any chunks that are not directly helpful for answering the question. \n",
				"\n",
				"Output this filtered list of the most relevant chunks. The format should be a valid Python list, like:\n",
				"['chunk 1 text', \n",
				"'chunk 2 text',\n",
				"'chunk 3 text']\n",
				"\n",
				"Remember, the goal is to create a focused list of only the most relevant chunks for answering the original question. Do not include any irrelevant or tangential chunks in your final result list.\n",
				"\"\"\"\n",
				"prompt_filter_context = prompts.ChatPromptTemplate.from_template(prompt_tpl_filter_context)\n",
				"\n",
				"prompt_tpl_rag = \"\"\"\\\n",
				"Here is the question to answer:\n",
				"<question>\n",
				"{question}\n",
				"</question>\n",
				"\n",
				"And here are the relevant pieces of context that may help answer the question:\n",
				"<context>\n",
				"{context_filtered}\n",
				"</context>\n",
				"\n",
				"Carefully read the question and context. Think through how the context can be used to answer the question. If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\".\n",
				"\n",
				"Provide your final answer to the question. If the question cannot be answered based on the provided context, simply say \"I don't know.\" Keep your answer to 3 sentences maximum and prioritize conciseness.\n",
				"\n",
				"Helpful Answer:\\\n",
				"\"\"\"\n",
				"prompt_rag = prompts.ChatPromptTemplate.from_template(prompt_tpl_rag)\n",
				"\n",
				"def format_docs_to_str(docs: list[document_loaders.Document]):\n",
				"\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
				"\n",
				"def format_docs_to_list(docs: list[document_loaders.Document]):\n",
				"\treturn [doc.page_content for doc in docs]\n",
				"\n",
				"chain_rag = runnables.RunnableParallel(\n",
				"\t{\n",
				"\t\t\"question\": runnables.RunnablePassthrough(),\n",
				"\t\t\"context\": my_retriever | format_docs_to_list,\n",
				"\t}\n",
				")\\\n",
				"  .assign(context_filtered=(\n",
				"\t\tprompt_filter_context | llm | output_parsers.StrOutputParser() | ast.literal_eval\n",
				"  )).with_retry()\\\n",
				"  .assign(output=(\n",
				"\t\tprompt_rag\t| llm\t| output_parsers.StrOutputParser()\n",
				"\t)).with_retry()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"Ellipsis"
						]
					},
					"execution_count": 14,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"class InputChainRag(BaseModel):\n",
				"\tquestion: str = Field(description=\"user question, natural language, NOT sql query\")\n",
				"\n",
				"class MyRagChain:\n",
				"\tdef __init__(\n",
				"\t\tself,\n",
				"\t\tretriever: BaseRetriever,\n",
				"\t\tis_debug: bool = False,\n",
				"\t\tjust_return_ctx: bool = False,\n",
				"  \n",
				"\t\ttool_name: str = None,\n",
				"\t\ttool_description: str = None,\n",
				"\t\ttool_metadata: Optional[Dict[str, Any]] = None,\n",
				"\t\ttool_tags: Optional[List[str]] = None,\n",
				"  ) -> None:\n",
				"   \n",
				"\t\tself.is_debug = is_debug\n",
				"\t\tself.just_return_ctx = just_return_ctx\n",
				"  \n",
				"\t\tself.tool_name = tool_name\n",
				"\t\tself.tool_description = tool_description\n",
				"\t\tself.tool_metadata = tool_metadata\n",
				"\t\tself.tool_tags = tool_tags\n",
				"  \n",
				"\t\tself.prompt_tpl_filter_context = \"\"\"\\\n",
				"\t\tYou are an AI assistant tasked with filtering a list of retrieved text chunks to only the most relevant ones for answering a given question.\n",
				"\n",
				"\t\tHere is the question:\n",
				"\t\t<question>\n",
				"\t\t{question}\n",
				"\t\t</question>\n",
				"\n",
				"\t\tAnd here are the retrieved text chunks:\n",
				"\t\t<retrieved_chunks>\n",
				"\t\t{context}\n",
				"\t\t</retrieved_chunks>\n",
				"\n",
				"\t\tCarefully analyze each chunk for how relevant it is to answering the question. Consider the following:\n",
				"\t\t- Does the chunk contain information that helps answer the question?\n",
				"\t\t- Does the chunk provide important context or background for the question?\n",
				"\t\t- Is the chunk focused on the key concepts and entities mentioned in the question?\n",
				"\n",
				"\t\tCreate a Python list containing only the most relevant chunks. The list should be a subset of the original retrieved_chunks list. Omit any chunks that are not directly helpful for answering the question. \n",
				"\n",
				"\t\tOutput this filtered list of the most relevant chunks. The format should be a valid Python list, like:\n",
				"\t\t['chunk 1 text', \n",
				"\t\t'chunk 2 text',\n",
				"\t\t'chunk 3 text']\n",
				"\n",
				"\t\tRemember, the goal is to create a focused list of only the most relevant chunks for answering the original question. Do not include any irrelevant or tangential chunks in your final result list.\n",
				"\t\t\"\"\"\n",
				"\t\tself.prompt_filter_context = prompts.ChatPromptTemplate.from_template(self.prompt_tpl_filter_context)\n",
				"\n",
				"\t\tself.prompt_tpl_rag = \"\"\"\\\n",
				"\t\tHere is the question to answer:\n",
				"\t\t<question>\n",
				"\t\t{question}\n",
				"\t\t</question>\n",
				"\n",
				"\t\tAnd here are the relevant pieces of context that may help answer the question:\n",
				"\t\t<context>\n",
				"\t\t{context_filtered}\n",
				"\t\t</context>\n",
				"\n",
				"\t\tCarefully read the question and context. Think through how the context can be used to answer the question. If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\".\n",
				"\n",
				"\t\tProvide your final answer to the question. If the question cannot be answered based on the provided context, simply say \"I don't know.\" Keep your answer to 3 sentences maximum and prioritize conciseness.\n",
				"\n",
				"\t\tHelpful Answer:\\\n",
				"\t\t\"\"\"\n",
				"\t\tself.prompt_rag = prompts.ChatPromptTemplate.from_template(self.prompt_tpl_rag)\n",
				"\n",
				"\t\tself.retriever = retriever\n",
				"  \n",
				"\t\tself.chain_rag = runnables.RunnableParallel(\n",
				"\t\t\t{\n",
				"\t\t\t\t\"question\": runnables.RunnablePassthrough(),\n",
				"\t\t\t\t\"context\": self.retriever | self.format_docs_to_list,\n",
				"\t\t\t}\n",
				"\t\t)\\\n",
				"\t\t\t.assign(context_filtered=(\n",
				"\t\t\t\tself.prompt_filter_context | llm | output_parsers.StrOutputParser() | ast.literal_eval\n",
				"\t\t\t)).with_retry()\n",
				"\t\tif self.just_return_ctx:\n",
				"\t\t\tself.chain_rag.assign(result=(\n",
				"\t\t\t\tself.prompt_rag\t| llm\t| output_parsers.StrOutputParser()\n",
				"\t\t\t)).with_retry()\n",
				"\t\t\n",
				"\t\tself.metadata_chain_rag = {\"is_my_rag_chain_run\": True}\n",
				"\n",
				"\tdef format_docs_to_str(self, docs: list[document_loaders.Document]):\n",
				"\t\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
				"\n",
				"\tdef format_docs_to_list(self, docs: list[document_loaders.Document]):\n",
				"\t\treturn [doc.page_content for doc in docs]\n",
				"\n",
				"\tdef prepare_chain_input(self, question: str):\n",
				"\t\tresult = question\n",
				"\t\treturn result\n",
				"\t\n",
				"\tdef invoke_chain(self, question: str) -> Union[str, dict]:\n",
				"\t\t\"\"\"Get natural user question, turn it into SQL query and execute\"\"\"\n",
				"\t\t# question = self.prepare_chain_input(question)\n",
				"\t\tresult = self.chain_rag.invoke(\n",
				"\t\t\tquestion,\n",
				"\t\t\tconfig={\"metadata\": self.metadata_chain_rag},\n",
				"\t\t)\n",
				"\t\t\n",
				"\t\tif not self.is_debug and self.just_return_ctx:\n",
				"\t\t\tresult = result[\"context_filtered\"]\n",
				"\n",
				"\t\treturn result\n",
				"\t\n",
				"\tasync def ainvoke_chain(self, question: str) -> Union[str, dict]:\n",
				"\t\t\"\"\"Get natural user question, turn it into SQL query and execute\"\"\"\n",
				"\t\t# question = self.prepare_chain_input(question)\n",
				"\t\tresult = await self.chain_rag.ainvoke(\n",
				"\t\t\tquestion,\n",
				"\t\t\tconfig={\"metadata\": self.metadata_chain_rag},\n",
				"\t\t)\n",
				"\t\t\n",
				"\t\tif not self.is_debug and self.just_return_ctx:\n",
				"\t\t\tresult = result[\"context_filtered\"]\n",
				"\n",
				"\t\treturn result\n",
				"\n",
				"\tdef create_tool_chain_rag(\n",
				"\t\tself,\n",
				"\t\tfunc: Callable = None,\n",
				"\t\targs_schema: Type[BaseModel] = None,\n",
				"\t\tcoroutine: Optional[Callable[..., Awaitable[Any]]] = None,\n",
				"\t\tname: str = None,\n",
				"\t\tdescription: str = None,\n",
				"\t\treturn_direct: bool = False, # True: Agent will stop after tool completed\n",
				"\t\thandle_tool_error: Optional[Union[bool, str, Callable[[ToolException], str]]] = True,\n",
				"\t\thandle_validation_error: Optional[Union[bool, str, Callable[[ValidationError], str]]] = True,\n",
				"\t\tverbose: bool = False,\n",
				"\t\tmetadata: Optional[Dict[str, Any]] = None,\n",
				"\t\ttags: Optional[List[str]] = None,\n",
				"\t):\n",
				"\t\tfunc = self.invoke_chain if func is None else func\n",
				"\t\targs_schema = InputChainRag if args_schema is None else args_schema\n",
				"\t\tcoroutine = self.ainvoke_chain if coroutine is None else coroutine\n",
				"\t\t\n",
				"\t\tname = self.tool_name if name is None else name\n",
				"\t\tdescription = self.tool_description if description is None else description\n",
				"\t\tmetadata = self.tool_metadata if metadata is None else metadata\n",
				"\t\ttags = self.tool_tags if tags is None else tags\n",
				"\t\t\n",
				"\t\ttool_chain_rag = StructuredTool.from_function(\n",
				"\t\t\tfunc=func,\n",
				"\t\t\targs_schema=args_schema,\n",
				"\t\t\tcoroutine=coroutine,\n",
				"\t\t\tname=name,\n",
				"\t\t\tdescription=description,\n",
				"\t\t\treturn_direct=return_direct,\n",
				"\t\t\thandle_tool_error=handle_tool_error,\n",
				"\t\t\thandle_validation_error=handle_validation_error,\n",
				"\t\t\tverbose=verbose,\n",
				"\t\t\tmetadata=metadata,\n",
				"\t\t\ttags=tags,\n",
				"\t\t)\n",
				"\n",
				"\t\treturn tool_chain_rag\n",
				"\n",
				"..."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"metadata": {},
			"outputs": [],
			"source": [
				"my_rag_chain = MyRagChain(\n",
				"  retriever=my_retriever,\n",
				"  is_debug=False,\n",
				"\tjust_return_ctx=True,\n",
				"\ttool_name=None,\n",
				"\ttool_description=None,\n",
				"\ttool_metadata=None,\n",
				"\ttool_tags=None,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = my_rag_chain.chain_rag.invoke(\n",
				"  \"What is Task Decomposition?\"\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = my_rag_chain.invoke_chain(\n",
				"  question=\"What is Task Decomposition?\"\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 18,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"2\n"
					]
				}
			],
			"source": [
				"pprint(len(result))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.9"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
