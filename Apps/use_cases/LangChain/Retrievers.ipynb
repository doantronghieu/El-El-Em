{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import add_packages\n",
    "import dotenv, yaml, os\n",
    "from pprint import pprint\n",
    "\n",
    "from my_langchain import (\n",
    "  retrievers, vectorstores, document_loaders, text_splitters, text_embedding_models,\n",
    "  chat_models, chains, documents\n",
    ")\n",
    "from my_configs import constants\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOpenAIEmbeddings(text_embedding_models.OpenAIEmbeddings):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        super().__init__(openai_api_key=openai_api_key, *args, **kwargs)\n",
    "\n",
    "    def _embed_documents(self, texts):\n",
    "        embeddings = [\n",
    "            self.client.create(\n",
    "                input=text, model=\"text-embedding-ada-002\").data[0].embedding\n",
    "            for text in texts\n",
    "        ]\n",
    "        return embeddings\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self._embed_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store-backed retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{add_packages.APP_PATH}/data/movies.yaml', 'r') as file:\n",
    "  data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc_data in data['docs']:\n",
    "  doc = documents.Document(\n",
    "    page_content=doc_data['page_content'], metadata=doc_data['metadata']\n",
    "  )\n",
    "  docs.append(doc)\n",
    "\n",
    "# Recreate metadata_field_info list\n",
    "metadata_field_info = []\n",
    "for info_data in data['metadata_field_info']:\n",
    "  info = chains.AttributeInfo(\n",
    "    name=info_data['name'], description=info_data['description'], \n",
    "    type=info_data['type']\n",
    "  )\n",
    "  metadata_field_info.append(info)\n",
    "\n",
    "document_content_description = data[\"document_content_description\"]\n",
    "\n",
    "embeddings = CustomOpenAIEmbeddings()\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "vectorstore = vectorstores.chroma.Chroma.from_documents(docs, embeddings)\n",
    "retriever = retrievers.SelfQueryRetriever.from_llm(\n",
    "  llm=llm,\n",
    "  vectorstore=vectorstore,\n",
    "  document_contents=document_content_description,\n",
    "  metadata_field_info=metadata_field_info,\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# doc = document_loaders.TextLoader(f\"{add_packages.APP_PATH}/data/state_of_the_union.txt\").load()\n",
    "# text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
    "#   chunk_size=500, chunk_overlap=100,\n",
    "# )\n",
    "# docs = text_splitter.split_documents(doc)\n",
    "# embeddings = text_embedding_models.CohereEmbeddings(\n",
    "#   model=constants.EMBEDDINGS[\"COHERE\"][\"EMBED-ENGLISH-V2.0\"]\n",
    "# )\n",
    "# retriever = vectorstores.faiss.FAISS.from_documents(docs, embeddings).as_retriever(\n",
    "#   search_type=\"mmr\",\n",
    "#   search_kwargs={\n",
    "#     \"k\": 10,\n",
    "#   }\n",
    "# )\n",
    "\n",
    "# llm = chat_models.chat_openai\n",
    "\n",
    "# query = \"What did the president say about Ketanji Jackson Brown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MultiQueryRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever)\n",
    "\n",
    "Distance-based vector database retrieval embeds queries in high-dimensional space to find similar embedded documents based on distance. Retrieval results may vary with slight changes in query wording or inadequate semantics captured by the embeddings. Manual prompt engineering or tuning is often used to address these issues, but it can be laborious.\n",
    "\n",
    "The MultiQueryRetriever automates prompt tuning using an LLM to generate multiple queries from various perspectives. It retrieves relevant documents for each query and combines them to get a larger set of potentially relevant documents. Generating multiple perspectives can overcome limitations of distance-based retrieval and provide richer results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple usage\n",
    "\n",
    "Specify LLM for query generation, retriever will handle the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multi_query = retrievers.MultiQueryRetriever.from_llm(\n",
    "  retriever=retriever, llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_multi_query.get_relevant_documents(query)\n",
    "pprint(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Supplying your own prompt\n",
    "\n",
    "Supply a prompt with an output parser to split results into a list of queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Contextual compression](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression)\n",
    "\n",
    "One challenge with retrieval is not knowing the specific queries your document storage system will face when ingesting data. This can result in relevant information being buried in a document with irrelevant text, leading to costly LLM calls and poor responses.\n",
    "\n",
    "Contextual compression compresses retrieved documents based on the query context to only return relevant information.\n",
    "\n",
    "To use the Contextual Compression Retriever, you need a base retriever and a Document Compressor.\n",
    "\n",
    "The Contextual Compression Retriever sends queries to the base retriever, which then processes the initial documents through the Document Compressor to shorten the list by reducing or dropping content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Contextual compression enhancement with LLMChainExtractor\n",
    "\n",
    "Wrap base retriever with ContextualCompressionRetriever. Add LLMChainExtractor to iterate over returned documents and extract relevant content for query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = retrievers.LLMChainExtractor.from_llm(llm)\n",
    "retriever_compression = retrievers.ContextualCompressionRetriever(\n",
    "  base_compressor=compressor, base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='nominated Circuit Court of Appeals Judge Ketanji Brown Jackson.', metadata={'source': '/Users/thung/Documents/Me/Coding/Learn-LLM/Apps/data/state_of_the_union.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs_compressed = retriever_compression.get_relevant_documents(query)\n",
    "pprint(docs_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## More built-in compressors: filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LLMChainFilter\n",
    "\n",
    "LLMChainFilter: Simple yet robust compressor using LLM chain to filter out documents and return others without altering content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = retrievers.LLMChainFilter.from_llm(llm)\n",
    "retriever_compression = retrievers.ContextualCompressionRetriever(\n",
    "  base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '/Users/thung/Documents/Me/Coding/Learn-LLM/Apps/data/state_of_the_union.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs_compressed = retriever_compression.get_relevant_documents(query)\n",
    "pprint(docs_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### EmbeddingsFilter\n",
    "\n",
    "Making an extra LLM call for each document is costly and slow. The EmbeddingsFilter embedds the documents and query, only returning documents with similar embeddings to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = retrievers.EmbeddingsFilter(\n",
    "  embeddings=embeddings, similarity_threshold=0.76,\n",
    ")\n",
    "retriever_compression = retrievers.ContextualCompressionRetriever(\n",
    "  base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "docs_compressed = retriever_compression.get_relevant_documents(query)\n",
    "pprint(docs_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stringing compressors and document transformers together\n",
    "\n",
    "Using the DocumentCompressorPipeline allows combining multiple compressors in sequence. BaseDocumentTransformers can also be added to the pipeline, performing transformations on a set of documents. For instance, TextSplitters split documents into smaller pieces, while EmbeddingsRedundantFilter filters out redundant documents based on embedding similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_embeddings_redundant = retrievers.EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "filter_embeddings_relevant = retrievers.EmbeddingsFilter(\n",
    "  embeddings=embeddings, similarity_threshold=0.76,\n",
    ")\n",
    "filter_llmchain = retrievers.LLMChainFilter.from_llm(llm)\n",
    "extractor_llmchain = retrievers.LLMChainExtractor.from_llm(llm)\n",
    "compressor_pipeline = retrievers.DocumentCompressorPipeline(\n",
    "  transformers=[\n",
    "    # filter_embeddings_redundant, \n",
    "    # filter_embeddings_relevant,\n",
    "    filter_llmchain,\n",
    "    extractor_llmchain,\n",
    "  ]\n",
    ")\n",
    "\n",
    "retriever_compression = retrievers.ContextualCompressionRetriever(\n",
    "  base_compressor=compressor_pipeline, base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='nominated Circuit Court of Appeals Judge Ketanji Brown Jackson.', metadata={'source': '/Users/thung/Documents/Me/Coding/Learn-LLM/Apps/data/state_of_the_union.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs_compressed = retriever_compression.get_relevant_documents(query)\n",
    "pprint(docs_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = chat_models.ChatCohere(\n",
    "#   model=constants.MODELS[\"COHERE\"][\"COMMAND\"]\n",
    "# )\n",
    "llm = chat_models.chat_openai\n",
    "compressor = retrievers.CohereRerank()\n",
    "retriever_compression = retrievers.ContextualCompressionRetriever(\n",
    "  base_compressor=compressor, base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_compressed = retriever_compression.get_relevant_documents(query)\n",
    "pprint(docs_compressed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
