{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"True"
						]
					},
					"execution_count": 1,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"import add_packages\n",
				"import dotenv, yaml, os, logging\n",
				"from pprint import pprint\n",
				"\n",
				"from toolkit.langchain import (\n",
				"  retrievers, vectorstores, document_loaders, text_splitters, text_embedding_models,\n",
				"  chat_models, chains, documents\n",
				")\n",
				"from my_configs import constants\n",
				"\n",
				"dotenv.load_dotenv()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Function"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"llm = chat_models.chat_openai\n",
				"embeddings = text_embedding_models.OpenAIEmbeddings()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Docs"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"with open(f'{add_packages.APP_PATH}/data/movies.yaml', 'r') as file:\n",
				"  data = yaml.safe_load(file)\n",
				"\n",
				"docs = []\n",
				"for doc_data in data['docs']:\n",
				"  doc = documents.Document(\n",
				"    page_content=doc_data['page_content'], metadata=doc_data['metadata']\n",
				"  )\n",
				"  docs.append(doc)\n",
				"\n",
				"# Recreate metadata_field_info list\n",
				"metadata_field_info = []\n",
				"for info_data in data['metadata_field_info']:\n",
				"  info = chains.AttributeInfo(\n",
				"    name=info_data['name'], description=info_data['description'], \n",
				"    type=info_data['type']\n",
				"  )\n",
				"  metadata_field_info.append(info)\n",
				"\n",
				"document_content_description = data[\"document_content_description\"]\n",
				"\n",
				"vectorstore = vectorstores.chroma.Chroma.from_documents(docs, embeddings)\n",
				"\n",
				"queries = [\n",
				"  \"I want to watch a movie rated higher than 8.5\",\n",
				"  \"Has Greta Gerwig directed any movies about women\",\n",
				"  \"What's a highly rated (above 8.5) science fiction film?\",\n",
				"  \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"doc = document_loaders.TextLoader(f\"{add_packages.APP_PATH}/data/state_of_the_union.txt\").load()\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=500, chunk_overlap=100,\n",
				")\n",
				"docs = text_splitter.split_documents(doc)\n",
				"\n",
				"query = \"What did the president say about Ketanji Jackson Brown\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"docs = [\n",
				"  documents.Document(page_content=\"foo\"),\n",
				"  documents.Document(page_content=\"bar\"),\n",
				"  documents.Document(page_content=\"world\"),\n",
				"  documents.Document(page_content=\"hello\"),\n",
				"  documents.Document(page_content=\"foo bar\"),\n",
				"  documents.Document(page_content=\"Langchain supports cohere RAG!\"),\n",
				"  documents.Document(page_content=\"The sky is blue!\"),\n",
				"]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.CustomOpenAIEmbeddings()\n",
				"\n",
				"loader = document_loaders.WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
				"doc = loader.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"  chunk_size=500, chunk_overlap=0,\n",
				")\n",
				"docs = text_splitter.split_documents(doc)\n",
				"\n",
				"vectorstore = vectorstores.chroma.Chroma.from_documents(docs, embedding=embeddings)\n",
				"\n",
				"queries = [\n",
				"  \"Hi I'm Lance. What are the approaches to Task Decomposition?\",\n",
				"  \"I live in San Francisco. What are the Types of Memory?\",\n",
				"]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Vector store-backed retriever"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.CustomOpenAIEmbeddings()\n",
				"# embeddings = text_embedding_models.CohereEmbeddings(\n",
				"#   model=constants.EMBEDDINGS[\"COHERE\"][\"EMBED-ENGLISH-V2.0\"]\n",
				"# )\n",
				"\n",
				"llm = chat_models.chat_openai\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [Self-querying Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query)\n",
				"\n",
				"A self-querying retriever can query itself by using a query-constructing LLM chain to write a structured query and applying it to its VectorStore. This allows the retriever to compare user-input queries with stored documents for semantic similarity and extract filters from user queries to execute on document metadata.\n",
				"\n",
				"![tmp](https://python.langchain.com/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Creating\n",
				"\n",
				"Instantiate the retriever by providing information on metadata fields and document contents.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"retriever = retrievers.SelfQueryRetriever.from_llm(\n",
				"  llm=llm,\n",
				"  vectorstore=vectorstore,\n",
				"  document_contents=document_content_description,\n",
				"  metadata_field_info=metadata_field_info,\n",
				"  verbose=True,\n",
				"  # structured_query_translator=None,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"retriever.get_relevant_documents(queries[2])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Filter k\n",
				"\n",
				"Use the self query retriever to specify the number of documents to fetch by passing enable_limit=True to the constructor.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Building from the ground up with LCEL\n",
				"\n",
				"Reconstruct retriever from scratch for custom control and insight into operations.\n",
				"\n",
				"Create a query-construction chain to generate a StructuredQuery object from a user query. Helper functions are available for prompt creation and output parsing with tunable parameters.\n",
				"\n",
				"The query constructor is crucial for the self-query retriever. To create an effective retrieval system, ensure the query constructor is optimized by adjusting prompts, examples, and attribute descriptions. Refer to this cookbook for a detailed example using hotel inventory data.\n",
				"\n",
				"The key element is the structured query translator, responsible for translating the StructuredQuery object into a metadata filter in the syntax of the vector store. LangChain has built-in translators available in the Integrations section.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [MultiQueryRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever)\n",
				"\n",
				"Distance-based vector database retrieval embeds queries in high-dimensional space to find similar embedded documents based on distance. Retrieval results may vary with slight changes in query wording or inadequate semantics captured by the embeddings. Manual prompt engineering or tuning is often used to address these issues, but it can be laborious.\n",
				"\n",
				"The MultiQueryRetriever automates prompt tuning using an LLM to generate multiple queries from various perspectives. It retrieves relevant documents for each query and combines them to get a larger set of potentially relevant documents. Generating multiple perspectives can overcome limitations of distance-based retrieval and provide richer results.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Simple usage\n",
				"\n",
				"Specify LLM for query generation, retriever will handle the rest.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"retriever_multi_query = retrievers.MultiQueryRetriever.from_llm(\n",
				"  retriever=retriever, llm=llm,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"unique_docs = retriever_multi_query.get_relevant_documents(query)\n",
				"pprint(unique_docs)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Supplying your own prompt\n",
				"\n",
				"Supply a prompt with an output parser to split results into a list of queries.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [Contextual compression](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression)\n",
				"\n",
				"One challenge with retrieval is not knowing the specific queries your document storage system will face when ingesting data. This can result in relevant information being buried in a document with irrelevant text, leading to costly LLM calls and poor responses.\n",
				"\n",
				"Contextual compression compresses retrieved documents based on the query context to only return relevant information.\n",
				"\n",
				"To use the Contextual Compression Retriever, you need a base retriever and a Document Compressor.\n",
				"\n",
				"The Contextual Compression Retriever sends queries to the base retriever, which then processes the initial documents through the Document Compressor to shorten the list by reducing or dropping content.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Contextual compression enhancement with LLMChainExtractor\n",
				"\n",
				"Wrap base retriever with ContextualCompressionRetriever. Add LLMChainExtractor to iterate over returned documents and extract relevant content for query.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"compressor = retrievers.LLMChainExtractor.from_llm(llm)\n",
				"retriever_compression = retrievers.ContextualCompressionRetriever(\n",
				"  base_compressor=compressor, base_retriever=retriever,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"docs_compressed = retriever_compression.get_relevant_documents(query)\n",
				"pprint(docs_compressed)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## More built-in compressors: filters\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### LLMChainFilter\n",
				"\n",
				"LLMChainFilter: Simple yet robust compressor using LLM chain to filter out documents and return others without altering content.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"compressor = retrievers.LLMChainFilter.from_llm(llm)\n",
				"retriever_compression = retrievers.ContextualCompressionRetriever(\n",
				"  base_compressor=compressor, base_retriever=retriever\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"docs_compressed = retriever_compression.get_relevant_documents(query)\n",
				"pprint(docs_compressed)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### EmbeddingsFilter\n",
				"\n",
				"Making an extra LLM call for each document is costly and slow. The EmbeddingsFilter embedds the documents and query, only returning documents with similar embeddings to the query.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"compressor = retrievers.EmbeddingsFilter(\n",
				"  embeddings=embeddings, similarity_threshold=0.76,\n",
				")\n",
				"retriever_compression = retrievers.ContextualCompressionRetriever(\n",
				"  base_compressor=compressor, base_retriever=retriever\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"docs_compressed = retriever_compression.get_relevant_documents(query)\n",
				"pprint(docs_compressed)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Stringing compressors and document transformers together\n",
				"\n",
				"Using the DocumentCompressorPipeline allows combining multiple compressors in sequence. BaseDocumentTransformers can also be added to the pipeline, performing transformations on a set of documents. For instance, TextSplitters split documents into smaller pieces, while EmbeddingsRedundantFilter filters out redundant documents based on embedding similarity.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"filter_embeddings_redundant = retrievers.EmbeddingsRedundantFilter(embeddings=embeddings)\n",
				"filter_embeddings_relevant = retrievers.EmbeddingsFilter(\n",
				"  embeddings=embeddings, similarity_threshold=0.76,\n",
				")\n",
				"filter_llmchain = retrievers.LLMChainFilter.from_llm(llm)\n",
				"extractor_llmchain = retrievers.LLMChainExtractor.from_llm(llm)\n",
				"compressor_pipeline = retrievers.DocumentCompressorPipeline(\n",
				"  transformers=[\n",
				"    # filter_embeddings_redundant, \n",
				"    # filter_embeddings_relevant,\n",
				"    filter_llmchain,\n",
				"    extractor_llmchain,\n",
				"  ]\n",
				")\n",
				"\n",
				"retriever_compression = retrievers.ContextualCompressionRetriever(\n",
				"  base_compressor=compressor_pipeline, base_retriever=retriever,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"docs_compressed = retriever_compression.get_relevant_documents(query)\n",
				"pprint(docs_compressed)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [Ensemble Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)\n",
				"\n",
				"The EnsembleRetriever combines retrievers' results and reranks them using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
				"\n",
				"EnsembleRetriever achieves better performance by combining strengths of different algorithms.\n",
				"\n",
				"The most common pattern is to combine a sparse retriever with a dense retriever, known as \"hybrid search\". The sparse retriever finds relevant documents based on keywords, while the dense retriever finds relevant documents based on semantic similarity.\n",
				"\n",
				"Configure retrievers at runtime by marking fields as configurable to ensure only one source is returned from the FAISS retriever with the relevant configuration passed in at runtime.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Cohere"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Cohere Reranker"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.CohereEmbeddings(\n",
				"  model=constants.EMBEDDINGS[\"COHERE\"][\"EMBED-MULTILINGUAL-V3.0\"]\n",
				")\n",
				"\n",
				"vectorstore = vectorstores.faiss.FAISS.from_documents(\n",
				"  docs, embeddings\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"retriever = create_retriever(\n",
				"  embeddings=embeddings,\n",
				"  retriever_types=[\n",
				"    \"CohereRerank\",\n",
				"  ],\n",
				"  vectorstore=vectorstore,\n",
				"  # search_kwargs=\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"print(query)\n",
				"retriever.get_relevant_documents(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.CohereEmbeddings(\n",
				"  model=constants.EMBEDDINGS[\"COHERE\"][\"EMBED-MULTILINGUAL-V3.0\"]\n",
				")\n",
				"\n",
				"retriever = create_retriever(\n",
				"  embeddings=embeddings,\n",
				"  retriever_types=[\n",
				"    \"CohereRerank\",\n",
				"  ],\n",
				"  vectorstore=vectorstore,\n",
				"  # search_kwargs=\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [Cohere RAG retriever](https://python.langchain.com/docs/integrations/retrievers/cohere)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"retriever_cohere_rag = retrievers.CohereRagRetriever(\n",
				"  llm=chat_models.ChatCohere(), \n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"retriever_cohere_rag.get_relevant_documents(\"What is cohere ai?\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [BM25](https://python.langchain.com/docs/integrations/retrievers/bm25)\n",
				"\n",
				"BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"retriever = retrievers.BM25Retriever.from_documents(docs)\n",
				"retriever.get_relevant_documents(\"foo\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [LLMLingua](https://python.langchain.com/docs/integrations/retrievers/llmlingua) \n",
				"\n",
				"LLMLingua Document Compressor uses a compact, well-trained language model to identify and remove non-essential tokens in prompts, enabling efficient inference with large language models. Up to 20x compression is achieved with minimal performance loss.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Vectorstore\n",
				"\n",
				"Initialize a simple vector store retriever and store the 2023 State of the Union speech in chunks. Set up the retriever to retrieve a high number of docs (20).\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Compression\n",
				"\n",
				"Wrap base retriever with ContextualCompressionRetriever using LLMLinguaCompressor as compressor.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## QA generation\n",
				"\n",
				"See the result of using this in the generation step now.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [RePhraseQuery](https://python.langchain.com/docs/integrations/retrievers/re_phrase)\n",
				"\n",
				"RePhraseQuery: Simple retriever using LLM between user input and query.\n",
				"\n",
				"Pre-process user input effectively.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Setting up\n",
				"\n",
				"Create vector store.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Default prompt\n",
				"\n",
				"The default prompt in the from_llm classmethod.\n",
				"\n",
				"```\n",
				"DEFAULT_TEMPLATE = \"\"\"You are an assistant tasked with taking a natural language \n",
				"query from a user and converting it into a query for a vectorstore. \n",
				"In this process, you strip out information that is not relevant for \n",
				"the retrieval task. Here is the user query: {question}\"\"\"\n",
				"```"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"llm = chat_models.chat_openai\n",
				"retriever_rephrase_query = retrievers.RePhraseQueryRetriever.from_llm(\n",
				"  retriever=vectorstore.as_retriever(), llm=llm\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Hi I'm Lance. What are the approaches to Task Decomposition?\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:langchain.retrievers.re_phraser:Re-phrased question: Query for vectorstore: approaches to Task Decomposition\n"
					]
				},
				{
					"data": {
						"text/plain": [
							"[Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
							" Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
							" Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
							" Document(page_content='Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]"
						]
					},
					"execution_count": 14,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"query = queries[0]\n",
				"print(query)\n",
				"retriever_rephrase_query.get_relevant_documents(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\u001b[32m2024-04-03 11:37:37.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_retriever\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mRetrievers: ['RePhraseQueryRetriever']\u001b[0m\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Hi I'm Lance. What are the approaches to Task Decomposition?\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"INFO:langchain.retrievers.re_phraser:Re-phrased question: Query for vectorstore: Approaches to Task Decomposition\n"
					]
				},
				{
					"data": {
						"text/plain": [
							"[Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
							" Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
							" Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
							" Document(page_content='Planning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]"
						]
					},
					"execution_count": 17,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"retriever = create_retriever(\n",
				"  llm=chat_models.chat_openai,\n",
				"  vectorstore=vectorstore,\n",
				"  embeddings=text_embedding_models.OpenAIEmbeddings(),\n",
				"  retriever_types=[\n",
				"    \"RePhraseQueryRetriever\",\n",
				"  ],\n",
				"  search_kwargs={\"k\": 4},\n",
				"  search_type='similarity',\n",
				")\n",
				"\n",
				"query = queries[0]\n",
				"print(query)\n",
				"retriever.get_relevant_documents(query)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Custom prompt\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# todo"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Test"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Best"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from loguru import logger\n",
				"from langchain_core.language_models.chat_models import BaseChatModel\n",
				"from langchain_core.vectorstores import VectorStore\n",
				"from langchain_core.embeddings import Embeddings\n",
				"from typing import Literal, Union\n",
				"\n",
				"def create_portion(input_list):\n",
				"\t\tlength = len(input_list)\n",
				"\t\toutput_value = 1 / length\n",
				"\t\toutput_list = [output_value] * length\n",
				"\t\treturn output_list\n",
				"\n",
				"def create_retriever(\n",
				"\tllm: Union[BaseChatModel, None] = None,\n",
				"\tvectorstore: Union[VectorStore, None] = None,\n",
				"\tembeddings: Union[Embeddings, None] = None,\n",
				"\tretriever_types: list[Literal[\n",
				"\t\t'base', 'SelfQueryRetriever', 'MultiQueryRetriever', 'CohereRerank', \n",
				"\t\t'BM25Retriever', 'RePhraseQueryRetriever',\n",
				"  ]] = [],\n",
				"\tcompressor_types: list[Literal[\n",
				"\t\t'EmbeddingsRedundantFilter', 'EmbeddingsFilter', 'LLMChainFilter', \n",
				"\t\t'LLMChainExtractor',\n",
				"  ]] = [],\n",
				"\tsearch_type: Literal['mmr', 'similarity'] = \"mmr\",\n",
				"\tsearch_kwargs: dict = {\n",
				"\t\t\"k\": 10,\n",
				"\t},\n",
				"\tdocument_content_description: Union[str, None] = None,\n",
				"\tmetadata_field_info: Union[list, None] = None,\n",
				"):\n",
				"\tmy_retrievers = []\n",
				"\tmy_compressors = []\n",
				"\t\n",
				"\t#*----------------------------------------------------------------------------\n",
				"\tretriever_base = vectorstore.as_retriever(\n",
				"\t\tsearch_type=search_type,\n",
				"\t\tsearch_kwargs=search_kwargs,\n",
				"\t)\n",
				"\t\n",
				"\tif \"base\" in retriever_types:\n",
				"\t\tmy_retrievers.append(retriever_base)\n",
				"\n",
				"\tif \"SelfQueryRetriever\" in retriever_types:\n",
				"\t\tretriever_self_querying = retrievers.SelfQueryRetriever.from_llm(\n",
				"\t\t\tllm=llm,\n",
				"\t\t\tvectorstore=vectorstore,\n",
				"\t\t\tdocument_contents=document_content_description,\n",
				"\t\t\tmetadata_field_info=metadata_field_info,\n",
				"\t\t\tverbose=True,\n",
				"\t\t)\n",
				"\t\tmy_retrievers.append(retriever_self_querying)\n",
				"\t\t\n",
				"\tif \"MultiQueryRetriever\" in retriever_types:\n",
				"\t\tretriever_multi_query = retrievers.MultiQueryRetriever.from_llm(\n",
				"\t\t\tretriever=retriever_base, llm=llm,\n",
				"\t\t)\n",
				"\t\tmy_retrievers.append(retriever_multi_query)\n",
				"\n",
				"\tif \"CohereRerank\" in retriever_types:\n",
				"\t\tlogger.warning(f\"Remember to use CohereEmbeddings for Vectorstore.\")\n",
				"\t\tembeddings = text_embedding_models.CohereEmbeddings(\n",
				"\t\t\tmodel=constants.EMBEDDINGS[\"COHERE\"][\"EMBED-MULTILINGUAL-V3.0\"]\n",
				"\t\t)\n",
				"\t\tcompressor_cohere = retrievers.CohereRerank()\n",
				"\t\tretriever_cohere_rerank = retrievers.ContextualCompressionRetriever(\n",
				"\t\t\tbase_compressor=compressor_cohere, base_retriever=retriever_base,\n",
				"\t\t)\n",
				"\t\tmy_retrievers.append(retriever_cohere_rerank)\n",
				"\t\n",
				"\tif \"RePhraseQueryRetriever\" in retriever_types:\n",
				"\t\tretriever_rephrase_query = retrievers.RePhraseQueryRetriever.from_llm(\n",
				"\t\t\tretriever=retriever_base, llm=llm\n",
				"\t\t)\n",
				"\t\tmy_retrievers.append(retriever_rephrase_query)\n",
				"\t\t\n",
				"\tif \"BM25Retriever\" in retriever_types:\n",
				"\t\tretriever_bm25 = retrievers.BM25Retriever() # todo\n",
				"\t\tmy_retrievers.append(retriever_bm25)\n",
				"\t#*----------------------------------------------------------------------------\n",
				"\n",
				"\tif \"ContextualCompressionRetriever\" in retriever_types:\n",
				"\t\tif \"EmbeddingsRedundantFilter\" in compressor_types:\n",
				"\t\t\tfilter_embeddings_redundant = retrievers.EmbeddingsRedundantFilter(embeddings=embeddings)\n",
				"\t\t\tmy_compressors.append(filter_embeddings_redundant)\n",
				"\t\tif \"EmbeddingsFilter\" in compressor_types:\n",
				"\t\t\tfilter_embeddings_relevant = retrievers.EmbeddingsFilter(\n",
				"\t\t\t\tembeddings=embeddings, similarity_threshold=0.75,\n",
				"\t\t\t)\n",
				"\t\t\tmy_compressors.append(filter_embeddings_relevant)\n",
				"\t\t\n",
				"\t\tif \"LLMChainFilter\" in compressor_types:\n",
				"\t\t\tfilter_llmchain = retrievers.LLMChainFilter.from_llm(llm)\n",
				"\t\t\tmy_compressors.append(filter_llmchain)\n",
				"\t\t\n",
				"\t\tif \"LLMChainExtractor\" in compressor_types:\n",
				"\t\t\textractor_llmchain = retrievers.LLMChainExtractor.from_llm(llm)\n",
				"\t\t\tmy_compressors.append(extractor_llmchain)\n",
				"\t\t\t\n",
				"\t\tcompressor_pipeline = retrievers.DocumentCompressorPipeline(\n",
				"\t\t\ttransformers=my_compressors,\n",
				"\t\t)\n",
				"\n",
				"\t\tretriever_contextual_compression = retrievers.ContextualCompressionRetriever(\n",
				"\t\t\tbase_compressor=compressor_pipeline, base_retriever=retriever_base,\n",
				"\t\t)\n",
				"\t\t\n",
				"\t\tmy_retrievers.append(retriever_contextual_compression)\n",
				"\t\n",
				"\tlogger.info(f\"Retrievers: {retriever_types}\")\n",
				"\tretriever_ensemble = retrievers.EnsembleRetriever(\n",
				"\t\tretrievers=my_retrievers,\n",
				"\t\tweights=create_portion(my_retrievers),\n",
				"\t)\n",
				"\n",
				"\treturn retriever_ensemble"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"embeddings = text_embedding_models.CohereEmbeddings(\n",
				"  model=constants.EMBEDDINGS[\"COHERE\"][\"EMBED-MULTILINGUAL-V3.0\"]\n",
				")\n",
				"\n",
				"retriever = create_retriever(\n",
				"  llm=chat_models.chat_openai,\n",
				"  embeddings=embeddings,\n",
				"  retriever_types=[\n",
				"    # \"base\",\n",
				"    # \"SelfQueryRetriever\",\n",
				"    \"CohereRerank\",\n",
				"    \"RePhraseQueryRetriever\",\n",
				"  ],\n",
				"  compressor_types=[\n",
				"\n",
				"  ],\n",
				"  vectorstore=vectorstore,\n",
				"  search_kwargs={\"k\": 4},\n",
				"  search_type='similarity',\n",
				"  document_content_description=document_content_description,\n",
				"  metadata_field_info=metadata_field_info,\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# TODOs\n",
				"\n",
				"[Qdrant Self Query](https://python.langchain.com/docs/integrations/retrievers/self_query/qdrant_self_query)\n",
				"\n",
				"[Tavily Search API](https://python.langchain.com/docs/integrations/retrievers/tavily)\n",
				"\n",
				"[Wikipedia](https://python.langchain.com/docs/integrations/retrievers/wikipedia)\n",
				"\n",
				"[You.com](https://python.langchain.com/docs/integrations/retrievers/you-retriever)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "LLM",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.8"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
