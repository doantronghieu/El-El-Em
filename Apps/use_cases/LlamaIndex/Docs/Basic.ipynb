{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import add_packages\n",
    "import os, dotenv, logging, sys\n",
    "from loguru import logger\n",
    "from pprint import pprint\n",
    "import llama_index\n",
    "from toolkit.llamaindex import readers, llms, stores, cores\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, build and store an index\n",
    "\n",
    "\n",
    "The data loaded is stored as vector embeddings in memory.\n",
    "\n",
    "Save time by storing the embeddings on disk.\n",
    "\n",
    "By default, data is saved to the storage directory, but can be changed by passing a persist_dir parameter.\n",
    "\n",
    "Generate and store the index in starter.py to benefit from persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds an index over the documents in the data folder.\n",
    "\n",
    "DIR = f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1\"\n",
    "INDEX_DIR = f\"{DIR}/index\"\n",
    "\n",
    "if not os.path.exists(INDEX_DIR):\n",
    "  documents = readers.SimpleDirectoryReader(DIR).load_data()\n",
    "  index = stores.VectorStoreIndex.from_documents(documents)\n",
    "  # Store it for later\n",
    "  index.storage_context.persist(persist_dir=INDEX_DIR)\n",
    "  logger.info(f\"Created: {INDEX_DIR}\")\n",
    "else:\n",
    "  storage_context = stores.StorageContext.from_defaults(persist_dir=INDEX_DIR)\n",
    "  index = stores.load_index_from_storage(storage_context)\n",
    "  logger.info(f\"Loaded: {INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Q&A engine by indexing and asking a simple question.\n",
    "query_engine = index.as_query_engine(llm=llms.openai_llms["GPT-3.5-TURBO"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the author do growing up?\"\n",
    "response = query_engine.query(query)\n",
    "pprint(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize (FAQ)\n",
    "\n",
    "- Parse documents into smaller chunks\n",
    "- Use a different vector store\n",
    "- Use a different LLM\n",
    "- Use a different response mode\n",
    "- Stream the response back\n",
    "- Chatbot instead of Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-02 05:00:05.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mLoaded: /Users/thung/Documents/Me/Coding/Learn-LLM/Apps/data/llamaindex_tmp/1/index\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author, growing up, realized the limitations of the ideas presented in mainstream media like TV and sought alternatives. They found the surrounding environment to be lacking in authenticity and sought out different sources of information and experiences to broaden their perspective."
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "cores.Settings.chunk_size = 512\n",
    "cores.Settings.llm = llms.openai_llms["GPT-3.5-TURBO"]\n",
    "\n",
    "DIR = f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1\"\n",
    "INDEX_DIR = f\"{DIR}/index\"\n",
    "\n",
    "chroma_client = stores.chromadb.PersistentClient(INDEX_DIR)\n",
    "\n",
    "if len(os.listdir(INDEX_DIR)) < 2:\n",
    "  chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "else:\n",
    "  chroma_collection = \"quickstart\"\n",
    "\n",
    "vectorstore = vectorstores.ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Builds an index over the documents in the data folder\n",
    "if len(os.listdir(INDEX_DIR)) < 2:\n",
    "  # StorageContext defines the storage backend for where the documents, embeddings,\n",
    "  # and indexes are stored.\n",
    "  storage_context = indexes.StorageContext.from_defaults(vector_store=vectorstore)\n",
    "  \n",
    "  documents = readers.SimpleDirectoryReader(DIR).load_data()\n",
    "  index = indexes.VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    transformations=[\n",
    "      SentenceSplitter(chunk_size=512)\n",
    "    ],\n",
    "    storage_context=storage_context,\n",
    "  )\n",
    "  logger.info(f\"Created: {INDEX_DIR}\")\n",
    "else:\n",
    "  index = indexes.load_index_from_storage(storage_context)\n",
    "  logger.info(f\"Loaded: {INDEX_DIR}\")\n",
    "\n",
    "# as_query_engine builds a default retriever and query engine on top of the \n",
    "# index. Configure the retriever to return the top 5 most similar documents.\n",
    "query_engine = index.as_query_engine(\n",
    "  llm=llms.openai_llms["GPT-3.5-TURBO"], \n",
    "  streaming=True,\n",
    "  similarity_top_k=5, \n",
    "  response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "query = \"What did the author do growing up?\"\n",
    "response = query_engine.query(query)\n",
    "response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
