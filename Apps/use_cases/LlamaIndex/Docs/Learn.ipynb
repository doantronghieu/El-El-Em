{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import add_packages\n",
    "from my_configs import constants\n",
    "from my_llamaindex import readers, llms, stores_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs\n",
    "\n",
    "Choose the LLM for your application, you can use multiple.\n",
    "\n",
    "LLMs used at various pipeline stages.\n",
    "\n",
    "- During Indexing: Determine data relevance or summarize raw data and index the summaries instead.\n",
    "\n",
    "- During Querying\n",
    "\n",
    "  - During Retrieval (fetching data from index) can be given options and make decisions about where to find information. An agentic LLM can use tools to query different data sources.\n",
    "\n",
    "  - During Response Synthesis, an LLM can combine answers to multiple sub-queries into a single coherent answer or transform data from unstructured text to JSON or another programmatic output format.\n",
    "\n",
    "LlamaIndex: One interface for multiple LLMs, choose any for any pipeline stage.\n",
    "\n",
    "Instantiate an LLM and pass it to Settings, then pass it to other pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OpenAI with default gpt-3.5-turbo and adjust temperature. \n",
    "# VectorStoreIndex uses this for answering queries.\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llms.llm_openai_3_5\n",
    "\n",
    "documents = readers.SimpleDirectoryReader(\n",
    "  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
    ").load_data()\n",
    "index = stores_index.VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available LLMs\n",
    "\n",
    "Integrations with OpenAI, Hugging Face, PaLM, and more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local LLM\n",
    "\n",
    "LlamaIndex supports hosted LLM APIs and allows running a local model like Llama2.\n",
    "\n",
    "If Ollama is installed and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "LlamaIndex has built-in prompts that handle data formatting, a key benefit of using the platform. Customization is also an option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (Ingestion)\n",
    "\n",
    "To use LLM, process and load data first. Similar to data cleaning/feature engineering pipelines in ML or ETL pipelines in traditional data settings.\n",
    "\n",
    "Ingestion pipeline: Load, Transform, Index and store data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loaders\n",
    "Load data using data connectors (Reader) for LLM to act on it. These connectors \n",
    "ingest data from various sources and format it into Document objects, which contain\n",
    "text and metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SimpleDirectoryReader\n",
    "\n",
    "It creates documents from every file in a directory. It can read various formats like Markdown, PDFs, Word documents, PowerPoint decks, images, audio, and video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Readers from [LlamaHub](https://docs.llamaindex.ai/en/stable/understanding/loading/llamahub/)\n",
    "\n",
    "There are numerous data sources available through LlamaHub, not all of which are built-in.\n",
    "\n",
    "LlamaIndex downloads and installs DatabaseReader to query a SQL database and return results as a Document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Documents directly\n",
    "\n",
    "Instead of using a loader, use a Document directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformations\n",
    "\n",
    "After loading the data, process and transform it before storing. Chunk, extract metadata, and embed each chunk for optimal retrieval and use by the LLM.\n",
    "\n",
    "Transformation input/outputs are Node objects. Transformations can be stacked and reordered.\n",
    "\n",
    "We offer a high-level and lower-level API for document transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transformation API\n",
    "\n",
    "Indexes have a .from_documents() method for parsing and chunking Document objects. For more control over document splitting, consider other options.\n",
    "\n",
    "Under the hood, Document is split into Node objects, similar to Documents but with a relationship to their parent Document.\n",
    "\n",
    "Customize core components by passing in a custom transformations list or applying to the global Settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lower-Level Transformation API\n",
    "\n",
    "Define steps explicitly.\n",
    "\n",
    "You can use transformation modules like text splitters and metadata extractors separately or combine them in our Transformation Pipeline interface.\n",
    "\n",
    "Walk through the steps below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Split Documents into Nodes\n",
    "\n",
    "Split documents into \"chunks\"/Node objects to process data into bite-sized pieces for retrieval/feeding to the LLM.\n",
    "\n",
    "LlamaIndex supports various text splitters, including paragraph, sentence, and token-based splitters, as well as file-based splitters like HTML and JSON.\n",
    "\n",
    "Can be used independently or as part of an ingestion pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adding Metadata\n",
    "\n",
    "Add metadata to documents and nodes manually or with automatic extractors.\n",
    "\n",
    "Customize Documents and Nodes with these guides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adding Embeddings\n",
    "\n",
    "Insert node into vector index requires embedding. Refer to ingestion pipeline or embeddings guide for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create and pass Nodes directly\n",
    "\n",
    "Create nodes directly and pass a list of nodes to an indexer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing & Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing and Debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
