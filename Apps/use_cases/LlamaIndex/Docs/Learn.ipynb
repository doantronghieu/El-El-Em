{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"import add_packages\n",
				"\n",
				"import logging, sys\n",
				"\n",
				"from my_configs import constants\n",
				"from toolkit.llamaindex import (\n",
				"\tnode_parsers, readers, cores, stores, ingestions, schemas, embeddings,\n",
				"\tquery_engines, postprocessors,\n",
				")\n",
				"\n",
				"cores.Settings.llm = llms.openai_llms[\"GPT-3.5-TURBO\"]\n",
				"cores.Settings.embed_model = embeddings.openai_embeddings[\"TEXT_EMBED_ADA_002\"]\n",
				"cores.Settings.chunk_size = 512\n",
				"\n",
				"logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
				"logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Using LLMs\n",
				"\n",
				"Choose the LLM for your application, you can use multiple.\n",
				"\n",
				"LLMs used at various pipeline stages.\n",
				"\n",
				"- During Indexing: Determine data relevance or summarize raw data and index the summaries instead.\n",
				"\n",
				"- During Querying\n",
				"\n",
				"  - During Retrieval (fetching data from index) can be given options and make decisions about where to find information. An agentic LLM can use tools to query different data sources.\n",
				"\n",
				"  - During Response Synthesis, an LLM can combine answers to multiple sub-queries into a single coherent answer or transform data from unstructured text to JSON or another programmatic output format.\n",
				"\n",
				"LlamaIndex: One interface for multiple choose any for any pipeline stage.\n",
				"\n",
				"Instantiate an LLM and pass it to Settings, then pass it to other pipeline stages."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Instantiate OpenAI with default gpt-3.5-turbo and adjust temperature. \n",
				"# VectorStoreIndex uses this for answering queries.\n",
				"from llama_index.core import Settings\n",
				"\n",
				"Settings.llm = llms.openai_llms[\"GPT-3.5-TURBO\"]\n",
				"\n",
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"index = stores.VectorStoreIndex.from_documents(documents)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Available LLMs\n",
				"\n",
				"Integrations with OpenAI, Hugging Face, PaLM, and more. \n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Local LLM\n",
				"\n",
				"LlamaIndex supports hosted LLM APIs and allows running a local model like Llama2.\n",
				"\n",
				"If Ollama is installed and running."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Prompts\n",
				"\n",
				"LlamaIndex has built-in prompts that handle data formatting, a key benefit of using the platform. Customization is also an option.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Loading Data (Ingestion)\n",
				"\n",
				"To use LLM, process and load data first. Similar to data cleaning/feature engineering pipelines in ML or ETL pipelines in traditional data settings.\n",
				"\n",
				"Ingestion pipeline: Load, Transform, Index and store data.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Loaders\n",
				"Load data using data connectors (Reader) for LLM to act on it. These connectors \n",
				"ingest data from various sources and format it into Document objects, which contain\n",
				"text and metadata.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### SimpleDirectoryReader\n",
				"\n",
				"It creates documents from every file in a directory. It can read various formats like Markdown, PDFs, Word documents, PowerPoint decks, images, audio, and video.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Readers from [LlamaHub](https://docs.llamaindex.ai/en/stable/understanding/loading/llamahub/)\n",
				"\n",
				"There are numerous data sources available through LlamaHub, not all of which are built-in.\n",
				"\n",
				"LlamaIndex downloads and installs DatabaseReader to query a SQL database and return results as a Document.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Create Documents directly\n",
				"\n",
				"Instead of using a loader, use a Document directly.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"doc = cores.Document(text=\"text\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Transformations\n",
				"\n",
				"After loading the data, process and transform it before storing. Chunk, extract metadata, and embed each chunk for optimal retrieval and use by the LLM.\n",
				"\n",
				"Transformation input/outputs are Node objects. Transformations can be stacked and reordered.\n",
				"\n",
				"We offer a high-level and lower-level API for document transformation.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### High-Level API\n",
				"\n",
				"Indexes have a .from_documents() method for parsing and chunking Document objects. For more control over document splitting, consider other options.\n",
				"\n",
				"Under the hood, Document is split into Node objects, similar to Documents but with a relationship to their parent Document.\n",
				"\n",
				"Customize core components by passing in a custom transformations list or applying to the global Settings.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"text_splitter = node_parsers.SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
				"cores.Settings.text_splitter = text_splitter\n",
				"\n",
				"index = stores.VectorStoreIndex.from_documents(\n",
				"\tdocuments, transformations=[text_splitter],\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Lower-Level API\n",
				"\n",
				"Define steps explicitly. Use transformation modules like text splitters and metadata extractors separately or combine them in Transformation Pipeline interface.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"#### Split Documents into Nodes\n",
				"\n",
				"Split documents into \"chunks\"/Node objects to process data into bite-sized pieces for retrieval/feeding to the LLM.\n",
				"\n",
				"LlamaIndex supports various text splitters, including paragraph, sentence, token-based splitters, file-based splitters like HTML and JSON.\n",
				"\n",
				"Can be used independently or as part of an ingestion pipeline.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"pipeline = ingestions.IngestionPipeline(\n",
				"\ttransformations=[\n",
				"\t\tnode_parsers.TokenTextSplitter(),\n",
				"\t]\n",
				")\n",
				"\n",
				"nodes = pipeline.run(documents)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Adding Metadata\n",
				"\n",
				"Add metadata to documents and nodes manually or with automatic extractors.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"document = cores.Document(\n",
				"\ttext=\"text\",\n",
				"\tmetadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Adding Embeddings\n",
				"\n",
				"Insert node into vector index requires embedding. "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Create and pass Nodes directly\n",
				"\n",
				"Create nodes directly and pass a list of nodes to an indexer.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"node1 = schemas.TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
				"node2 = schemas.TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
				"\n",
				"index = stores.VectorStoreIndex(nodes=[node1, node2])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Indexing & Embedding\n",
				"\n",
				"With data loaded, create an Index over Document objects or Nodes to begin querying.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## What is an Index?\n",
				"\n",
				"In LlamaIndex, an Index is a data structure of Document objects for querying by an LLM, complementary to your querying strategy.\n",
				"\n",
				"LlamaIndex offers various index types."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Vector Store Index\n",
				"\n",
				"It splits Documents into Nodes and creates vector embeddings for each node, which can be queried by an LLM.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Embedding Definition\n",
				"\n",
				"Vector embedding is a numerical representation of text semantics. Similar meanings result in similar embeddings.\n",
				"\n",
				"The mathematical relationship enables semantic search, allowing LlamaIndex to locate text related to query terms' meaning rather than simple keyword matching. \n",
				"\n",
				"There are various types of embeddings with different efficiency, effectiveness, and computational costs. LlamaIndex defaults to text-embedding-ada-002, the default embedding from OpenAI. Different LLMs may require different embeddings.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Vector Store Index embeds documents\n",
				"\n",
				"Vector Store Index converts text into embeddings using an API from your LLM, which is known as \"embedding your text\". Generating embeddings for large amounts of text can be time-consuming due to multiple API calls.\n",
				"\n",
				"Search embeddings by turning query into vector embedding and using VectorStoreIndex to rank based on semantic similarity.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Top K Retrieval\n",
				"\n",
				"Once ranking is complete, VectorStoreIndex returns the most-similar embeddings as corresponding text chunks. The number of embeddings returned is known as k, with the parameter controlling this known as top_k. This search type is often called \"top-k semantic retrieval.\"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Vector Store Index\n",
				"\n",
				"Pass the Vector Store Index the list of Documents created during the loading stage.\n",
				"\n",
				"from_documents also takes an optional argument show_progress. Set to True for progress bar display during index construction.\n",
				"\n",
				"Build an index over a list of Node objects directly.\n",
				"\n",
				"With indexed text, querying is ready. Embedding all text can be time-consuming and costly with a hosted LLM. Store embeddings first to save time and money.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"index = stores.VectorStoreIndex.from_documents(documents, show_progress=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Summary Index\n",
				"\n",
				"A Summary Index is a simplified form of Index ideal for generating summaries of text in Documents. It stores all Documents and returns them to the query engine.\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Storing\n",
				"\n",
				"Once data is loaded and indexed, store it to prevent re-indexing. By default, indexed data is stored in memory only.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Persisting to disk\n",
				"\n",
				"Use the .persist() method of every Index, which writes data to disk at the specified location.\n",
				"\n",
				"Composable Graph: Example\n",
				"\n",
				"Avoid re-loading and re-indexing data by loading the persisted index.\n",
				"\n",
				"Important: Ensure that the same options used for initializing the index are passed during load_index_from_storage.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"PERSIST_DIR = f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/index\"\n",
				"\n",
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"index = stores.VectorStoreIndex.from_documents(documents, show_progress=True)\n",
				"\n",
				"index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
				"\n",
				"# Rebuild storage context\n",
				"storage_context = stores.StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
				"\n",
				"# Load index\n",
				"index = stores.load_index_from_storage(storage_context)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Using Vector Stores\n",
				"\n",
				"Creating embeddings in a VectorStoreIndex can be costly, store them to avoid frequent re-indexing.\n",
				"\n",
				"LlamaIndex supports various vector stores with different architectures, complexities, and costs. \n",
				"\n",
				"To store embeddings with Chroma:\n",
				"- Initialize Chroma client\n",
				"- Create Collection in Chroma to store data\n",
				"- Assign Chroma as vector_store in StorageContext\n",
				"- Initialize VectorStoreIndex with StorageContext\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [],
			"source": [
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"# initialize client, setting path to save data\n",
				"db = stores.chromadb.PersistentClient(\n",
				"\tpath=f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/chroma_db\"\n",
				")\n",
				"\n",
				"# create collection\n",
				"chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
				"\n",
				"# assign chroma as the vector_store to the context\n",
				"vector_store = stores.ChromaVectorStore(chroma_collection=chroma_collection)\n",
				"storage_context = stores.StorageContext.from_defaults(vector_store=vector_store)\n",
				"\n",
				"# create index\n",
				"index = stores.VectorStoreIndex.from_documents(\n",
				"\tdocuments=documents, storage_context=storage_context, show_progress=True\n",
				")\n",
				"\n",
				"# Now that data is loaded, indexed, and stored, it's time to query.\n",
				"# create a query engine and query\n",
				"query_engine = index.as_query_engine(llm=llms.openai_llms[\"GPT-3.5-TURBO\"])\n",
				"query = \"What is Lisp?\"\n",
				"response = query_engine.query(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load embeddings directly if already created and stored, without loading \n",
				"# documents or creating a new VectorStoreIndex.\n",
				"# initialize client, setting path to save data\n",
				"db = stores.chromadb.PersistentClient(\n",
				"\tpath=f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/chroma_db\"\n",
				")\n",
				"\n",
				"# create collection\n",
				"chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
				"\n",
				"# assign chroma as the vector_store to the context\n",
				"vector_store = stores.ChromaVectorStore(chroma_collection=chroma_collection)\n",
				"storage_context = stores.StorageContext.from_defaults(vector_store=vector_store)\n",
				"\n",
				"# create index\n",
				"index = stores.VectorStoreIndex.from_vector_store(\n",
				"\tvector_store, storage_context=storage_context, show_progress=True\n",
				")\n",
				"\n",
				"# Now that data is loaded, indexed, and stored, it's time to query.\n",
				"# create a query engine and query\n",
				"query_engine = index.as_query_engine(llm=llms.openai_llms[\"GPT-3.5-TURBO\"])\n",
				"query = \"What is Lisp?\"\n",
				"response = query_engine.query(query)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Inserting Documents or Nodes\n",
				"\n",
				"Add new documents to your index using the insert method if you've already created an index.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"documents = readers.SimpleDirectoryReader(\n",
				"\tf\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"index = stores.VectorStoreIndex([])\n",
				"\n",
				"for doc in documents:\n",
				"\tindex.insert(doc)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Querying\n",
				"\n",
				"Now data loaded, index built, stored for later, time for querying.\n",
				"\n",
				"Querying is a prompt call to an LLM for a question, answer, summarization, or complex instruction.\n",
				"\n",
				"Complex querying may require repeated/chained prompt + LLM calls or a reasoning loop across multiple components.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Getting started\n",
				"\n",
				"Use index to create The QueryEngine.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
						"  from .autonotebook import tqdm as notebook_tqdm\n",
						"Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 72.39it/s]"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n",
						"Generating embeddings: 100%|██████████| 15/15 [00:01<00:00, 11.64it/s]\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Lisp is a programming language that was used in the development of the software for Viaweb. It is described as having a unique syntax with a heavy use of parentheses. Despite being less commonly used in business settings at the time, it provided Viaweb with a technical advantage that was not easily understood by competitors.\n"
					]
				}
			],
			"source": [
				"PERSIST_DIR = f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/index\"\n",
				"\n",
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"index = stores.VectorStoreIndex.from_documents(documents, show_progress=True)\n",
				"\n",
				"query_engine = index.as_query_engine(llm=llms.openai_llms[\"GPT-3.5-TURBO\"])\n",
				"query = \"What is Lisp?\"\n",
				"response = query_engine.query(query)\n",
				"print(response)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Querying Stages\n",
				"\n",
				"Querying consists of three distinct stages:\n",
				"- Retrieval: Find and return relevant documents from the Index.\n",
				"- Postprocessing: Rerank, transform, or filter retrieved Nodes.\n",
				"- Response synthesis: Combine query, most-relevant data, and prompt to send to LLM for response.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Customizing Querying Stages\n",
				"\n",
				"LlamaIndex: low-level composition API for granular querying.\n",
				"\n",
				"Example: Customize retriever to use different top_k number and add post-processing step requiring minimum similarity score for retrieved nodes to be included. Results vary based on relevance.\n",
				"\n",
				"Customize by implementing interfaces for retrieval, response synthesis, and query logic.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"documents = readers.SimpleDirectoryReader(\n",
				"  f\"{add_packages.APP_PATH}/data/llamaindex_tmp/1/\"\n",
				").load_data()\n",
				"\n",
				"index = stores.VectorStoreIndex.from_documents(documents, show_progress=True)\n",
				"\n",
				"retriever = stores.VectorIndexRetriever(\n",
				"\tindex=index,\n",
				"\tsimilarity_top_k=10,sto\n",
				")\n",
				"\n",
				"response_synthesizer = cores.get_response_synthesizer()\n",
				"\n",
				"# Configure desired node postprocessors.\n",
				"node_postprocessors = [\n",
				"\tpostprocessors.SimilarityPostprocessor(similarity_cutoff=0.7),\n",
				"]\n",
				"\n",
				"# assemble query engine\n",
				"query_engine = query_engines.RetrieverQueryEngine(\n",
				"\tretriever=retriever,\n",
				"\tresponse_synthesizer=response_synthesizer,\n",
				"\tnode_postprocessors=node_postprocessors,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [],
			"source": [
				"query = \"What is Lisp\"\n",
				"response = query_engine.query(query)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Node postprocessors configuration\n",
				"\n",
				"Advanced Node filtering and augmentation improve relevancy of retrieved Node objects, reducing LLM calls/cost and enhancing response quality.\n",
				"\n",
				"- KeywordNodePostprocessor: filters nodes by required_keywords and exclude_keywords.\n",
				"- SimilarityPostprocessor: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n",
				"- PrevNextNodePostprocessor: augments retrieved Node objects with additional relevant context based on Node relationships.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"### Configuring response synthesis\n",
				"\n",
				"After retriever fetches nodes, BaseSynthesizer combines information to synthesize final response.\n",
				"\n",
				"Options:\n",
				"- default: \"Create and refine\" an answer by making a separate LLM call for each retrieved Node. Ideal for detailed answers.\n",
				"\n",
				"- compact: Stuff as many Node text chunks into each LLM call to maximize prompt size. Create and refine answers through multiple prompts if necessary.\n",
				"\n",
				"- tree_summarize: Recursively construct a tree from Node objects based on the query. Return the root node as the response\n",
				"\n",
				"- no_text: Runs retriever to fetch nodes for LLM without sending them. Check response.source_nodes for inspection.\n",
				"\n",
				"- Accumulate: Apply query to each Node text chunk, accumulating responses into an array. Returns concatenated string of all responses. Ideal for running same query against each text chunk separately.\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tracing and Debugging\n",
				"\n",
				"Key for understanding and optimizing app.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Basic logging\n",
				"\n",
				"To understand your application, enable debug logging.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Callback handler\n",
				"\n",
				"LlamaIndex offers callbacks for debugging, tracking, and tracing library operations. The callback manager allows for adding multiple callbacks.\n",
				"\n",
				"Track event data including duration and frequency.\n",
				"\n",
				"A trace map of events is recorded for callbacks to use as needed. The LlamaDebugHandler will print the trace of events after most operations.\n",
				"\n",
				"Simple callback handler example.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Observability\n",
				"\n",
				"LlamaIndex: one-click observability for building principled LLM applications in production.\n",
				"\n",
				"This feature integrates the LlamaIndex library with observability tools. Configure a variable once to:\n",
				"- View LLM/prompt inputs/outputs\n",
				"- Ensure component outputs are performing well\n",
				"- View call traces for indexing and querying.\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Evaluating\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Putting it all Together"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "llm",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.8"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
