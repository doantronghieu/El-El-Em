{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import add_packages\n",
    "import config\n",
    "from pprint import pprint\n",
    "\n",
    "from my_langchain import (\n",
    "    document_loaders, text_splitters, text_embedding_models, vector_stores,\n",
    "    chat_models, prompts, utils, output_parsers, agents, documents, runnables,\n",
    "    llms, histories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt + model + output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain that takes a topic and generates a joke.\n",
    "\n",
    "# Takes in a dictionary of template variables and produces a PromptValue. \n",
    "# PromptValue is a wrapper around a completed prompt that can be passed to \n",
    "# either an LLM or ChatModel. \n",
    "# It defines logic for producing BaseMessages and a string.\n",
    "prompt = prompts.ChatPromptTemplate.from_template(\n",
    "  \"Tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "# The PromptValue is passed to model. \n",
    "# ChatModel outputs a BaseMessage. LLM outputs string.\n",
    "model = chat_models.chat_openai\n",
    "\n",
    "# Pass model output to output_parser, a BaseOutputParser inputs string or \n",
    "# BaseMessage and converts to string.\n",
    "output_parser = output_parsers.str_output_parser()\n",
    "\n",
    "# chains together different components feeds the output from one component as \n",
    "# input into the next component.\n",
    "\n",
    "# user input passed to prompt template, prompt template output passed to model,\n",
    "# model output passed to output parser.\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "pprint(prompt_value)\n",
    "pprint(prompt_value.to_messages())\n",
    "pprint(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = vector_stores.chroma.Chroma.from_texts(\n",
    "  [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "  embedding=text_embedding_models.openai_embeddings()\n",
    ")\n",
    "\n",
    "# In memory store to retrieve documents based on a query. \n",
    "# Runnable component can be chained with other components or run separately.\n",
    "retriever = vectorstore.as_retriever() # .invoke(\"\")\n",
    "\n",
    "# The prompt template takes in context and question as values. \n",
    "# Before building the prompt template, relevant documents should be retrieved \n",
    "# for inclusion in the context.\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = prompts.ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = chat_models.chat_openai\n",
    "output_parser = output_parsers.str_output_parser()\n",
    "\n",
    "# Prepare inputs for the prompt with retrieved documents and user question, \n",
    "# retriever for document search, RunnablePassthrough for user's question.\n",
    "setup_and_retrieval = runnables.RunnableParallel(\n",
    "  {\"context\": retriever, \"question\": runnables.RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate + ChatModel chain.\n",
    "\n",
    "model = chat_models.chat_openai\n",
    "prompt = prompts.ChatPromptTemplate.from_template(\n",
    "  \"Tell me a joke about {topic}\"\n",
    ")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Schema\n",
    "\n",
    "A description of the inputs accepted by a Runnable. \n",
    "\n",
    "Pydantic model generated from the structure of any Runnable. \n",
    "\n",
    "Call .schema() to obtain a JSONSchema representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "  print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of concurrent requests using the max_concurrency parameter.\n",
    "chain.batch([\n",
    "  {\"topic\": \"bears\"},\n",
    "  {\"topic\": \"cats\"},\n",
    "], config={\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism\n",
    "\n",
    "LangChain Expression Language supports parallel requests by executing each \n",
    "element in parallel when using a RunnableParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template1 = \"tell me a joke about {topic}\"\n",
    "template2 = \"write a short (2 line) poem about {topic}\"\n",
    "chain1 = prompts.ChatPromptTemplate.from_template(template1) | model\n",
    "chain2 = prompts.ChatPromptTemplate.from_template(template2) | model\n",
    "combined = runnables.RunnableParallel(joke=chain1, poem=chain2)\n",
    "combined.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating inputs & output - RunnableParallel\n",
    "\n",
    "RunnableParallel manipulating output of one Runnable match input format of next \n",
    "Runnable in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorstore = vector_stores.chroma.Chroma.from_texts(\n",
    "  [\"harrison worked at kensho\"], embedding=text_embedding_models.openai_embeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# The input to prompt is a map with keys “context” and “question”. \n",
    "# Retrieve the context using the retriever and pass through the user input \n",
    "# under the “question” key.\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = prompts.ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = chat_models.chat_openai\n",
    "\n",
    "retrieval_chain = (\n",
    "  {\n",
    "    \"context\": retriever,\n",
    "    \"question\": runnables.RunnablePassthrough()\n",
    "  }\n",
    "  | prompt\n",
    "  | model\n",
    "  | output_parsers.str_output_parser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### itemgetter\n",
    "\n",
    "Extract data from the map using Python's itemgetter with RunnableParallel.\n",
    "\n",
    "itemgetter is used to extract specific keys from the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = prompts.ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "  {\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"language\": itemgetter(\"language\"),\n",
    "  }\n",
    "  | prompt \n",
    "  | model\n",
    "  | output_parsers.StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\n",
    "  \"question\": \"where did harrison work\",\n",
    "  \"language\": \"italian\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize Runnables\n",
    "RunnableParallel (aka. RunnableMap) executes multiple Runnables in parallel and returns their output as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = llms.llm_openai\n",
    "\n",
    "template1 = \"tell me a joke about {topic}\"\n",
    "template2 = \"write a 2-line poem about {topic}\"\n",
    "chain1 = prompts.ChatPromptTemplate.from_template(template1) | model\n",
    "chain2 = prompts.ChatPromptTemplate.from_template(template2) | model\n",
    "\n",
    "map_chain = runnables.RunnableParallel(Chain1=chain1, Chain2=chain2)\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing data through - RunnablePassthrough\n",
    "\n",
    "RunnablePassthrough allows passing inputs unchanged or with extra keys, used with RunnableParallel to assign data to a new key in the map. \n",
    "\n",
    "RunnablePassthrough() pass input through. \n",
    "\n",
    "RunnablePassthrough called with assign will take the input and add the extra arguments passed to the assign function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runnable = runnables.RunnableParallel(\n",
    "  # passed key was called with RunnablePassthrough() and it passed on {'num': 1}.\n",
    "  passed=runnables.RunnablePassthrough(),\n",
    "  # multiplies the numerical value by 3\n",
    "  extra=runnables.RunnablePassthrough.assign(mult=lambda x: x[\"num\"]*3),\n",
    "  # set single value adding 1 to num\n",
    "  modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "The input is a map with keys “context” and “question”. The user input is the question. Retrieve the context and pass the user input as the question key. The RunnablePassthrough allows passing the user’s question to the prompt and model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = vector_stores.chroma.Chroma.from_texts(\n",
    "  [\"harrison worked at kensho\"],\n",
    "  embedding=text_embedding_models.openai_embeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = prompts.ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = chat_models.chat_openai\n",
    "\n",
    "chain = (\n",
    "  {\n",
    "    \"context\": retriever, \"question\": runnables.RunnablePassthrough()\n",
    "  }\n",
    "  | prompt\n",
    "  | model\n",
    "  | output_parsers.str_output_parser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Custom Functions - RunnableLambda\n",
    "\n",
    "Use arbitrary functions in the pipeline.\n",
    "\n",
    "All inputs to functions must be a single argument. If a function accepts multiple arguments, create a wrapper that accepts a single input and unpacks it.\n",
    "\n",
    "Optional RunnableConfig can be used by Runnable lambdas to pass callbacks, tags, and configuration information to nested runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def length_function(text):\n",
    "  return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "  return len(text1)*len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "  return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = prompts.ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "model = chat_models.chat_openai\n",
    "\n",
    "chain = (\n",
    "  {\n",
    "    \"a\": itemgetter(\"foo\") | runnables.RunnableLambda(length_function),\n",
    "    \"b\": {\n",
    "      \"text1\": itemgetter(\"foo\"),\n",
    "      \"text2\": itemgetter(\"bar\"),\n",
    "    } | runnables.RunnableLambda(multiple_length_function)\n",
    "  }\n",
    "  | prompt\n",
    "  | model\n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | ChatOpenAI()\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamically route logic based on input - RunnableBranch\n",
    "\n",
    "Routing creates non-deterministic chains based on previous step output, providing structure and consistency for LLM interactions.\n",
    "\n",
    "Two ways to perform routing: Conditionally return runnables from a RunnableLambda or using a RunnableBranch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Illustrate both methods using a two step sequence. The first step classifies an input question as LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain to identify incoming questions as LangChain, Anthropic, or Other.\n",
    "template = \"\"\"\\\n",
    "Given the user question below, classify it as either being about `LangChain`, \\\n",
    "`Anthropic`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\\\n",
    "\"\"\"\n",
    "prompt = prompts.PromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "  prompt\n",
    "  | chat_models.chat_openai\n",
    "  | output_parsers.StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"how do I call Anthropic?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "\n",
    "# create three sub chains\n",
    "\n",
    "template_langchain = \"\"\"\\\n",
    "You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\\\n",
    "\"\"\"\n",
    "prompt_langchain = prompts.PromptTemplate.from_template(template_langchain)\n",
    "chain_langchain = prompt_langchain | chat_models.chat_openai\n",
    "\n",
    "template_anthropic = \"\"\"\\\n",
    "You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\\\n",
    "\"\"\"\n",
    "prompt_anthropic = prompts.PromptTemplate.from_template(template_anthropic)\n",
    "chain_anthropic = prompt_anthropic | chat_models.chat_openai\n",
    "\n",
    "template_general = \"\"\"\\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\\\n",
    "\"\"\"\n",
    "prompt_general = prompts.PromptTemplate.from_template(template_general)\n",
    "chain_general = prompt_general | chat_models.chat_openai\n",
    "\n",
    "# Use a custom function to route between different outputs\n",
    "def route(info: typing.Dict[str, str]):\n",
    "  if \"anthropic\" in info[\"topic\"].lower():\n",
    "    return chain_anthropic\n",
    "  elif \"langchain\" in info[\"topic\"].lower():\n",
    "    return chain_langchain\n",
    "  else:\n",
    "    return chain_general\n",
    "\n",
    "chain_full = (\n",
    "  {\n",
    "    \"topic\": chain,\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "  }\n",
    "  | runnables.RunnableLambda(route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_full.invoke({\"question\": \"how do I use Anthropic?\"}))\n",
    "print(chain_full.invoke({\"question\": \"how do I use LangChain?\"}))\n",
    "print(chain_full.invoke({\"question\": \"whats 2 + 2?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind runtime args\n",
    "\n",
    "Invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and not part of the user input. Use Runnable.bind() to pass these arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt + model sequence\n",
    "prompt = prompts.ChatPromptTemplate.from_messages([\n",
    "  (\n",
    "    \"system\",\n",
    "    \"Write out the following equation using algebraic symbols then solve it. \\\n",
    "     Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "  ),\n",
    "  (\n",
    "    \"human\",\n",
    "    \"{equation_statement}\"\n",
    "  )\n",
    "])\n",
    "\n",
    "model = chat_models.chat_openai\n",
    "runnable = (\n",
    "  {\n",
    "    \"equation_statement\": runnables.RunnablePassthrough()\n",
    "  }\n",
    "  | prompt\n",
    "  # call the model with certain stop words\n",
    "  | model.bind(stop=\"SOLUTION\")\n",
    "  # | model\n",
    "  | output_parsers.StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching OpenAI functions\n",
    "\n",
    "Attach OpenAI functions to a compatible OpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {\n",
    "    \"name\": \"solver\",\n",
    "    \"description\": \"Formulates and solves an equation\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"equation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The algebraic expression of the equation\",\n",
    "            },\n",
    "            \"solution\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The solution to the equation\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = prompts.ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Write out the following equation using algebraic symbols then solve it. \\\n",
    "     Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"{equation_statement}\"\n",
    "    )\n",
    "])\n",
    "\n",
    "model = chat_models.chat_openai.bind(\n",
    "    function_call={\"name\": \"solver\"}, functions=[function]\n",
    ")\n",
    "\n",
    "runnable = (\n",
    "    {\n",
    "        \"equation_statement\": runnables.RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "pprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching OpenAI tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "model = chat_models.chat_openai.bind(tools=tools)\n",
    "model.invoke(\"What's the weather in SF, NYC and LA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure chain internals at runtime\n",
    "\n",
    "Two methods for experimenting with and exposing multiple ways of doing things to the end user.\n",
    "\n",
    "- configurable_fields method. Configure particular fields of a runnable.\n",
    "- configurable_alternatives method to list alternatives for any runnable set during runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With LLMs\n",
    "\n",
    "LLMs configure temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = chat_models.chat_openai.configurable_fields(\n",
    "  temperature=runnables.ConfigurableField(\n",
    "    id=\"llm_temperature\",\n",
    "    name=\"LLM Temperature\",\n",
    "    description=\"The temperature of the LLM\",\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"pick a random number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\n",
    "  \"pick a random number\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used as part of a chain\n",
    "template = \"Pick a random number above {x}\"\n",
    "prompt = prompts.PromptTemplate.from_template(template)\n",
    "chain = prompt | model\n",
    "\n",
    "print(chain.invoke({\"x\": 0}))\n",
    "print(chain.with_config(\n",
    "  configurable={\"llm_temperature\": 0.9}\n",
    ").invoke({\"x\": 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With HubRunnables\n",
    "\n",
    "Useful for switching prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = runnables.HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n",
    "  owner_repo_commit=runnables.ConfigurableField(\n",
    "    id=\"hub_commit\",\n",
    "    name=\"Hub Commit\",\n",
    "    description=\"The Hub commit to pull from\"\n",
    "  )\n",
    ")\n",
    "\n",
    "print(prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"}))\n",
    "print(prompt.with_config(\n",
    "  configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}\n",
    ").invoke({\"question\": \"foo\", \"context\": \"bar\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Alternatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = chat_models.chat_openai.configurable_alternatives(\n",
    "  runnables.ConfigurableField(id=\"llm\"), # Field with an ID for configuration\n",
    "  default_key=\"openai\",  # Default LLM key\n",
    "  anthropic=chat_models.chat_anthropic, # option\n",
    "  gpt4=chat_models.ChatOpenAI(model=\"gpt-4\") # option\n",
    "  # Add more configuration options here\n",
    ")\n",
    "\n",
    "template = \"Tell me a joke about {topic}\"\n",
    "prompt = prompts.PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"topic\": \"bears\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.with_config(\n",
    "  configurable={\"llm\": \"gpt4\"}\n",
    ").invoke({\"topic\": \"bears\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Prompts\n",
    "\n",
    "Alternate between prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = chat_models.chat_openai\n",
    "\n",
    "template_joke = \"Tell me a joke about {topic}\"\n",
    "template_poem = \"Write a short poem about {topic}\"\n",
    "prompt = prompts.PromptTemplate.from_template(template_joke)\\\n",
    "  .configurable_alternatives(\n",
    "    # Field with an ID for configuration\n",
    "    runnables.ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"joke\",\n",
    "    poem=prompts.PromptTemplate.from_template(template_poem),  # Option: poem\n",
    "    # Add more configuration options here\n",
    "  )\n",
    "  \n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.with_config(\n",
    "  configurable={\"prompt\": \"poem\"}\n",
    ").invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Prompts and LLMs\n",
    "\n",
    "Multiple things configurable with prompts and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = chat_models.chat_openai.configurable_alternatives(\n",
    "  runnables.ConfigurableField(id=\"llm\"), # Field with an ID for configuration\n",
    "  default_key=\"openai\",  # Default LLM key\n",
    "  anthropic=chat_models.chat_anthropic, # option\n",
    "  gpt4=chat_models.ChatOpenAI(model=\"gpt-4\") # option\n",
    "  # Add more configuration options here\n",
    ")\n",
    "\n",
    "template_joke = \"Tell me a joke about {topic}\"\n",
    "template_poem = \"Write a short poem about {topic}\"\n",
    "prompt = prompts.PromptTemplate.from_template(template_joke)\\\n",
    "  .configurable_alternatives(\n",
    "    # Field with an ID for configuration\n",
    "    runnables.ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"joke\",\n",
    "    poem=prompts.PromptTemplate.from_template(template_poem),  # Option: poem\n",
    "    # Add more configuration options here\n",
    "  )\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.with_config(\n",
    "  configurable={\n",
    "    \"prompt\": \"poem\",\n",
    "    \"llm\": \"gpt4\"\n",
    "  }\n",
    ").invoke(\n",
    "  {\n",
    "    \"topic\": \"bears\",\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving configurations\n",
    "\n",
    "Save configured chains as objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_gpt4_joke = chain.with_config(\n",
    "  configurable={\n",
    "    \"prompt\": \"poem\",\n",
    "    \"llm\": \"gpt4\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a runnable with the `@chain` decorator\n",
    "\n",
    "Turn an arbitrary function into a chain by adding a @chain decorator. Equivalent to wrapping in a RunnableLambda.\n",
    "\n",
    "Improved observability by tracing chain correctly. Calls to runnables inside function traced as nested children.\n",
    "\n",
    "Allow use as runnable, compose in chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"Tell me a joke about {topic}\"\n",
    "prompt1 = prompts.ChatPromptTemplate.from_template(template1)\n",
    "template2 = \"What is the subject of this joke: {joke}\"\n",
    "prompt2 = prompts.ChatPromptTemplate.from_template(template2)\n",
    "\n",
    "@runnables.chain\n",
    "def custom_chain(text):\n",
    "  prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "  output1 = chat_models.chat_openai.invoke(prompt_val1)\n",
    "  parsed_output1 = output_parsers.StrOutputParser().invoke(output1)\n",
    "  \n",
    "  chain2 = prompt2 | chat_models.chat_openai | output_parsers.StrOutputParser()\n",
    "  return chain2.invoke({\"joke\": parsed_output1})\n",
    "\n",
    "# Check LangSmith traces for custom_chain trace with calls to OpenAI nested underneath.\n",
    "custom_chain.invoke(\"bears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add fallbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling LLM API Errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying errors to handle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallbacks for Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream custom generator functions\n",
    "\n",
    "Use generator functions (yield) in a LCEL pipeline.\n",
    "\n",
    "The signature of generators: Iterator[Input] -> Iterator[Output]. \n",
    "\n",
    "For async generators: AsyncIterator[Input] -> AsyncIterator[Output].\n",
    "\n",
    "useful for: \n",
    "\n",
    "- implementing custom output parser\n",
    "- modifying output of previous step, preserving streaming capabilities\n",
    "\n",
    "Implement custom output parser for comma-separated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "template = \"Write a comma-separated list of 5 animals simiar to: {animal}\" \n",
    "prompt = prompts.ChatPromptTemplate.from_template(template)\n",
    "model = chat_models.chat_openai\n",
    "chain = prompt | model | output_parsers.StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream({\"animal\": \"bear\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom parser splits an iterator of llm tokens into a list of strings \n",
    "# separated by commas\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
    "  # Hold partial input until we get a comma\n",
    "  buffer = \"\"\n",
    "  for chunk in input:\n",
    "    # Add the current chunk to the buffer\n",
    "    buffer += chunk\n",
    "    # While there are commas in the buffer\n",
    "    while \",\" in buffer:\n",
    "      # Split the buffer on the comma\n",
    "      comma_index = buffer.index(\",\")\n",
    "      # Yield everything before the comma\n",
    "      yield [buffer[:comma_index].strip()]\n",
    "      # Save the rest for the next iteration\n",
    "      buffer = buffer[comma_index + 1:]\n",
    "  # Yield the last chunk\n",
    "  yield [buffer.strip()]\n",
    "\n",
    "list_chain = chain | split_into_list\n",
    "\n",
    "for chunk in list_chain.stream({\"animal\": \"bear\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect runnables\n",
    "\n",
    "Inspecting a runnable with LCEL can help understand what is happening. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL does retrieval.\n",
    "vectorstore = vector_stores.chroma.Chroma.from_texts(\n",
    "  [\"harrison worked at kensho\"], embedding=text_embedding_models.OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\\\n",
    "\"\"\"\n",
    "prompt = prompts.ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = chat_models.chat_openai\n",
    "\n",
    "chain = (\n",
    "  {\n",
    "    \"context\": retriever, \"question\": runnables.RunnablePassthrough()\n",
    "  }\n",
    "  | prompt\n",
    "  | model\n",
    "  | output_parsers.StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a graph of the runnable\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}'))])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompts present in the chain.\n",
    "chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add message history (memory)\n",
    "\n",
    "The RunnableWithMessageHistory adds message history to chain by wrapping another Runnable and managing chat message history.\n",
    "\n",
    "Used for any Runnable that takes as input \n",
    "\n",
    "- a sequence of BaseMessage\n",
    "- a dict with a key that takes a sequence of BaseMessage\n",
    "- a dict with a key that takes the latest message(s) as a string or sequence of BaseMessage, and a separate key that takes historical messages.\n",
    "\n",
    "Output: Return a string content of AIMessage, sequence of BaseMessage, or dict with a key containing a sequence of BaseMessage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Construct a runnable (accepts a dict as input and returns a message as output).\n",
    "\n",
    "To manage message history, need: \n",
    "- Runnable; \n",
    "- Callable returning instance of BaseChatMessageHistory.\n",
    "\n",
    "Demonstrate using in-memory ChatMessageHistory and persistent storage using RedisChatMessageHistory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = chat_models.chat_openai\n",
    "template_msg = [\n",
    "  (\n",
    "    \"system\",\n",
    "     \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "  ),\n",
    "  prompts.MessagesPlaceholder(variable_name=\"history\"),\n",
    "  (\n",
    "    \"human\",\n",
    "    \"{input}\",\n",
    "  )\n",
    "]\n",
    "prompt = prompts.ChatPromptTemplate.from_messages(template_msg)\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\n",
      "Cosine is a math function that helps find the ratio of the adjacent side to the hypotenuse in a triangle.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chat history is stored in memory using a global Python dictionary.\n",
    "store = {}\n",
    "\n",
    "def get_session_history(\n",
    "  user_id: str, conversation_id: str\n",
    ") -> histories.BaseChatMessageHistory:\n",
    "  \"\"\"\n",
    "  Callable references a dict to return an instance of ChatMessageHistory. \n",
    "  \n",
    "  The arguments can be specified by passing a configuration to the \n",
    "  RunnableWithMessageHistory at runtime. \n",
    "  \n",
    "  The configuration parameters for tracking message histories can be customized \n",
    "  by passing a list of ConfigurableFieldSpec objects to the \n",
    "  history_factory_config parameter. \n",
    "  \n",
    "  Two parameters used are user_id and conversation_id.\n",
    "  \"\"\"\n",
    "  if (user_id, conversation_id) not in store:\n",
    "    store[(user_id, conversation_id)] = histories.ChatMessageHistory()\n",
    "  return store[(user_id, conversation_id)]\n",
    "\n",
    "with_message_history = runnables.RunnableWithMessageHistory(\n",
    "  runnable,\n",
    "  get_session_history,\n",
    "  input_messages_key=\"input\",  # latest input message\n",
    "  history_messages_key=\"history\",  # key to add historical messages to\n",
    "  history_factory_config=[\n",
    "    runnables.ConfigurableFieldSpec(\n",
    "      id=\"user_id\", annotation=str, name=\"User ID\", default=\"\",\n",
    "      description=\"Unique identifier for the user.\", is_shared=True,\n",
    "    ),\n",
    "    runnables.ConfigurableFieldSpec(\n",
    "      id=\"conversation_id\", annotation=str, name=\"Conversation ID\", default=\"\", \n",
    "      description=\"Unique identifier for the conversation.\", is_shared=True,\n",
    "    ),\n",
    "  ]\n",
    ")\n",
    "\n",
    "# When invoking new runnable, specify corresponding chat history via \n",
    "# configuration parameter.\n",
    "result1 = with_message_history.invoke(\n",
    "  { \"ability\": \"math\", \"input\": \"What does cosine mean?\" },\n",
    "  config={ \"configurable\": { \"user_id\": \"123\", \"conversation_id\": \"1\" } }\n",
    ")\n",
    "result2 = with_message_history.invoke(\n",
    "  { \"ability\": \"math\", \"input\": \"What?\" },\n",
    "  config={ \"configurable\": { \"user_id\": \"123\", \"conversation_id\": \"1\" } }\n",
    ")\n",
    "\n",
    "print(result1.content)\n",
    "print(result2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\n"
     ]
    }
   ],
   "source": [
    "# continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples with runnables of different signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistent storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt + LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a SQL DB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code writing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by semantic similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding moderation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing prompt size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
