{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import add_packages\n",
    "import psycopg2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "SQL_DB = os.getenv(\"SQL_DB\")\n",
    "SQL_HOST = os.getenv(\"SQL_HOST\")\n",
    "SQL_USER = os.getenv(\"SQL_USER\")\n",
    "SQL_PASSWORD = os.getenv(\"SQL_PASSWORD\")\n",
    "SQL_PORT = os.getenv(\"SQL_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "\tdatabase=SQL_DB, \n",
    "\thost=SQL_HOST, \n",
    "\tuser=SQL_USER,\n",
    "\tpassword=SQL_PASSWORD,\n",
    "\tport=SQL_PORT,\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "conn.close()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Literal\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from psycopg2.pool import SimpleConnectionPool\n",
    "\n",
    "\n",
    "class SQLDatabase:\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tdbname: str = os.getenv(\"SQL_DB\"),\n",
    "\t\tuser: str = os.getenv(\"SQL_USER\"),\n",
    "\t\tpassword: str = os.getenv(\"SQL_PASSWORD\"),\n",
    "\t\thost: str = os.getenv(\"SQL_HOST\"),\n",
    "\t\tport: str = os.getenv(\"SQL_PORT\"),\n",
    "\t\tmin_conn: int = 1,\n",
    "\t\tmax_conn: int = 1,\n",
    "\t):\n",
    "\t\tself.dbname = dbname\n",
    "\t\tself.user = user\n",
    "\t\tself.password = password\n",
    "\t\tself.host = host\n",
    "\t\tself.port = port\n",
    "\t\tself.logger = logging.getLogger(__name__)\n",
    "\n",
    "\t\tself.pool = SimpleConnectionPool(\n",
    "\t\t\tminconn=min_conn,\n",
    "\t\t\tmaxconn=max_conn,\n",
    "\t\t\tdsn=\"dbname={dbname} user={user} password={password} host={host} port={port}\".format(\n",
    "\t\t\t\tdbname=dbname, user=user, password=password, host=host, port=port\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\tdef execute_query(\n",
    "\t\t\tself,\n",
    "\t\t\tquery: str,\n",
    "\t\t\tparams: tuple = None,\n",
    "\t\t\tfetch_option: Literal[\"one\", \"many\", \"all\"] = \"all\",\n",
    "\t\t):\n",
    "\t\t\tconn: psycopg2.extensions.connection = None\n",
    "\t\t\tcur: RealDictCursor = None\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tconn = self.pool.getconn()\n",
    "\t\t\t\t\tcur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "\t\t\t\t\tif params:\n",
    "\t\t\t\t\t\t\tcur.execute(query, params)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tcur.execute(query)\n",
    "\t\t\t\t\tif fetch_option == \"one\":\n",
    "\t\t\t\t\t\t\treturn cur.fetchone()\n",
    "\t\t\t\t\telif fetch_option == \"many\":\n",
    "\t\t\t\t\t\t\treturn cur.fetchmany()\n",
    "\t\t\t\t\telif fetch_option == \"all\":\n",
    "\t\t\t\t\t\t\treturn cur.fetchall()\n",
    "\t\t\texcept (Exception, psycopg2.DatabaseError) as error:\n",
    "\t\t\t\t\tself.logger.error(f\"Error: {error}\")\n",
    "\t\t\t\t\treturn None\n",
    "\t\t\tfinally:\n",
    "\t\t\t\t\tif cur is not None:\n",
    "\t\t\t\t\t\t\tcur.close()\n",
    "\t\t\t\t\tif conn is not None:\n",
    "\t\t\t\t\t\t\tself.pool.putconn(conn)\n",
    "\n",
    "\tdef execute_commit(self, query, params=None):\n",
    "\t\tconn: psycopg2.extensions.connection = None\n",
    "\t\tcur: RealDictCursor = None\n",
    "\t\ttry:\n",
    "\t\t\tconn = self.pool.getconn()\n",
    "\t\t\tcur = conn.cursor()\n",
    "\t\t\tif params:\n",
    "\t\t\t\tcur.execute(query, params)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcur.execute(query)\n",
    "\t\t\tconn.commit()\n",
    "\t\t\treturn cur.rowcount\n",
    "\t\texcept (Exception, psycopg2.DatabaseError) as error:\n",
    "\t\t\tself.logger.error(f\"Error: {error}\")\n",
    "\t\t\tif conn is not None:\n",
    "\t\t\t\tconn.rollback()\n",
    "\t\t\treturn None\n",
    "\t\tfinally:\n",
    "\t\t\tif cur is not None:\n",
    "\t\t\t\tcur.close()\n",
    "\t\t\tif conn is not None:\n",
    "\t\t\t\tself.pool.putconn(conn)\n",
    "\t\n",
    "\tdef get_uri(self):\n",
    "\t\t\t\treturn f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.dbname}\"\n",
    "\n",
    "\t\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SQLTable:\n",
    "\t\tdef __init__(self, name: str, schema: list[str], db: SQLDatabase):\n",
    "\t\t\t\tself.name = name\n",
    "\t\t\t\tself.schema = schema\n",
    "\t\t\t\tself.db = db\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tschema_users = [\n",
    "\t\t\t\t\t\"id SERIAL\",\n",
    "\t\t\t\t\t\"username VARCHAR(50) UNIQUE NOT NULL\",\n",
    "\t\t\t\t\t\"email VARCHAR(255) UNIQUE NOT NULL\",\n",
    "\t\t\t\t\t\"password_hash VARCHAR(128) NOT NULL\",\n",
    "\t\t\t\t\t\"created_at TIMESTAMP DEFAULT NOW()\",\n",
    "\t\t\t\t\t\"updated_at TIMESTAMP DEFAULT NOW()\",\n",
    "\t\t\t\t\t\"PRIMARY KEY (id)\"\n",
    "\t\t\t\t]\n",
    "\t\t\t\t\"\"\"\n",
    "\n",
    "\t\t\t\t# Extract column names from schema and store them in a separate list\n",
    "\t\t\t\tself.col_names = [col.split()[0] for col in schema if col.split()[0] not in [\"PRIMARY\", \"FOREIGN\"]]\n",
    "\n",
    "\t\t\t\t# Store the timezone of the local system\n",
    "\t\t\t\tself.tz = datetime.timezone.utc if datetime.timezone.utc.tzname(None) == \"UTC\" else datetime.timezone(datetime.timedelta(seconds=time.timezonezone(None)))\n",
    "\n",
    "\t\tdef get_schema(self, is_get_all=False):\n",
    "\t\t\t\t\"\"\"Get the schema of the table.\n",
    "\n",
    "\t\t\t\tArgs:\n",
    "\t\t\t\t\t\tis_get_all (bool, optional): If True, returns all column information. If False, returns only essential column information. Defaults to False.\n",
    "\n",
    "\t\t\t\tReturns:\n",
    "\t\t\t\t\t\tlist: A list of dictionaries containing column information.\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tquery = f\"\"\"\n",
    "\t\t\t\t\t\tSELECT *\n",
    "\t\t\t\t\t\tFROM information_schema.columns\n",
    "\t\t\t\t\t\tWHERE table_name = '{self.name}'\n",
    "\t\t\t\t\t\tORDER BY ordinal_position;\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tschema = self.db.execute_query(query, fetch_option=\"all\")\n",
    "\t\t\t\trows = []\n",
    "\t\t\t\tfor row in schema:\n",
    "\t\t\t\t\t\tif is_get_all:\n",
    "\t\t\t\t\t\t\t\trow_info = {key: val for key, val in row.items() if val is not None}\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\trow_info = {\n",
    "\t\t\t\t\t\t\t\t\t\t'table_catalog': row['table_catalog'],\n",
    "\t\t\t\t\t\t\t\t\t\t'table_schema': row['table_schema'],\n",
    "\t\t\t\t\t\t\t\t\t\t'table_name': row['table_name'],\n",
    "\t\t\t\t\t\t\t\t\t\t'column_name': row['column_name'],\n",
    "\t\t\t\t\t\t\t\t\t\t'is_nullable': row['is_nullable'],\n",
    "\t\t\t\t\t\t\t\t\t\t'data_type': row['data_type']\n",
    "\t\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\trows.append(row_info)\n",
    "\t\t\t\treturn rows\n",
    "\t\n",
    "\t\tdef create(self):\n",
    "\t\t\t\tquery = f\"CREATE TABLE IF NOT EXISTS {self.name} (\"\n",
    "\t\t\t\tfor i, col in enumerate(self.schema):\n",
    "\t\t\t\t\t\tquery += f\"{col} \"\n",
    "\t\t\t\t\t\tif i < len(self.schema) - 1:\n",
    "\t\t\t\t\t\t\t\tquery += \", \"\n",
    "\t\t\t\tquery += \")\"\n",
    "\t\t\t\tself.db.execute_commit(query)\n",
    "\n",
    "\t\tdef insert(self, data: dict):\n",
    "\t\t\t\t# Exclude the 'id SERIAL', 'created_at', and 'updated_at' columns from the list of columns and placeholders\n",
    "\t\t\t\tcols = \", \".join(col for col in self.col_names if col not in [\"id\", \"created_at\", \"updated_at\"])\n",
    "\t\t\t\tplaceholders = \", \".join(\"%s\" for col in self.col_names if col not in [\"id\", \"created_at\", \"updated_at\"])\n",
    "\n",
    "\t\t\t\t# Convert the timestamps to JSON objects with a timezone offset\n",
    "\t\t\t\tnow = datetime.datetime.now(self.tz)\n",
    "\t\t\t\tdata[\"created_at\"] = self._to_json_date(now)\n",
    "\t\t\t\tdata[\"updated_at\"] = self._to_json_date(now)\n",
    "\n",
    "\t\t\t\tquery = f\"INSERT INTO {self.name} ({cols}) VALUES ({placeholders})\"\n",
    "\t\t\t\tself.db.execute_commit(query, tuple(data[col] for col in self.col_names if col not in [\"id\", \"created_at\", \"updated_at\"]))\n",
    "\n",
    "\t\tdef delete_by_col(self, col: str, value: str):\n",
    "\t\t\t\t\"\"\"Delete rows from the table that match a given column value.\"\"\"\n",
    "\t\t\t\tcondition = f\"{col} = %s\"\n",
    "\t\t\t\tparams = (value,)\n",
    "\t\t\t\tself.delete(condition, params)\n",
    "\n",
    "\t\tdef update_by_col(self, col: str, value: str, data: dict):\n",
    "\t\t\t\t\"\"\"Update rows in the table that match a given column value.\"\"\"\n",
    "\t\t\t\tcondition = f\"{col} = %s\"\n",
    "\t\t\t\tparams = (value,)\n",
    "\n",
    "\t\t\t\t# Generate a list of column-value pairs with placeholders\n",
    "\t\t\t\tcol_value_pairs = [f\"{col}=%s\" for col in data]\n",
    "\n",
    "\t\t\t\t# Substitute the placeholders with the actual values\n",
    "\t\t\t\tfor i, col_value_pair in enumerate(col_value_pairs):\n",
    "\t\t\t\t\t\tcol_value_pairs[i] = col_value_pair % (data[col],)\n",
    "\n",
    "\t\t\t\t# Join the column-value pairs into a single string\n",
    "\t\t\t\tcols = \", \".join(col_value_pairs)\n",
    "\n",
    "\t\t\t\tquery = f\"UPDATE {self.name} SET {cols} WHERE {condition}\"\n",
    "\t\t\t\tself.db.execute_commit(query, params + tuple(data[col] for col in data))\n",
    "\n",
    "\t\tdef delete(self, condition: str, params: tuple = None):\n",
    "\t\t\t\tquery = f\"DELETE FROM {self.name} WHERE {condition}\"\n",
    "\t\t\t\tself.db.execute_commit(query, params)\n",
    "\n",
    "\t\tdef update(self, data: dict, condition: str, params: tuple = None):\n",
    "\t\t\t\tcols = \", \".join(f\"{col}=%s\" for col in data)\n",
    "\t\t\t\tquery = f\"UPDATE {self.name} SET {cols} WHERE {condition}\"\n",
    "\t\t\t\tall_params = tuple(data[col] for col in data) + (params if params is not None else ())\n",
    "\t\t\t\tself.db.execute_commit(query, all_params)\n",
    "\n",
    "\t\tdef query(self, query: str, params: tuple = None, fetch_option: Literal[\"one\", \"many\", \"all\"] = \"all\"):\n",
    "\t\t\t\tcur = self.db.execute_query(query, params)\n",
    "\t\t\t\tif fetch_option == \"one\":\n",
    "\t\t\t\t\t\treturn cur.fetchone()\n",
    "\t\t\t\telif fetch_option == \"many\":\n",
    "\t\t\t\t\t\treturn cur.fetchmany()\n",
    "\t\t\t\telif fetch_option == \"all\":\n",
    "\t\t\t\t\t\treturn cur.fetchall()\n",
    "\n",
    "\t\tdef _to_json_date(self, dt: datetime.datetime):\n",
    "\t\t\t\t\"\"\"Convert a datetime.datetime object to a JSON object with a timezone offset.\"\"\"\n",
    "\t\t\t\treturn json.dumps({\"$date\": dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + self.tz.tzname(None)})\n",
    "\n",
    "\t\tdef _from_json_date(self, json_str: str):\n",
    "\t\t\t\t\"\"\"Convert a JSON object with a timezone offset to a datetime.datetime object.\"\"\"\n",
    "\t\t\t\tobj = json.loads(json_str)\n",
    "\t\t\t\tdt_str = obj[\"$date\"][:-6] + obj[\"$date\"][-5:]\n",
    "\t\t\t\tdt = datetime.datetime.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "\t\t\t\ttz_str = obj[\"$date\"][-6:]\n",
    "\t\t\t\ttz = pytz.timezone(tz_str) if tz_str != \"UTC\" else datetime.timezone.utc\n",
    "\t\t\t\treturn dt.replace(tzinfo=tz)\n",
    "\n",
    "\t\tdef insert_from_dataframe(self, df: pd.DataFrame):\n",
    "\t\t\t\t\"\"\"Insert rows from a Pandas DataFrame into the table.\"\"\"\n",
    "\t\t\t\t# Exclude the 'id SERIAL', 'created_at', and 'updated_at' columns from the DataFrame\n",
    "\t\t\t\tdf = df.loc[:, df.columns.intersection(set(self.col_names) - {\"id\", \"created_at\", \"updated_at\"})]\n",
    "\n",
    "\t\t\t\t# # Check if all the required columns are present in the DataFrame\n",
    "\t\t\t\t# required_cols = {\"username\", \"email\", \"password_hash\"}\n",
    "\t\t\t\t# if not required_cols.issubset(df.columns):\n",
    "\t\t\t\t# \t\traise ValueError(\"Missing required columns in the DataFrame\")\n",
    "\n",
    "\t\t\t\t# Convert the timestamps to PostgreSQL timestamp strings\n",
    "\t\t\t\tnow = datetime.datetime.now(self.tz)\n",
    "\t\t\t\tif \"created_at\" in self.col_names:\n",
    "\t\t\t\t\tdf[\"created_at\"] = [self._to_pg_timestamp(now)] * len(df)\n",
    "\t\t\t\tif \"updated_at\" in self.col_names:\n",
    "\t\t\t\t\tdf[\"updated_at\"] = [self._to_pg_timestamp(now)] * len(df)\n",
    "\n",
    "\t\t\t\t# Generate a list of column names\n",
    "\t\t\t\tcols = \", \".join(df.columns)\n",
    "\n",
    "\t\t\t\t# Construct the SQL query with the %s placeholders\n",
    "\t\t\t\tquery = f\"INSERT INTO {self.name} ({cols}) VALUES ({', '.join(['%s'] * len(df.columns))})\"\n",
    "\n",
    "\t\t\t\t# Convert the DataFrame to a list of tuples\n",
    "\t\t\t\tvalues = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "\t\t\t\t# Execute the query with the list of tuples\n",
    "\t\t\t\tfor value in values:\n",
    "\t\t\t\t\t\tself.db.execute_commit(query, value)\n",
    "\n",
    "\t\tdef _to_pg_timestamp(self, dt: datetime.datetime):\n",
    "\t\t\t\t\"\"\"Convert a datetime.datetime object to a PostgreSQL timestamp string.\"\"\"\n",
    "\t\t\t\treturn dt.astimezone(self.tz).strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "\t\tdef _from_pg_timestamp(self, timestamp_str: str):\n",
    "\t\t\t\t\"\"\"Convert a PostgreSQL timestamp string to a datetime.datetime object.\"\"\"\n",
    "\t\t\t\tdt = datetime.datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "\t\t\t\treturn dt.replace(tzinfo=self.tz)\n",
    "\t\t\t\n",
    "\t\tdef insert_from_csv(self, csv_path: str):\n",
    "\t\t\t\t\"\"\"Insert rows from a CSV file into the table.\"\"\"\n",
    "\n",
    "\t\t\t\t# Read the CSV file into a Pandas DataFrame\n",
    "\t\t\t\tdf = pd.read_csv(csv_path)\n",
    "\n",
    "\t\t\t\t# Insert the rows from the DataFrame into the table\n",
    "\t\t\t\tself.insert_from_dataframe(df)\n",
    "\n",
    "\t\tdef insert_from_excel(self, excel_path: str):\n",
    "\t\t\t\t\"\"\"Insert rows from a Excel file into the table.\"\"\"\n",
    "\n",
    "\t\t\t\t# Read the CSV file into a Pandas DataFrame\n",
    "\t\t\t\tdf = pd.read_excel(excel_path)\n",
    "\n",
    "\t\t\t\t# Insert the rows from the DataFrame into the table\n",
    "\t\t\t\tself.insert_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sql = SQLDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql://myuser:mysecretpassword@127.0.0.1:5432/mydatabase'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sql.get_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sql.execute_commit(\n",
    "\t\"\"\"CREATE TABLE datacamp_courses(\n",
    "\tcourse_id SERIAL PRIMARY KEY,\n",
    "\tcourse_name VARCHAR (50) UNIQUE NOT NULL,\n",
    "\tcourse_instructor VARCHAR (100) NOT NULL,\n",
    "\ttopic VARCHAR (20) NOT NULL);\n",
    "\t\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_users = [\n",
    "\t\"id SERIAL\",\n",
    "\t\"username VARCHAR(50) UNIQUE NOT NULL\",\n",
    "\t\"email VARCHAR(255) UNIQUE NOT NULL\",\n",
    "\t\"password_hash VARCHAR(128) NOT NULL\",\n",
    "\t\"created_at TIMESTAMP DEFAULT NOW()\",\n",
    "\t\"updated_at TIMESTAMP DEFAULT NOW()\",\n",
    "\t\"PRIMARY KEY (id)\"\n",
    "]\n",
    "\n",
    "table_users = SQLTable(\n",
    "\tname=\"users\", \n",
    "\tschema=schema_users,\n",
    "\tdb=my_sql\n",
    ")\n",
    "\n",
    "table_users.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_users.get_schema(is_get_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = {\n",
    "\t\"username\": \"johndoe\",\n",
    "\t\"email\": \"johndoe@example.com\",\n",
    "\t\"password_hash\": \"12345678\"  # This should be a hashed version of the user's password\n",
    "}\n",
    "table_users.insert(new_user)\n",
    "\n",
    "new_user = {\n",
    "\t\"username\": \"johndoee\",\n",
    "\t\"email\": \"johndoee@example.com\",\n",
    "\t\"password_hash\": \"12345678\"  # This should be a hashed version of the user's password\n",
    "}\n",
    "table_users.insert(new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a user with the email \"johndoeeee@example.com\"\n",
    "table_users.delete_by_col(\"email\", \"johndoee@example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a user with the username \"johndoe\" and email \"johndoe@example.com\"\n",
    "data = {\n",
    "\t\"username\": \"johndoe_new\",\n",
    "\t\"email\": \"johndoe_new@example.com\",\n",
    "\t\"password_hash\": \"12345678\"  # This should be a hashed version of the user's password\n",
    "}\n",
    "table_users.update_by_col(\"username\", \"johndoe\", data)\n",
    "\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert rows from a DataFrame\n",
    "df = pd.DataFrame({\n",
    "  \"username\": [\"johndoe5\", \"johndoe6\"], \n",
    "  \"email\": [\"johndoe5@example.com\", \"johndoe6@example.com\"], \n",
    "  \"password_hash\": [\"12345678\", \"87654321\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_users.insert_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "class SQLTable:\n",
    "\t\tdef __init__(self, name: str, schema: list, db: SQLDatabase):\n",
    "\t\t\t\tself.name = name\n",
    "\t\t\t\tself.schema = schema\n",
    "\t\t\t\tself.db = db\n",
    "\n",
    "\t\t\t\t# Extract column names from schema and store them in a separate list\n",
    "\t\t\t\tself.col_names = [col.split()[0] for col in schema if col.split()[0] not in [\"PRIMARY\", \"FOREIGN\"]]\n",
    "\n",
    "\t\t\t\t# Store the timezone of the local system\n",
    "\t\t\t\tself.tz = datetime.timezone.utc if datetime.timezone.utc.tzname(None) == \"UTC\" else datetime.timezone(datetime.timedelta(seconds=time.timezonezone(None)))\n",
    "\n",
    "\t\tdef get_schema(self, is_get_all=False):\n",
    "\t\t\t\t\"\"\"Get the schema of the table.\n",
    "\n",
    "\t\t\t\tArgs:\n",
    "\t\t\t\t\t\tis_get_all (bool, optional): If True, returns all column information. If False, returns only essential column information. Defaults to False.\n",
    "\n",
    "\t\t\t\tReturns:\n",
    "\t\t\t\t\t\tlist: A list of dictionaries containing column information.\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tquery = f\"\"\"\n",
    "\t\t\t\t\t\tSELECT *\n",
    "\t\t\t\t\t\tFROM information_schema.columns\n",
    "\t\t\t\t\t\tWHERE table_name = '{self.name}'\n",
    "\t\t\t\t\t\tORDER BY ordinal_position;\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tschema = self.db.execute_query(query, fetch_option=\"all\")\n",
    "\t\t\t\trows = []\n",
    "\t\t\t\tfor row in schema:\n",
    "\t\t\t\t\t\tif is_get_all:\n",
    "\t\t\t\t\t\t\t\trow_info = {key: val for key, val in row.items() if val is not None}\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\trow_info = {\n",
    "\t\t\t\t\t\t\t\t\t\t'table_catalog': row['table_catalog'],\n",
    "\t\t\t\t\t\t\t\t\t\t'table_schema': row['table_schema'],\n",
    "\t\t\t\t\t\t\t\t\t\t'table_name': row['table_name'],\n",
    "\t\t\t\t\t\t\t\t\t\t'column_name': row['column_name'],\n",
    "\t\t\t\t\t\t\t\t\t\t'is_nullable': row['is_nullable'],\n",
    "\t\t\t\t\t\t\t\t\t\t'data_type': row['data_type']\n",
    "\t\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\trows.append(row_info)\n",
    "\t\t\t\treturn rows\n",
    "  \n",
    "\t\tdef create(self):\n",
    "\t\t\t\tquery = f\"CREATE TABLE IF NOT EXISTS {self.name} (\"\n",
    "\t\t\t\tfor i, col in enumerate(self.schema):\n",
    "\t\t\t\t\t\tquery += f\"{col} \"\n",
    "\t\t\t\t\t\tif i < len(self.schema) - 1:\n",
    "\t\t\t\t\t\t\t\tquery += \", \"\n",
    "\t\t\t\tquery += \")\"\n",
    "\t\t\t\tself.db.execute_commit(query)\n",
    "\n",
    "\t\tdef insert(self, data: dict):\n",
    "\t\t\t\t# Exclude the 'id SERIAL', 'created_at', and 'updated_at' columns from the list of columns and placeholders\n",
    "\t\t\t\tcols = \", \".join(col for col in self.col_names if col not in [\"id\", \"created_at\", \"updated_at\"])\n",
    "\t\t\t\tplaceholders = \", \".join(\"%s\" for col in self.col_names if col not in [\"id\", \"created_at\", \"updated_at\"])\n",
    "\n",
    "\t\t\t\t# Convert the timestamps to JSON objects with a timezone offset\n",
    "\t\t\t\tnow = datetime.datetime.now(self.tz)\n",
    "\t\t\t\tdata[\"created_at\"] = self._to_json_date(now)\n",
    "\t\t\t\tdata[\"updated_at\"] = self._to_json_date(now)\n",
    "\n",
    "\t\t\t\tquery = f\"INSERT INTO {self.name} ({cols}) VALUES ({placeholders})\"\n",
    "\t\t\t\tself.db.execute_commit(query, tuple(data[col] for col in self.col_names if col not in [\"id\", \"created_at\", \"updated_at\"]))\n",
    "\n",
    "\t\tdef delete_by_col(self, col: str, value: str):\n",
    "\t\t\t\t\"\"\"Delete rows from the table that match a given column value.\"\"\"\n",
    "\t\t\t\tcondition = f\"{col} = %s\"\n",
    "\t\t\t\tparams = (value,)\n",
    "\t\t\t\tself.delete(condition, params)\n",
    "\n",
    "\t\tdef update_by_col(self, col: str, value: str, data: dict):\n",
    "\t\t\t\t\"\"\"Update rows in the table that match a given column value.\"\"\"\n",
    "\t\t\t\tcondition = f\"{col} = %s\"\n",
    "\t\t\t\tparams = (value,)\n",
    "\n",
    "\t\t\t\t# Generate a list of column-value pairs with placeholders\n",
    "\t\t\t\tcol_value_pairs = [f\"{col}=%s\" for col in data]\n",
    "\n",
    "\t\t\t\t# Substitute the placeholders with the actual values\n",
    "\t\t\t\tfor i, col_value_pair in enumerate(col_value_pairs):\n",
    "\t\t\t\t\t\tcol_value_pairs[i] = col_value_pair % (data[col],)\n",
    "\n",
    "\t\t\t\t# Join the column-value pairs into a single string\n",
    "\t\t\t\tcols = \", \".join(col_value_pairs)\n",
    "\n",
    "\t\t\t\tquery = f\"UPDATE {self.name} SET {cols} WHERE {condition}\"\n",
    "\t\t\t\tself.db.execute_commit(query, params + tuple(data[col] for col in data))\n",
    "\n",
    "\t\tdef delete(self, condition: str, params: tuple = None):\n",
    "\t\t\t\tquery = f\"DELETE FROM {self.name} WHERE {condition}\"\n",
    "\t\t\t\tself.db.execute_commit(query, params)\n",
    "\n",
    "\t\tdef update(self, data: dict, condition: str, params: tuple = None):\n",
    "\t\t\t\tcols = \", \".join(f\"{col}=%s\" for col in data)\n",
    "\t\t\t\tquery = f\"UPDATE {self.name} SET {cols} WHERE {condition}\"\n",
    "\t\t\t\tall_params = tuple(data[col] for col in data) + (params if params is not None else ())\n",
    "\t\t\t\tself.db.execute_commit(query, all_params)\n",
    "\n",
    "\t\tdef query(self, query: str, params: tuple = None, fetch_option: Literal[\"one\", \"many\", \"all\"] = \"all\"):\n",
    "\t\t\t\tcur = self.db.execute_query(query, params)\n",
    "\t\t\t\tif fetch_option == \"one\":\n",
    "\t\t\t\t\t\treturn cur.fetchone()\n",
    "\t\t\t\telif fetch_option == \"many\":\n",
    "\t\t\t\t\t\treturn cur.fetchmany()\n",
    "\t\t\t\telif fetch_option == \"all\":\n",
    "\t\t\t\t\t\treturn cur.fetchall()\n",
    "\n",
    "\t\tdef _to_json_date(self, dt: datetime.datetime):\n",
    "\t\t\t\t\"\"\"Convert a datetime.datetime object to a JSON object with a timezone offset.\"\"\"\n",
    "\t\t\t\treturn json.dumps({\"$date\": dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + self.tz.tzname(None)})\n",
    "\n",
    "\t\tdef _from_json_date(self, json_str: str):\n",
    "\t\t\t\t\"\"\"Convert a JSON object with a timezone offset to a datetime.datetime object.\"\"\"\n",
    "\t\t\t\tobj = json.loads(json_str)\n",
    "\t\t\t\tdt_str = obj[\"$date\"][:-6] + obj[\"$date\"][-5:]\n",
    "\t\t\t\tdt = datetime.datetime.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "\t\t\t\ttz_str = obj[\"$date\"][-6:]\n",
    "\t\t\t\ttz = pytz.timezone(tz_str) if tz_str != \"UTC\" else datetime.timezone.utc\n",
    "\t\t\t\treturn dt.replace(tzinfo=tz)\n",
    "\n",
    "\t\tdef insert_from_dataframe(self, df: pd.DataFrame):\n",
    "\t\t\t\t\"\"\"Insert rows from a Pandas DataFrame into the table.\"\"\"\n",
    "\t\t\t\t# Exclude the 'id SERIAL', 'created_at', and 'updated_at' columns from the DataFrame\n",
    "\t\t\t\tdf = df.loc[:, df.columns.intersection(set(self.col_names) - {\"id\", \"created_at\", \"updated_at\"})]\n",
    "\n",
    "\t\t\t\t# Check if all the required columns are present in the DataFrame\n",
    "\t\t\t\trequired_cols = {\"username\", \"email\", \"password_hash\"}\n",
    "\t\t\t\tif not required_cols.issubset(df.columns):\n",
    "\t\t\t\t\t\traise ValueError(\"Missing required columns in the DataFrame\")\n",
    "\n",
    "\t\t\t\t# Convert the timestamps to PostgreSQL timestamp strings\n",
    "\t\t\t\tnow = datetime.datetime.now(self.tz)\n",
    "\t\t\t\tdf[\"created_at\"] = [self._to_pg_timestamp(now)] * len(df)\n",
    "\t\t\t\tdf[\"updated_at\"] = [self._to_pg_timestamp(now)] * len(df)\n",
    "\n",
    "\t\t\t\t# Generate a list of column names\n",
    "\t\t\t\tcols = \", \".join(df.columns)\n",
    "\n",
    "\t\t\t\t# Construct the SQL query with the %s placeholders\n",
    "\t\t\t\tquery = f\"INSERT INTO {self.name} ({cols}) VALUES ({', '.join(['%s'] * len(df.columns))})\"\n",
    "\n",
    "\t\t\t\t# Convert the DataFrame to a list of tuples\n",
    "\t\t\t\tvalues = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "\t\t\t\t# Execute the query with the list of tuples\n",
    "\t\t\t\tfor value in values:\n",
    "\t\t\t\t\t\tself.db.execute_commit(query, value)\n",
    "\n",
    "\t\tdef _to_pg_timestamp(self, dt: datetime.datetime):\n",
    "\t\t\t\t\"\"\"Convert a datetime.datetime object to a PostgreSQL timestamp string.\"\"\"\n",
    "\t\t\t\treturn dt.astimezone(self.tz).strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "\t\tdef _from_pg_timestamp(self, timestamp_str: str):\n",
    "\t\t\t\t\"\"\"Convert a PostgreSQL timestamp string to a datetime.datetime object.\"\"\"\n",
    "\t\t\t\tdt = datetime.datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "\t\t\t\treturn dt.replace(tzinfo=self.tz)\n",
    "      \n",
    "\t\tdef insert_from_csv(self, csv_path: str):\n",
    "\t\t\t\t\"\"\"Insert rows from a CSV file into the table.\"\"\"\n",
    "\n",
    "\t\t\t\t# Read the CSV file into a Pandas DataFrame\n",
    "\t\t\t\tdf = pd.read_csv(csv_path)\n",
    "\n",
    "\t\t\t\t# Insert the rows from the DataFrame into the table\n",
    "\t\t\t\tself.insert_from_dataframe(df)\n",
    "\n",
    "\t\tdef insert_from_excel(self, excel_path: str):\n",
    "\t\t\t\t\"\"\"Insert rows from a Excel file into the table.\"\"\"\n",
    "\n",
    "\t\t\t\t# Read the CSV file into a Pandas DataFrame\n",
    "\t\t\t\tdf = pd.read_excel(excel_path)\n",
    "\n",
    "\t\t\t\t# Insert the rows from the DataFrame into the table\n",
    "\t\t\t\tself.insert_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(f\"../data/tdtu/FEEE/NhanSu.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos\n",
    "\n",
    "- [ ] https://www.datacamp.com/tutorial/tutorial-postgresql-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
