{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import add_packages\n",
    "import config\n",
    "from pprint import pprint\n",
    "\n",
    "from my_langchain import (\n",
    "    agent_tools, document_loaders, text_splitters, text_embedding_models, vector_stores,\n",
    "    chat_models, prompts, utils, output_parsers, agents, documents, runnables,\n",
    "    llms, histories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "Build an agent with two tools: one for online searches and one for specific data retrieval from an index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tools needed: Tavily for online search and a retriever for local index.\n",
    "\n",
    "tool_tavily_search = agent_tools.tavily_search_results()\n",
    "\n",
    "# Create a retriever over data. \n",
    "loader = document_loaders.WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "document = loader.load()\n",
    "documents = text_splitters.RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000, chunk_overlap=200,\n",
    ").split_documents(document)\n",
    "\n",
    "embeddings = text_embedding_models.OpenAIEmbeddings()\n",
    "vectorstore = vector_stores.chroma.Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "tool_retriever = vector_stores.create_retriever_tool(\n",
    "  retriever=retriever,\n",
    "  name=\"langsmith_search\",\n",
    "  description=\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "\n",
    "# List of tools will use downstream.\n",
    "tools = [\n",
    "  tool_tavily_search, \n",
    "  tool_retriever,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose LLM guiding agent.\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "# Choose the prompt to guide the agent.\n",
    "prompt = prompts.hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# Initialize the agent with the LLM, prompt, and tools. \n",
    "# The agent takes in input and decides on actions. \n",
    "# AgentExecutor execute actions for Agent\n",
    "agent = agents.create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run agent on stateless queries.\n",
    "agent_executor.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in memory\n",
    "\n",
    "This agent is stateless, does not remember previous interactions. To give it memory, pass in previous chat_history. It needs to be called chat_history because of the prompt used. If a different prompt is used, the variable name could be changed.\n",
    "\n",
    "Keep track of messages automatically by wrapping in a RunnableWithMessageHistory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history is stored in memory using a global Python dictionary.\n",
    "store = {}\n",
    "\n",
    "def get_session_history(\n",
    "  user_id: str, conversation_id: str\n",
    ") -> histories.BaseChatMessageHistory:\n",
    "  \"\"\"\n",
    "  Callable references a dict to return an instance of ChatMessageHistory. \n",
    "  \n",
    "  The arguments can be specified by passing a configuration to the \n",
    "  RunnableWithMessageHistory at runtime. \n",
    "  \n",
    "  The configuration parameters for tracking message histories can be customized \n",
    "  by passing a list of ConfigurableFieldSpec objects to the \n",
    "  history_factory_config parameter. \n",
    "  \n",
    "  Two parameters used are user_id and conversation_id.\n",
    "  \"\"\"\n",
    "  if (user_id, conversation_id) not in store:\n",
    "    store[(user_id, conversation_id)] = histories.ChatMessageHistory()\n",
    "  return store[(user_id, conversation_id)]\n",
    "\n",
    "agent_with_memory = runnables.RunnableWithMessageHistory(\n",
    "  agent_executor,\n",
    "  get_session_history,\n",
    "  input_messages_key=\"input\",  # latest input message\n",
    "  history_messages_key=\"history\",  # key to add historical messages to\n",
    "  history_factory_config=[\n",
    "    runnables.ConfigurableFieldSpec(\n",
    "      id=\"user_id\", annotation=str, name=\"User ID\", default=\"\",\n",
    "      description=\"Unique identifier for the user.\", is_shared=True,\n",
    "    ),\n",
    "    runnables.ConfigurableFieldSpec(\n",
    "      id=\"conversation_id\", annotation=str, name=\"Conversation ID\", default=\"\", \n",
    "      description=\"Unique identifier for the conversation.\", is_shared=True,\n",
    "    ),\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(agent_with_memory.invoke(\n",
    "    {\"input\": \"Hi, I'm Bob\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
    "))\n",
    "\n",
    "print(agent_with_memory.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = histories.ChatMessageHistory()\n",
    "\n",
    "agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"hi! I'm bob\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")\n",
    "\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"what's my name?\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI tools\n",
    "\n",
    "OpenAI models detect function calls and provide input for API calls. Model outputs JSON object with function arguments for more reliable and useful function calls.\n",
    "\n",
    "OpenAI termed capability to invoke single function as functions, capability to invoke one or more functions as tools.\n",
    "\n",
    "Using tools allows the model to request multiple functions to be called when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tools\n",
    "tool_tavily_search = agent_tools.TavilySearchResults(max_results=1)\n",
    "tools = [\n",
    "  tool_tavily_search\n",
    "]\n",
    "\n",
    "# Create Agent\n",
    "prompt = prompts.hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = agents.create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# Run Agent\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"What is LangcChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct\n",
    "\n",
    "Using an agent to implement the [ReAct](https://react-lm.github.io/) logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "tool_tavily_search = agent_tools.TavilySearchResults(max_results=1)\n",
    "tools = [\n",
    "  tool_tavily_search,\n",
    "]\n",
    "\n",
    "# Create Agent\n",
    "prompt = prompts.hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = agents.create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using with chat history\n",
    "prompt = prompts.hub.pull(\"hwchase17/react-chat\")\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = agents.create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "message_history = histories.ChatMessageHistory()\n",
    "\n",
    "agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "questions = [\n",
    "  \"hi! I'm bob\",\n",
    "  \"what's my name?\",\n",
    "  \"Is there any actor with the same name as me?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": questions[2]},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-ask with search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tools\n",
    "tool_tavily_answer = agent_tools.TavilyAnswer(max_results=1, name=\"Intermediate Answer\")\n",
    "tools = [\n",
    "  tool_tavily_answer\n",
    "]\n",
    "\n",
    "# Create Agent\n",
    "prompt = prompts.hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "# Construct the Self Ask With Search Agent\n",
    "agent = agents.create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True,\n",
    "                                      handle_parsing_errors=True)\n",
    "\n",
    "# Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"What is the hometown of the reigning men's U.S. Open champion?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Chat Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent using OpenAI Tool Calling for reliability.\n",
    "# Create it without memory, then add memory for conversation.\n",
    "\n",
    "#* Load LLM\n",
    "# Load the language model used to control the agent.\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "#* Define Tools\n",
    "# Function docstring is important.\n",
    "# Python function to calculate word length.\n",
    "@agent_tools.tool\n",
    "def get_word_length(word: str) -> int:\n",
    "  \"\"\"Returns the length of a word.\"\"\"\n",
    "  return len(word)\n",
    "\n",
    "tools = [\n",
    "  get_word_length,\n",
    "]\n",
    "\n",
    "#* Create Prompt for OpenAI Function Calling.\n",
    "# Input variables: \n",
    "# - input: user objective, string\n",
    "# - agent_scratchpad: sequence of previous agent tool invocations and outputs (messages)\n",
    "prompt = prompts.ChatPromptTemplate.from_messages([\n",
    "  (\n",
    "    \"system\",\n",
    "    \"You are very powerful assistant, but don't know current events.\"\n",
    "  ),\n",
    "  (\n",
    "    \"user\",\n",
    "    \"{input}\"\n",
    "  ),\n",
    "  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "#* Bind tools to LLM\n",
    "# How agent knows tools can be used by relying on OpenAI tool calling LLMs. \n",
    "# Tools are passed in OpenAI tool format to the model by binding functions to \n",
    "# ensure they are passed each time the model is invoked.\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# * Create Agent & Adding memory\n",
    "# Utility functions: \n",
    "# - Component for formatting intermediate steps (agent action, tool output\n",
    "# pairs) to input messages sent to model\n",
    "# - Component for converting output message into agent action/agent finish.\n",
    "agent = (\n",
    "  {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"agent_scratchpad\": lambda x: agents.format_to_openai_tool_messages(\n",
    "      x[\"intermediate_steps\"]\n",
    "    ),\n",
    "  }\n",
    "  | prompt \n",
    "  | llm_with_tools\n",
    "  | agents.OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "message_history = histories.ChatMessageHistory()\n",
    "agent_with_chat_history = runnables.RunnableWithMessageHistory(\n",
    "  agent_executor,\n",
    "  lambda session_id: message_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "questions = [\n",
    "  \"Hello\",\n",
    "  \"My name is Bob\",\n",
    "  \"What is my name?\",\n",
    "  \"What is the length of word bob?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": questions[3]},\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning Structured Output\n",
    "\n",
    "Agent return a structured output instead of a single string.\n",
    "\n",
    "Example, agent doing question-answering over sources. Output should include answer and list of sources used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import json\n",
    "\n",
    "#* Retriever\n",
    "\n",
    "# Create a retriever over mock data.\n",
    "loader = document_loaders.TextLoader(\"../data/state_of_the_union.txt\")\n",
    "document = loader.load()\n",
    "\n",
    "text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000, chunk_overlap=0,\n",
    ")\n",
    "docs = text_splitter.split_documents(document)\n",
    "\n",
    "# add in the fake source information\n",
    "# Add a “page_chunk” tag to the metadata of each document.\n",
    "for i, doc in enumerate(docs):\n",
    "  doc.metadata[\"page_chunk\"] = i\n",
    "  \n",
    "vectorstore = vector_stores.chroma.Chroma.from_documents(\n",
    "  docs, text_embedding_models.OpenAIEmbeddings(), collection_name=\"state-of-union\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#* Tools\n",
    "# Create tools for the agent, specifically one tool to wrap the retriever.\n",
    "retriever_tool = vector_stores.create_retriever_tool(\n",
    "  retriever=retriever,\n",
    "  name=\"state-of-union-retriever\",\n",
    "  description=\"Create tools for the agent, specifically one tool to wrap the retriever.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_.\n",
    "\n",
    "Create custom parsing logic by passing the Response schema to the LLM via functions parameter, similar to passing tools for the agent to use.\n",
    "\n",
    "When Response function called by LLM, use as signal to return to user. \n",
    "When any other function called by LLM, treat as tool invocation.\n",
    "\n",
    "Parsing logic:\n",
    "- If no function is called, assume response to user is AgentFinish\n",
    "- If Response function is called, respond to user with inputs (structured output) \n",
    "and return AgentFinish\n",
    "- If any other function is called, treat as tool invocation and return AgentActionMessageLog\n",
    "\n",
    "Using AgentActionMessageLog allows to attach a log of messages for future use in passing back to the agent prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#* Response schema\n",
    "# Two fields: answer and list of sources.\n",
    "class Response(BaseModel):\n",
    "  \"\"\"Final response to the question being asked\"\"\"\n",
    "  \n",
    "  answer: str = Field(\n",
    "    description=\"The final response to the user\"\n",
    "  )\n",
    "  sources: List[int] = Field(\n",
    "      description=(\"List of page chunks that contain answer to the question. \"\n",
    "                   \"Only include a page chunk if it contains relevant information\")\n",
    "  )\n",
    "  \n",
    "#* Custom parsing logic\n",
    "def parse(output):\n",
    "  # If no function was invoked, return to user\n",
    "  if \"function_call\" not in output.additional_kwargs:\n",
    "    return agents.AgentFinish(\n",
    "      return_values={\"output\": output.content}, log=output.content\n",
    "    )\n",
    "  \n",
    "  # Parse out the function call\n",
    "  function_call = output.additional_kwargs[\"function_call\"]\n",
    "  name = function_call[\"name\"]\n",
    "  inputs = json.loads((function_call[\"arguments\"]))\n",
    "  \n",
    "  # If the Response function was invoked, return to the user with the function inputs\n",
    "  if name == \"Response\":\n",
    "    return agents.AgentFinish(return_values=inputs, log=str(function_call)) \n",
    "  # Return an agent action\n",
    "  else:\n",
    "    return agents.AgentActionMessageLog(\n",
    "      tool=name, tool_input=inputs, log=\"\", message_log=[output]\n",
    "    )\n",
    "\n",
    "#* Create Agent\n",
    "# prompt: placeholders for user's question and agent_scratchpad (intermediate steps)\n",
    "prompt = prompts.ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant\"),\n",
    "  (\"user\", \"{input}\"),\n",
    "  prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# tools: attach tools and Response format to LLM as functions\n",
    "llm = chat_models.chat_openai\n",
    "llm_with_tools = llm.bind_functions([retriever_tool, Response])\n",
    "\n",
    "agent = (\n",
    "  {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    # format agent_scratchpad from intermediate steps (AIMessages, FunctionMessages)\n",
    "    \"agent_scratchpad\": lambda x: agents.format_to_openai_function_messages(\n",
    "      x[\"intermediate_steps\"]\n",
    "    )\n",
    "  }\n",
    "  | prompt\n",
    "  | llm_with_tools\n",
    "  # custom output parser: parse LLM response\n",
    "  | parse\n",
    ")\n",
    "\n",
    "# AgentExecutor: run agent-tool loop\n",
    "agent_executor = agents.AgentExecutor(\n",
    "  tools=[retriever_tool], agent=agent, verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Run agent\n",
    "# It responds with a dictionary answer and sources keys\n",
    "agent_executor.invoke(\n",
    "  {\"input\": \"what did the president say about ketanji brown jackson\"},\n",
    "  return_only_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle parsing errors\n",
    "\n",
    "Occasionally LLM cannot determine step to take because outputs are not correctly formatted for output parser. Default agent errors easily control functionality with handle_parsing_errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access intermediate steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cap the max number of iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeouts for agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Agent as an Iterator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools as OpenAI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tools\n",
    "tool_tavily_search = agent_tools.TavilySearchResults(max_results=1)\n",
    "tools = [\n",
    "  tool_tavily_search,\n",
    "]\n",
    "\n",
    "# Create Agent\n",
    "prompt = prompts.hub.pull(\"hwchase17/react-chat\")\n",
    "# prompt = prompts.hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = chat_models.chat_openai\n",
    "\n",
    "\n",
    "my_agent = agents.MyAgent(prompt=prompt, tools=tools, agent_type=\"react\", llm=llm)\n",
    "\n",
    "questions = [\n",
    "    \"Hello\",\n",
    "    \"My name is Bob\",\n",
    "    \"what's my name?\",\n",
    "    \"Is there any actor with the same name as me?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = my_agent.invoke_agent(questions[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
