# PyTorch Documentation

## Basics

### [Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)

### [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)

### [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)

### [Defining a Neural Network](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)

### [What is a state_dict in PyTorch](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html)

### [Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)

### [Good usage of `non_blocking` and `pin_memory()` in PyTorch](https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html)

### [Zeroing out gradients in PyTorch](https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)

### [Reasoning about Shapes in PyTorch](https://pytorch.org/tutorials/recipes/recipes/reasoning_about_shapes.html)

### [Extension points in nn.Module for loading state_dict and tensor subclasses](https://pytorch.org/tutorials/recipes/recipes/swap_tensors.html)

### Saving and loading

#### [Saving and loading models for inference in PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html)

#### [Saving and loading a general checkpoint in PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)

#### [Saving and loading multiple models in one file using PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html)

#### [Warmstarting model using parameters from a different model in PyTorch](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)

#### [Saving and loading models across devices in PyTorch](https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html)

#### [Tips for Loading an nn.Module from a Checkpoint](https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html)

## Distributed Training

### [Distributed and Parallel Training Tutorials](https://pytorch.org/tutorials/distributed/home.html)

CODED

### [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)

CODED

### [Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html)

CODED

### [Single-Machine Model Parallel Best Practices](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)

CODED

### DistributedDataParallel (DDP)

#### [Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)

CODED

#### [Getting Started with Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

CODED

#### [Shard Optimizer States with ZeroRedundancyOptimizer](https://pytorch.org/tutorials/advanced/generic_join.html)

CODED

#### [Distributed Training with Uneven Inputs Using the Join Context Manager](https://pytorch.org/tutorials/advanced/generic_join.html)

CODED

#### [Data Parallelism](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)

CODED

#### [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)

CODED

#### [DistributedDataParallel Notes](https://pytorch.org/docs/master/notes/ddp.html)

CODED

### Fully Sharded Data Parallel (FSDP)

#### [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)

CODED

#### [Getting Started with Fully Sharded Data Parallel(FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)

CODED

#### [Advanced Model Training with Fully Sharded Data Parallel (FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html)

CODED

#### [torchrun (Elastic Launch)](https://pytorch.org/docs/stable/elastic/run.html)

#### [FullyShardedDataParallel](https://pytorch.org/docs/stable/fsdp.html)

#### [FSDP Notes](https://pytorch.org/docs/stable/notes/fsdp.html#fsdp-notes)

CODED

### Device Mesh

#### [torch.distributed](https://pytorch.org/docs/stable/distributed.html)

#### [Getting Started with DeviceMesh](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html)

CODED

### Tensor Parallel (TP)

#### [torch.distributed.tensor.parallel](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)

CODED

#### [Large Scale Transformer model training with Tensor Parallel (TP)](https://pytorch.org/tutorials/intermediate/TP_tutorial.html)

#### [Tensor Parallelism](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)

CODED

### Custom Extensions

#### [Distributed Optimizers](https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer)

CODED

#### [Distributed RPC Framework](https://pytorch.org/docs/stable/rpc.html)

#### [Customize Process Group Backends Using Cpp Extensions](https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html)

### Remote Procedure Call (RPC) distributed training

#### [Getting Started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)

#### [RPC API documents](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)

#### [Implementing a Parameter Server Using Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/#rpc_param_server_tutorial.html)

#### [Implementing Batch RPC Processing Using Asynchronous Executions](https://pytorch.org/tutorials/intermediate/rpc_async_execution.html)

#### [Combining Distributed DataParallel with Distributed RPC Framework](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)

#### [Direct Device-to-Device Communication with TensorPipe RPC](https://pytorch.org/tutorials/recipes/cuda_rpc.html)

### [Introduction to Libuv TCPStore Backend](https://pytorch.org/tutorials/intermediate/TCPStore_libuv_backend.html)

### Pipeline Parallelism

#### [Pipeline Parallelism](https://pytorch.org/docs/main/distributed.pipelining.html)

#### [Introduction to Distributed Pipeline Parallelism](https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html)

xxx

### [Shard Optimizer States with ZeroRedundancyOptimizer](https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html)

CODED

### [Distributed Optimizer with TorchScript support](https://pytorch.org/tutorials/recipes/distributed_optim_torchscript.html)

### Checkpoint (DCP)

#### [Getting Started with Distributed Checkpoint (DCP)](https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)

#### [Asynchronous Checkpointing (DCP)](https://pytorch.org/tutorials/recipes/distributed_async_checkpoint_recipe.html)

### [Deploying with Flask](https://pytorch.org/tutorials/recipes/deployment_with_flask.html)
