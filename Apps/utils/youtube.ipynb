{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_youtube_script(video_url):\n",
    "    try:\n",
    "        video_id = video_url.split(\"v=\")[1]\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        script = ' '.join([entry['text'] for entry in transcript])\n",
    "        return script\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=RYZ0FMAKRFs\"\n",
    "# script = get_youtube_script(youtube_url)\n",
    "\n",
    "# if script:\n",
    "#     print(script)\n",
    "# else:\n",
    "#     print(\"Failed to retrieve script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text.lower())\n",
    "\n",
    "    # Tokenize, lemmatize, and remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Additional custom stopwords\n",
    "    custom_stopwords = []\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in word_tokenize(\n",
    "        text) if token.lower() not in (stop_words | set(custom_stopwords))]\n",
    "\n",
    "    # Remove specific words and replace specified words\n",
    "    words_to_remove = [\"ill\", \"easily\", \"well\", \"lets\", \"usually\", \"basically\", \"basic\", \"okay\", \"just\", \"really\", \"simply\",\n",
    "                       \"literally\", \"quite\", \"actually\", \"definitely\", \"totally\", \"seriously\", \"probably\", \"absolutely\", \"hopefully\", \"clearly\"]\n",
    "    tokens = [token for token in tokens if token not in words_to_remove]\n",
    "\n",
    "    replaced_words = {\"leverage\": \"use\", \"aai\": \"ai\"}\n",
    "    tokens = [replaced_words.get(token, token) for token in tokens]\n",
    "\n",
    "    # Part-of-speech tagging and filter out non-meaningful words\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    meaningful_tokens = [word for word,\n",
    "                         pos in tagged_tokens if pos in ['NN', 'VB', 'JJ', 'RB']]\n",
    "\n",
    "    # Word frequency analysis\n",
    "    word_frequencies = Counter(meaningful_tokens)\n",
    "\n",
    "    # Remove very high and very low-frequency words\n",
    "    meaningful_tokens = [word for word in meaningful_tokens if 1 <\n",
    "                         word_frequencies[word] < len(meaningful_tokens)/2]\n",
    "\n",
    "    # Remove very short words\n",
    "    meaningful_tokens = [word for word in meaningful_tokens if len(word) > 2]\n",
    "\n",
    "    # Join the meaningful tokens back into a string\n",
    "    processed_text = ' '.join(meaningful_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def process_input_file(input_filename, output_filename):\n",
    "    try:\n",
    "        # Read input from input file\n",
    "        with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "            input_text = file.read()\n",
    "\n",
    "        # Preprocess the text\n",
    "        processed_text = preprocess_text(input_text)\n",
    "\n",
    "        # Write processed text to output file\n",
    "        with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(processed_text)\n",
    "\n",
    "        print(f\"Processing completed. Output written to {output_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# input_file = \"input.txt\"\n",
    "# output_file = \"output.txt\"\n",
    "# process_input_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Youtube Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_youtube_url(video_url, output_filename):\n",
    "    script = get_youtube_script(video_url)\n",
    "\n",
    "    if script:\n",
    "        processed_text = preprocess_text(script)\n",
    "\n",
    "        # Write processed text to output file\n",
    "        with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(processed_text)\n",
    "\n",
    "        print(f\"Processing completed. Output written to {output_filename}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve script.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# youtube_url = \"https://www.youtube.com/watch?v=RYZ0FMAKRFs\"\n",
    "# output_file = \"./data/youtube_processed.txt\"\n",
    "# process_youtube_url(youtube_url, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.text import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_loader = TextLoader(\"./data/youtube_input.txt\")\n",
    "text = text_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=12500, chunk_overlap=2000)\n",
    "docs = text_splitter.split_documents(text)\n",
    "\n",
    "output = [doc.page_content for doc in docs]\n",
    "output_splitted = \"\\n\\n\".join(output)\n",
    "\n",
    "with open(\"./data/youtube_output.txt\", 'w', encoding='utf-8') as file:\n",
    "    file.write(output_splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import config\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = \"https://www.youtube.com/watch?v=vU2S6dVf79M\"\n",
    "script = get_youtube_script(youtube_url)\n",
    "\n",
    "with open(\"./data/llm_input.txt\", 'w', encoding='utf-8') as file:\n",
    "    file.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/llm_input.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content into paragraphs based on the newline character\n",
    "paragraphs = content.split('\\n\\n')\n",
    "\n",
    "# Remove empty paragraphs\n",
    "paragraphs = [paragraph.strip()\n",
    "              for paragraph in paragraphs if paragraph.strip()]\n",
    "\n",
    "# gpt-3.5-turbo-0125, gpt-3.5-turbo-instruct\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "\n",
    "template = \"\"\"\\\n",
    "Your output should use the following template:\n",
    "\n",
    "# Keywords/Entities/Concepts/Complex Words\n",
    "\n",
    "## Name\n",
    "- Definition: Explanation, Core Meaning,  Key Features, Essential Attributes, Distinguishing Traits\n",
    "- Types: Varieties, Classifications, Different Forms\n",
    "- Usage: Practical Applications, Common Scenarios, Real-world Examples\n",
    "- Benefits, Challenges/Limitations/Issues\n",
    "- Others: Additional Insights, Miscellaneous Information, Noteworthy Details, History, Related Concepts\n",
    "\n",
    "# Techniques\n",
    "\n",
    "## Name\n",
    "- Description: Overview of the technique. Explanation of the fundamental concept/idea behind the technique.\n",
    "- Components: Breakdown of the key elements or parts involved in the technique.\n",
    "- Pipeline: Stages/Steps that outline the process flow of the technique, illustrating how data or tasks move through the system.\n",
    "- Implementation: Details on how to apply or integrate the technique. Recommended guidelines, strategies, Best Practices for using the technique effectively.\n",
    "- Use Cases: Examples and scenarios where the technique is particularly useful.\n",
    "- Advantages: Discussion of the benefits and strengths of the technique.\n",
    "- Limitations: Identification of any drawbacks or constraints associated with the technique. Potential mistakes or issues to be aware of when implementing the technique.\n",
    "\n",
    "Apply the following guidelines:\n",
    "- Create a detailed summary of the YouTube video using its transcription.\n",
    "- Extract important keywords from the transcript.\n",
    "- Identify complex words that may be unfamiliar to the average reader.\n",
    "- Extract techniques mentioned in the video.\n",
    "- If a keyword and a technique share the same name, combine them into one section.\n",
    "- Ensure that explanations are derived from the entire script.\n",
    "- Provide a comprehensive and clear understanding of the video's content.\n",
    "- Don't make it up. Only output content from the script only.\n",
    "\n",
    "Here is the script:\n",
    "{text}\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "chain = prompt_template | model\n",
    "\n",
    "output_file_path = './data/llm_output.txt'\n",
    "with open(output_file_path, 'a', encoding='utf-8') as f:\n",
    "  for i in tqdm(range(len(paragraphs))):\n",
    "    result = chain.invoke({\"text\": paragraphs[i]}).content\n",
    "    f.writelines(result + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
